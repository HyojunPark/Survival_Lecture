[["index.html", "SOC6280: Survival Analysis: Practice Chapter 1 About", " SOC6280: Survival Analysis: Practice Hyojun Park 2023-03-23 Chapter 1 About This is an additional lecture notes for Survival Analysis course, drawn from multiple textbooks and materials. "],["bias-assessment.html", "Chapter 2 Bias assessment 2.1 Bias due to omitted confounders 2.2 Overadjustment bias 2.3 Total effect 2.4 Overadjustment 2.5 Logistic models", " Chapter 2 Bias assessment Available at https://rpubs.com/Hyojun/bias 2.1 Bias due to omitted confounders \\[y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_2 + \\dots + \\epsilon_i; \\;\\; for \\;\\; i=1, \\dots, n\\] where the errors \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) with independent and identically distributed (i.i.d.) Let’s assume the following association is true (i.e., gold standard) without any selection bias, measurement bias, and other unmeasured confoundings. N &lt;- 100000 C &lt;- rnorm(N) X &lt;- .5 * C + rnorm(N) Y &lt;- .3 * C + .4 * X + rnorm(N) 2.1.1 Gold standard With the correct model specification (i.e., \\(C\\) as a confounder), we get an unbiased estimate of \\(X\\) on \\(Y\\). # Gold standard glm.unbiased &lt;- glm(Y~X + C, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + C, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.3913 -0.6740 0.0017 0.6729 4.5984 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.002618 0.003155 -0.83 0.407 ## X 0.401592 0.003158 127.18 &lt;2e-16 *** ## C 0.298346 0.003526 84.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.9953407) ## ## Null deviance: 140736 on 99999 degrees of freedom ## Residual deviance: 99531 on 99997 degrees of freedom ## AIC: 283326 ## ## Number of Fisher Scoring iterations: 2 2.1.2 Misspecified model: a confounder, \\(C\\), was omitted from the model By omitting \\(C\\), the estimate of \\(X\\) was biased either “away from” or “towards to” the null # C was omitted glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.7547 -0.7009 0.0010 0.6986 4.5059 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.002099 0.003266 -0.643 0.52 ## X 0.521810 0.002919 178.757 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.066573) ## ## Null deviance: 140736 on 99999 degrees of freedom ## Residual deviance: 106655 on 99998 degrees of freedom ## AIC: 290237 ## ## Number of Fisher Scoring iterations: 2 2.1.3 Bias “away from” or “towards to” the null? N &lt;- 100000 C &lt;- rnorm(N) X &lt;- -.5 * C + rnorm(N) Y &lt;- -.3 * C + .4 * X + rnorm(N) # C was omitted glm.unbiased &lt;- glm(Y~X + C, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + C, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.2198 -0.6817 0.0005 0.6768 4.2460 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.004496 0.003167 -1.42 0.156 ## X 0.403438 0.003168 127.35 &lt;2e-16 *** ## C -0.297215 0.003529 -84.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.002803) ## ## Null deviance: 141235 on 99999 degrees of freedom ## Residual deviance: 100277 on 99997 degrees of freedom ## AIC: 284073 ## ## Number of Fisher Scoring iterations: 2 glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.3327 -0.7020 -0.0003 0.6974 4.4609 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.004441 0.003277 -1.355 0.175 ## X 0.521703 0.002939 177.523 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.07393) ## ## Null deviance: 141235 on 99999 degrees of freedom ## Residual deviance: 107391 on 99998 degrees of freedom ## AIC: 290924 ## ## Number of Fisher Scoring iterations: 2 2.1.4 A \\(C\\) is not a confounder on \\(X\\) and \\(Y\\) N &lt;- 100000 C &lt;- rnorm(N) X &lt;- rnorm(N) Y &lt;- .4 * X + rnorm(N) 2.1.5 Correct model specification: Without \\(C\\) glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.1421 -0.6714 -0.0002 0.6690 4.2192 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.0009383 0.0031535 -0.298 0.766 ## X 0.3924725 0.0031445 124.814 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.9944728) ## ## Null deviance: 114938 on 99999 degrees of freedom ## Residual deviance: 99445 on 99998 degrees of freedom ## AIC: 283237 ## ## Number of Fisher Scoring iterations: 2 2.1.6 Misspecified model with \\(C\\) glm.unbiased &lt;- glm(Y~X + C, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + C, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.1400 -0.6714 0.0001 0.6688 4.2218 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.000938 0.003154 -0.297 0.766 ## X 0.392446 0.003145 124.800 &lt;2e-16 *** ## C -0.002999 0.003159 -0.949 0.342 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.9944738) ## ## Null deviance: 114938 on 99999 degrees of freedom ## Residual deviance: 99444 on 99997 degrees of freedom ## AIC: 283239 ## ## Number of Fisher Scoring iterations: 2 2.1.7 A \\(C\\) is a colloder on \\(X\\) and \\(Y\\) N &lt;- 100000 X &lt;- rnorm(N) Y &lt;- .7 * X + rnorm(N) C &lt;- 1.2 * X + .6 * Y + rnorm(N) 2.1.8 Correct model specification: Without \\(C\\) glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.5592 -0.6752 -0.0002 0.6735 4.7663 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.003792 0.003166 -1.198 0.231 ## X 0.701010 0.003156 222.124 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.002096) ## ## Null deviance: 149650 on 99999 degrees of freedom ## Residual deviance: 100208 on 99998 degrees of freedom ## AIC: 284001 ## ## Number of Fisher Scoring iterations: 2 2.1.9 Misspecified model with \\(C\\) This is one of examples of selection bias. For example, let’s say, \\(X\\) is Education, \\(Y\\) is income, and \\(C\\) is social welfare program. People at lower education (i.e., high risk group in terms of exposure) and lower income (i.e., higher risk group in terms of outcome) are more likely to register social welfare program. If survey was conducted based on the registered social welfare program, the “estimated” association from this “disproportionally selected” respondents are likely biased. glm.unbiased &lt;- glm(Y~X + C, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + C, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.6964 -0.5801 0.0000 0.5778 3.6577 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.002126 0.002708 -0.785 0.432 ## X -0.020419 0.004633 -4.407 1.05e-05 *** ## C 0.443912 0.002317 191.574 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.7330606) ## ## Null deviance: 149650 on 99999 degrees of freedom ## Residual deviance: 73304 on 99997 degrees of freedom ## AIC: 252740 ## ## Number of Fisher Scoring iterations: 2 2.2 Overadjustment bias Please note that this is not a comprehensive example; only reflect one aspect of potential overadjustement bias. Let’s assume a model with \\(M\\) as a mediator. N &lt;- 100000 X &lt;- rnorm(N) M &lt;- .5 * X + rnorm(N) Y &lt;- .3 * X + .4 * M + rnorm(N) 2.3 Total effect glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.6920 -0.7238 0.0051 0.7266 4.6174 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.001431 0.003400 0.421 0.674 ## X 0.499182 0.003387 147.402 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.156214) ## ## Null deviance: 140741 on 99999 degrees of freedom ## Residual deviance: 115619 on 99998 degrees of freedom ## AIC: 298307 ## ## Number of Fisher Scoring iterations: 2 2.4 Overadjustment glm.unbiased &lt;- glm(Y~X + M, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + M, family = &quot;gaussian&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.4095 -0.6765 0.0006 0.6723 4.3489 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.003517 0.003157 1.114 0.265 ## X 0.301734 0.003509 85.991 &lt;2e-16 *** ## M 0.399822 0.003156 126.674 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.9963448) ## ## Null deviance: 140741 on 99999 degrees of freedom ## Residual deviance: 99631 on 99997 degrees of freedom ## AIC: 283427 ## ## Number of Fisher Scoring iterations: 2 2.5 Logistic models 2.5.1 Sex as a Confounder, \\(C\\) MYY &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;Yes&quot;, Cancer = 1, freq = 5 ) MYN &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;Yes&quot;, Cancer = 0, freq = 8 ) MNY &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;No&quot;, Cancer = 1, freq = 45 ) MNN &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;No&quot;, Cancer = 0, freq = 72 ) FYY &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;Yes&quot;, Cancer = 1, freq = 25 ) FYN &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;Yes&quot;, Cancer = 0, freq = 10 ) FNY &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;No&quot;, Cancer = 1, freq = 25 ) FNN &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;No&quot;, Cancer = 0, freq = 10 ) Ex_confounder &lt;- rbind(MYY, MYN, MNY, MNN, FYY, FYN, FNY, FNN) Convert Freq table to raw data library(tidyr) raw_confounder &lt;- Ex_confounder %&gt;% uncount(freq) glm.unbiased &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_confounder) summary(glm.unbiased) ## ## Call: ## glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_confounder) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.40059 -1.11100 -0.07073 1.24530 1.24530 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1582 0.1627 -0.972 0.3309 ## SmokingYes 0.6690 0.3397 1.970 0.0489 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 277.26 on 199 degrees of freedom ## Residual deviance: 273.28 on 198 degrees of freedom ## AIC: 277.28 ## ## Number of Fisher Scoring iterations: 4 Full model: glm_logit &lt;- glm(Cancer ~ Smoking + Sex , family=binomial(link = &quot;logit&quot;), data=raw_confounder) glm_logit ## ## Call: glm(formula = Cancer ~ Smoking + Sex, family = binomial(link = &quot;logit&quot;), ## data = raw_confounder) ## ## Coefficients: ## (Intercept) SmokingYes SexMale ## 9.163e-01 4.266e-15 -1.386e+00 ## ## Degrees of Freedom: 199 Total (i.e. Null); 197 Residual ## Null Deviance: 277.3 ## Residual Deviance: 257 AIC: 263 Stratified models ## For males raw_confounder_M &lt;- raw_confounder[ which(raw_confounder$Sex==&#39;Male&#39;), ] glm_logit_m &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_confounder_M) glm_logit_m ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_confounder_M) ## ## Coefficients: ## (Intercept) SmokingYes ## -4.700e-01 6.672e-16 ## ## Degrees of Freedom: 129 Total (i.e. Null); 128 Residual ## Null Deviance: 173.2 ## Residual Deviance: 173.2 AIC: 177.2 # For females raw_confounder_F &lt;- raw_confounder[ which(raw_confounder$Sex==&#39;Female&#39;), ] glm_logit_f &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_confounder_F) glm_logit_f ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_confounder_F) ## ## Coefficients: ## (Intercept) SmokingYes ## 9.163e-01 9.400e-16 ## ## Degrees of Freedom: 69 Total (i.e. Null); 68 Residual ## Null Deviance: 83.76 ## Residual Deviance: 83.76 AIC: 87.76 2.5.2 Sex as a Moderator, \\(M\\) MYY &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;Yes&quot;, Cancer = 1, freq = 5 ) MYN &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;Yes&quot;, Cancer = 0, freq = 4 ) MNY &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;No&quot;, Cancer = 1, freq = 45 ) MNN &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;No&quot;, Cancer = 0, freq = 68 ) FYY &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;Yes&quot;, Cancer = 1, freq = 25 ) FYN &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;Yes&quot;, Cancer = 0, freq = 14 ) FNY &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;No&quot;, Cancer = 1, freq = 25 ) FNN &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;No&quot;, Cancer = 0, freq = 14 ) Ex_moderator &lt;- rbind(MYY, MYN, MNY, MNN, FYY, FYN, FNY, FNN) Convert Freq table to raw data library(tidyr) raw_moderator &lt;- Ex_moderator %&gt;% uncount(freq) Full model: glm_logit &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_moderator) glm_logit ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_moderator) ## ## Coefficients: ## (Intercept) SmokingYes ## -0.1582 0.6690 ## ## Degrees of Freedom: 199 Total (i.e. Null); 198 Residual ## Null Deviance: 277.3 ## Residual Deviance: 273.3 AIC: 277.3 Stratified models ## For males raw_moderator_M &lt;- raw_moderator[ which(raw_moderator$Sex==&#39;Male&#39;), ] glm_logit_m &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_moderator_M) glm_logit_m ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_moderator_M) ## ## Coefficients: ## (Intercept) SmokingYes ## -0.4128 0.6360 ## ## Degrees of Freedom: 121 Total (i.e. Null); 120 Residual ## Null Deviance: 165.1 ## Residual Deviance: 164.3 AIC: 168.3 # For females raw_moderator_F &lt;- raw_moderator[ which(raw_moderator$Sex==&#39;Female&#39;), ] glm_logit_f &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_moderator_F) glm_logit_f ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_moderator_F) ## ## Coefficients: ## (Intercept) SmokingYes ## 5.798e-01 -2.621e-16 ## ## Degrees of Freedom: 77 Total (i.e. Null); 76 Residual ## Null Deviance: 101.8 ## Residual Deviance: 101.8 AIC: 105.8 "],["survival-analyses-introduction.html", "Chapter 3 Survival Analyses: Introduction 3.1 Set packages and library 3.2 dataset 3.3 Nonparametric estimation 3.4 Proportional Hazards and Cox Regression 3.5 Parametric estimation", " Chapter 3 Survival Analyses: Introduction 3.1 Set packages and library 3.2 dataset The child dataset in eha package summary(child) # descriptive statistics ## id m.id sex socBranch ## Min. : 9 Min. : 55 male :13676 official: 610 ## 1st Qu.:249504 1st Qu.:248826 female:12898 farming :18641 ## Median :500126 Median :504920 business: 318 ## Mean :500080 Mean :501874 worker : 7005 ## 3rd Qu.:750266 3rd Qu.:752827 ## Max. :999976 Max. :999932 ## birthdate enter exit event illeg ## Min. :1850-01-01 Min. :0 Min. : 0.003 Min. :0.0000 no :24567 ## 1st Qu.:1861-01-05 1st Qu.:0 1st Qu.:15.000 1st Qu.:0.0000 yes: 2007 ## Median :1870-08-08 Median :0 Median :15.000 Median :0.0000 ## Mean :1869-06-09 Mean :0 Mean :12.231 Mean :0.2113 ## 3rd Qu.:1878-05-08 3rd Qu.:0 3rd Qu.:15.000 3rd Qu.:0.0000 ## Max. :1884-12-31 Max. :0 Max. :15.000 Max. :1.0000 ## m.age cohort ## Min. :15.83 Min. :-10.000 ## 1st Qu.:27.18 1st Qu.: 1.000 ## Median :31.79 Median : 10.000 ## Mean :32.03 Mean : 8.943 ## 3rd Qu.:36.74 3rd Qu.: 18.000 ## Max. :50.86 Max. : 24.000 ## SurvObj.start SurvObj.stop SurvObj.status ## Min. :0 Min. : 0.003000 Min. :0.0000000 ## 1st Qu.:0 1st Qu.:15.000000 1st Qu.:0.0000000 ## Median :0 Median :15.000000 Median :0.0000000 ## Mean :0 Mean :12.231114 Mean :0.2113344 ## 3rd Qu.:0 3rd Qu.:15.000000 3rd Qu.:0.0000000 ## Max. :0 Max. :15.000000 Max. :1.0000000 str(child) # structure ## &#39;data.frame&#39;: 26574 obs. of 12 variables: ## $ id : int 9 150 158 178 263 342 363 393 408 486 ... ## $ m.id : int 246606 377744 118277 715337 978617 282943 341341 840879 586140 564736 ... ## $ sex : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 1 1 1 1 2 1 1 1 2 2 ... ## $ socBranch: Factor w/ 4 levels &quot;official&quot;,&quot;farming&quot;,..: 2 2 4 2 4 2 2 2 2 2 ... ## $ birthdate: Date, format: &quot;1853-05-23&quot; &quot;1853-07-19&quot; ... ## $ enter : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exit : num 15 15 15 15 0.559 0.315 15 15 15 15 ... ## $ event : num 0 0 0 0 1 1 0 0 0 0 ... ## $ illeg : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ m.age : num 35 30.6 29.3 41.2 42.1 ... ## $ cohort : num -7 -7 1 12 -5 -5 5 6 7 -2 ... ## $ SurvObj : &#39;Surv&#39; num [1:26574, 1:3] (0,15.000+] (0,15.000+] (0,15.000+] (0,15.000+] ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:3] &quot;start&quot; &quot;stop&quot; &quot;status&quot; ## ..- attr(*, &quot;type&quot;)= chr &quot;counting&quot; head(child) # preview ## id m.id sex socBranch birthdate enter exit event illeg m.age ## 3 9 246606 male farming 1853-05-23 0 15.000 0 no 35.009 ## 42 150 377744 male farming 1853-07-19 0 15.000 0 no 30.609 ## 47 158 118277 male worker 1861-11-17 0 15.000 0 no 29.320 ## 54 178 715337 male farming 1872-11-16 0 15.000 0 no 41.183 ## 78 263 978617 female worker 1855-07-19 0 0.559 1 no 42.138 ## 102 342 282943 male farming 1855-09-29 0 0.315 1 no 32.931 ## cohort SurvObj ## 3 -7 (0,15.000+] ## 42 -7 (0,15.000+] ## 47 1 (0,15.000+] ## 54 12 (0,15.000+] ## 78 -5 (0, 0.559] ## 102 -5 (0, 0.315] 3.3 Nonparametric estimation 3.3.1 Data for nonparametric models The following code creates a set of vector for survival analysis. It contains 5 individuals’ survival time. \\(1\\) is an event (i.e., failure, death) and \\(0\\) is a cencored case. tt &lt;- c(7,6,6,5,2,4) cens &lt;- c(0,1,0,0,1,1) Surv(tt,cens) ## [1] 7+ 6 6+ 5+ 2 4 aaa &lt;- Surv(tt,cens) # demonstration only for checking how survival dataset was constructed aaa ## [1] 7+ 6 6+ 5+ 2 4 3.3.2 Kaplan-Meier estimator ## Models result.km &lt;- survfit(Surv(tt,cens)~1, conf.type=&quot;log-log&quot;) ## Table result.km ## Call: survfit(formula = Surv(tt, cens) ~ 1, conf.type = &quot;log-log&quot;) ## ## n events median 0.95LCL 0.95UCL ## [1,] 6 3 6 2 NA summary(result.km) ## Call: survfit(formula = Surv(tt, cens) ~ 1, conf.type = &quot;log-log&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 2 6 1 0.833 0.152 0.2731 0.975 ## 4 5 1 0.667 0.192 0.1946 0.904 ## 6 3 1 0.444 0.222 0.0662 0.785 ## Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. plot(result.km, ylab = &quot;Survival probability&quot;, xlab = &quot;Time&quot;, mark.time = T, main=&quot;KM survival curve&quot;) abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) plot(result.km, ylab = &quot;Cumulative hazard&quot;, xlab = &quot;Time&quot;, mark.time = T, fun=&quot;cumhaz&quot;, main=&quot;KM cumulative hazard curve&quot;) abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) 3.3.3 Nelson-Aalen estimator ## Models result.fh &lt;- survfit(Surv(tt,cens)~1, conf.type=&quot;log-log&quot;, type=&quot;fh&quot;) ## Table result.fh ## Call: survfit(formula = Surv(tt, cens) ~ 1, conf.type = &quot;log-log&quot;, ## type = &quot;fh&quot;) ## ## n events median 0.95LCL 0.95UCL ## [1,] 6 3 6 2 NA summary(result.fh) ## Call: survfit(formula = Surv(tt, cens) ~ 1, conf.type = &quot;log-log&quot;, ## type = &quot;fh&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 2 6 1 0.846 0.141 0.306 0.977 ## 4 5 1 0.693 0.180 0.229 0.913 ## 6 3 1 0.497 0.210 0.101 0.807 # Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. plot(result.fh, ylab = &quot;Survival probability&quot;, xlab = &quot;Time&quot;, mark.time = T, main=&quot;NA survival curve&quot;) abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) plot(result.fh, ylab = &quot;Cumulative hazard&quot;, xlab = &quot;Time&quot;, mark.time = T, fun=&quot;cumhaz&quot;, main=&quot;NA cumulative hazard curve&quot;) abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) 3.3.4 Comparisons by groups bysex &lt;- survfit(Surv(enter, exit, event) ~ sex, data=child, conf.type=&quot;log-log&quot;) ## Tables #bysex #summary(bysex) summary(bysex, times=c(0, 3, 6, 9, 12, 15)) # add time points ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = child, ## conf.type = &quot;log-log&quot;) ## ## sex=male ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 13676 0 1.000 0.00000 1.000 1.000 ## 3 11614 1924 0.859 0.00298 0.853 0.865 ## 6 10955 555 0.818 0.00331 0.811 0.824 ## 9 10653 240 0.800 0.00344 0.793 0.806 ## 12 10452 146 0.789 0.00351 0.782 0.795 ## 15 10269 120 0.780 0.00356 0.773 0.786 ## ## sex=female ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 12898 0 1.000 0.00000 1.000 1.000 ## 3 11152 1611 0.875 0.00292 0.869 0.880 ## 6 10578 501 0.835 0.00328 0.829 0.842 ## 9 10262 242 0.816 0.00343 0.809 0.823 ## 12 10079 129 0.806 0.00350 0.799 0.813 ## 15 9872 148 0.794 0.00358 0.787 0.801 ## plots plot(bysex, ylab = &quot;Survival probabilities&quot;, xlab = &quot;Survival time&quot;, #mark.time = T, main=&quot;Kaplan-Meier survival curve estimate with 95% CIs&quot; ) legend(&quot;topright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty=c(&quot;solid&quot;,&quot;dashed&quot;), col=c(&quot;black&quot;,&quot;red&quot;)) 3.3.5 Better KM figures library(ggfortify) library(ggplot2) autoplot(bysex, ylab = &quot;Survival probabilities&quot;, xlab = &quot;Survival time&quot;, #mark.time = T, main=&quot;Kaplan-Meier survival curve estimate with 95% CIs&quot; ) 3.3.6 Nonparametric models using a \\(child\\) dataset from eha ## Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. with(child, plot(Surv(enter, exit, event), fun = &quot;cumhaz&quot;, main = &quot;Cumulativa hazards function&quot;, xlab = &quot;Duration&quot;)) with(child, plot(Surv(enter, exit, event), main = &quot;Survival function&quot;, xlab = &quot;Duration&quot;)) 3.4 Proportional Hazards and Cox Regression cox01 &lt;- coxreg(Surv(enter, exit, event) ~ sex + socBranch + birthdate, data = child) print(summary(cox01), digits = 4) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.0019 ## male 0.510 0 1 (reference) ## female 0.490 -0.083 0.920 0.027 ## socBranch 0.0001 ## official 0.021 0 1 (reference) ## farming 0.710 -0.017 0.983 0.092 ## business 0.011 0.330 1.391 0.141 ## worker 0.258 0.099 1.104 0.094 ## birthdate 1869-07-13 -0.000 1.000 0.000 0.0000 ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -56481 ## LR test statistic 67.10 ## Degrees of freedom 5 ## Overall p-value 4.11227e-13 child$cohort &lt;- floor(toTime(child$birthdate)) # age cohort cox02 &lt;- coxreg(Surv(enter, exit, event) ~ sex + socBranch + cohort, data = child) print(summary(cox02), digits = 4) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.0018 ## male 0.510 0 1 (reference) ## female 0.490 -0.083 0.920 0.027 ## socBranch 0.0001 ## official 0.021 0 1 (reference) ## farming 0.710 -0.017 0.984 0.092 ## business 0.011 0.330 1.390 0.141 ## worker 0.258 0.099 1.104 0.094 ## cohort 1869.035 -0.008 0.992 0.001 0.0000 ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -56481 ## LR test statistic 66.79 ## Degrees of freedom 5 ## Overall p-value 4.75731e-13 range(child$cohort) ## [1] 1850 1884 child$cohort &lt;- child$cohort - 1860 cox03 &lt;- coxreg(Surv(enter, exit, event) ~ sex + socBranch + cohort, data = child) # Table summary(cox03) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.002 ## male 0.510 0 1 (reference) ## female 0.490 -0.083 0.920 0.027 ## socBranch 0.000 ## official 0.021 0 1 (reference) ## farming 0.710 -0.017 0.984 0.092 ## business 0.011 0.330 1.390 0.141 ## worker 0.258 0.099 1.104 0.094 ## cohort 9.035 -0.008 0.992 0.001 0.000 ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -56481 ## LR test statistic 66.79 ## Degrees of freedom 5 ## Overall p-value 4.75731e-13 # Plots par(mfrow = c(1, 2), las = 1) plot(cox03, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(cox03, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) 3.4.1 A visual check for a proportionality assumption library(survival) ## Create survival vector for fish dataset child$SurvObj &lt;- with(child, Surv(enter, exit, event)) par(mfrow = c(1, 2), las = 1) plot(survfit(SurvObj ~ sex, data=child), main = &quot;Proportional hazard by sex&quot;, ylab = &quot;Survival&quot;, col=c(&quot;black&quot;, &quot;red&quot;) ) plot(survfit(SurvObj ~ sex, data=child), fun = &quot;cloglog&quot;, ylab = &quot;Log-log survival&quot;, main = &quot;Proportional hazard by sex&quot;, col=c(&quot;black&quot;, &quot;red&quot;) ) library(survival) ## Create survival vector for fish dataset child$SurvObj &lt;- with(child, Surv(enter, exit, event)) par(mfrow = c(1, 2), las = 1) plot(survfit(SurvObj ~ socBranch, data=child), main = &quot;Proportional hazard by sex&quot;, ylab = &quot;Survival&quot;, col=c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;) ) plot(survfit(SurvObj ~ socBranch, data=child), fun = &quot;cloglog&quot;, ylab = &quot;Log-log survival&quot;, main = &quot;Proportional hazard by sex&quot;, col=c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;) ) 3.5 Parametric estimation 3.5.1 Weibull model # Models parm_weib &lt;- phreg(Surv(enter, exit, event) ~ sex + socBranch + cohort , dist = &quot;weibull&quot;, data = child) # Table #print(summary(parm), digits = 4) parm_weib ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + socBranch + ## cohort, data = child, dist = &quot;weibull&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.510 0 1 (reference) ## female 0.490 -0.083 0.920 0.027 0.002 ## socBranch ## official 0.021 0 1 (reference) ## farming 0.710 -0.026 0.975 0.092 0.780 ## business 0.011 0.332 1.393 0.141 0.019 ## worker 0.258 0.092 1.097 0.094 0.329 ## cohort 9.035 -0.008 0.992 0.001 0.000 ## ## log(scale) 5.887 0.228 0.000 ## log(shape) -0.880 0.013 0.000 ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -25131 ## LR test statistic 68.69 ## Degrees of freedom 5 ## Overall p-value 1.91736e-13 # Plots par(mfrow = c(1, 2), las = 1) plot(parm_weib, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(parm_weib, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) 3.5.2 Gompertz model # Models parm_gomp &lt;- phreg(Surv(enter, exit, event) ~ sex + socBranch + cohort , dist = &quot;gompertz&quot;, data = child) # Table #print(summary(parm), digits = 4) parm_gomp ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + socBranch + ## cohort, data = child, dist = &quot;gompertz&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.510 0 1 (reference) ## female 0.490 -0.087 0.916 NA NA ## socBranch ## official 0.021 0 1 (reference) ## farming 0.710 -0.064 0.938 NA NA ## business 0.011 0.349 1.417 NA NA ## worker 0.258 0.066 1.068 NA NA ## cohort 9.035 -0.008 0.992 NA NA ## ## log(scale) 401.049 NA NA ## log(shape) 397.124 NA NA ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -28368 ## LR test statistic 78.89 ## Degrees of freedom 5 ## Overall p-value 1.44329e-15 # Plots par(mfrow = c(1, 2), las = 1) plot(parm_gomp, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(parm_gomp, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) "],["counting-process.html", "Chapter 4 Counting Process 4.1 Set packages and library 4.2 Example 4.3 Practice: AddHealth Public datasets", " Chapter 4 Counting Process In this section, we will cover 1) survival data structure (i.e., counting process) and 2) modeling survival data. 4.1 Set packages and library 4.2 Example “The oldmort dataset in eha package contains life histories of people followed from their 60th birthday to their 100th, or until death, born between June 28, 1765 and December 31, 1820 in Skellefteå. The variable enter is age at start of the given interval, exit contains the age at the end of the interval. We need to calculate follow-up time since age 60 - 60 is subtracted from enter and exit. The variable event is an indicator of death at the duration given by exit.” https://www.rdocumentation.org/packages/eha/versions/2.8.5/topics/oldmort [Göran Broström, http://ehar.se/r/ehar2/parametric.html] Here are the summary of the oldmort dataset. library(eha) oldmort01 &lt;- oldmort summary(oldmort01) # descriptive statistics ## id enter exit event ## Min. :765000603 Min. :60.00 Min. : 60.00 Mode :logical ## 1st Qu.:797001170 1st Qu.:60.00 1st Qu.: 63.88 FALSE:4524 ## Median :804001545 Median :60.07 Median : 68.51 TRUE :1971 ## Mean :803652514 Mean :64.07 Mean : 69.89 ## 3rd Qu.:812001564 3rd Qu.:66.88 3rd Qu.: 74.73 ## Max. :826002672 Max. :94.51 Max. :100.00 ## ## birthdate m.id f.id sex ## Min. :1765 Min. : 6039 Min. : 2458 male :2884 ## 1st Qu.:1797 1st Qu.:766000610 1st Qu.:763000610 female:3611 ## Median :1805 Median :775000742 Median :772000649 ## Mean :1804 Mean :771271398 Mean :762726961 ## 3rd Qu.:1812 3rd Qu.:783000743 3rd Qu.:780001077 ## Max. :1820 Max. :802000669 Max. :797001468 ## NA&#39;s :3155 NA&#39;s :3310 ## civ ses.50 birthplace imr.birth region ## unmarried: 557 middle : 233 parish:3598 Min. : 4.348 town : 657 ## married :3638 unknown:2565 region:1503 1st Qu.:12.709 industry:2214 ## widow :2300 upper : 55 remote:1394 Median :14.234 rural :3624 ## farmer :1562 Mean :15.209 ## lower :2080 3rd Qu.:17.718 ## Max. :31.967 ## str(oldmort01) # structure ## &#39;data.frame&#39;: 6495 obs. of 13 variables: ## $ id : int 765000603 765000669 768000648 770000562 770000707 771000617 771000619 771000638 771000670 772000622 ... ## $ enter : num 94.5 94.3 91.1 89 90 ... ## $ exit : num 95.8 95.8 91.9 89.6 90.2 ... ## $ event : logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ birthdate : num 1765 1766 1769 1771 1770 ... ## $ m.id : int NA NA NA NA NA NA NA NA NA NA ... ## $ f.id : int NA NA NA NA NA NA NA NA NA NA ... ## $ sex : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 2 2 2 2 2 2 1 2 2 2 ... ## $ civ : Factor w/ 3 levels &quot;unmarried&quot;,&quot;married&quot;,..: 3 1 3 3 3 3 3 3 3 3 ... ## $ ses.50 : Factor w/ 5 levels &quot;middle&quot;,&quot;unknown&quot;,..: 2 2 2 2 1 2 5 2 2 2 ... ## $ birthplace: Factor w/ 3 levels &quot;parish&quot;,&quot;region&quot;,..: 3 1 1 1 2 1 2 1 1 1 ... ## $ imr.birth : num 22.2 17.7 12.7 16.9 12 ... ## $ region : Factor w/ 3 levels &quot;town&quot;,&quot;industry&quot;,..: 3 2 3 2 3 3 3 2 3 2 ... head(oldmort01) # preview ## id enter exit event birthdate m.id f.id sex civ ses.50 ## 1 765000603 94.510 95.813 TRUE 1765.490 NA NA female widow unknown ## 2 765000669 94.266 95.756 TRUE 1765.734 NA NA female unmarried unknown ## 3 768000648 91.093 91.947 TRUE 1768.907 NA NA female widow unknown ## 4 770000562 89.009 89.593 TRUE 1770.991 NA NA female widow unknown ## 5 770000707 89.998 90.211 TRUE 1770.002 NA NA female widow middle ## 6 771000617 88.429 89.762 TRUE 1771.571 NA NA female widow unknown ## birthplace imr.birth region ## 1 remote 22.20000 rural ## 2 parish 17.71845 industry ## 3 parish 12.70903 rural ## 4 parish 16.90544 industry ## 5 region 11.97183 rural ## 6 parish 13.08594 rural To check how this dataset is constructed, we will need to identify any duplicated id. dup01 &lt;- data.frame(table(oldmort01$id)) dup02 &lt;- dup01[order(-dup01$Freq), ] In the following example, please check When is a new record for the same id created? What are the time-invariant variables? What are the time-variant variables? What does it mean by “TRUE” or “FALSE” in event? How does the time of enter and exit connected with each other? What would happen if there is a gap between two records? dup03 &lt;- oldmort01[oldmort01$id %in% c(&quot;789000771&quot;, &quot;796001158&quot;), ] dup03 ## id enter exit event birthdate m.id f.id sex civ ses.50 ## 536 789000771 70.570 72.059 FALSE 1789.430 NA NA female married unknown ## 537 789000771 72.059 79.391 FALSE 1789.430 NA NA female married unknown ## 538 789000771 79.391 79.947 FALSE 1789.430 NA NA female widow unknown ## 539 789000771 80.010 83.750 FALSE 1789.430 NA NA female widow unknown ## 540 789000771 84.358 87.274 TRUE 1789.430 NA NA female unmarried unknown ## 1472 796001158 63.531 64.020 FALSE 1796.469 NA NA male married farmer ## 1473 796001158 64.020 65.020 FALSE 1796.469 NA NA male widow farmer ## 1474 796001158 65.020 70.019 FALSE 1796.469 NA NA male widow farmer ## 1475 796001158 70.019 80.021 FALSE 1796.469 NA NA male widow farmer ## 1476 796001158 80.021 83.531 FALSE 1796.469 NA NA male widow farmer ## birthplace imr.birth region ## 536 region 19.92337 rural ## 537 region 19.92337 rural ## 538 region 19.92337 rural ## 539 region 19.92337 rural ## 540 region 19.92337 rural ## 1472 parish 19.92337 rural ## 1473 parish 19.92337 rural ## 1474 parish 19.92337 rural ## 1475 parish 19.92337 rural ## 1476 parish 19.92337 rural 4.3 Practice: AddHealth Public datasets There are many ways to construct long-form datasets with counting process. The following procedure is just one way to achieve the goal. Here are a couple of things to construct a long-form dataset with counting process. In practice, measuring outcomes, exposures, confounders, and other variables involves a separate procedure for each one of variables. I personally prefer to divide each measurement as time-variant and time-invariant datasets, respectively. Two variables should be ALWAYS included in every single dataset you are working on - AID and wave (or any other Time variable). Datasets with time-invariant variables can be merged by AID, while those with time-variant variables need to be merged by AID and wave. Time-varying variables will be assigned a single variable name. Let’s say we are to use self-rated health with a variable name of SRH for five waves. The dataset should contain AID, wave, and SRH. The SRH in each wave should be assigned the same name, SRH, and the wave information will be on wave. This way, you can simply “stack up” all 5-wave data to construct the long-form datasets. First, each rda dataset will be loaded and then saved as WAVE0X. After assigning a wave variable for each of them, we will keep the WAVE0X dataset and WX datasets only. #1st wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0001/21600-0001-Data.rda&quot;) wave01 &lt;- da21600.0001 wave01$wave &lt;- 1 rm(da21600.0001) w1 = subset(wave01, select = c(AID, wave)) #2nd wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0005/21600-0005-Data.rda&quot;) wave02 &lt;- da21600.0005 wave02$wave &lt;- 2 rm(da21600.0005) w2 = subset(wave02, select = c(AID, wave)) #3rd wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0008/21600-0008-Data.rda&quot;) wave03 &lt;- da21600.0008 wave03$wave &lt;- 3 rm(da21600.0008) w3 = subset(wave03, select = c(AID, wave)) #4th wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0022/21600-0022-Data.rda&quot;) wave04 &lt;- da21600.0022 wave04$wave &lt;- 4 rm(da21600.0022) w4 = subset(wave04, select = c(AID, wave)) # 5th wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0032/21600-0032-Data.rda&quot;) wave05 &lt;- da21600.0032 wave05$wave &lt;- 5 rm(da21600.0032) w5 = subset(wave05, select = c(AID, wave)) The complete list of respondents can be obtained by aggregating all WX datasets and then getting the unique AID. The numbers of cases for both occasions are the same. It looks like the first wave contains all respondents - no additional respondents were added. To check this observation, we will see both AID from the first wave and All matched. Because the n didn’t change, we confirmed that WAVE01 contains all respondents of the study. AH01 &lt;- unique(subset(rbind(w1, w2, w3, w4, w5), select = c(AID))) test01 &lt;- cbind(wave01, AH01, by=&quot;AID&quot;) 4.3.1 Generate a complete framework with AID and wave (Optional) I personally prefer working with a “complete framework” containing all AID and wave. lf01 &lt;- rbind(w1, w2, w3, w4, w5) lf &lt;- lf01[order(lf01$AID, lf01$wave), ] Here is the merged (“stacked”) dataset. head(lf01) ## AID wave ## 1 57100270 1 ## 2 57101310 1 ## 3 57103171 1 ## 4 57103869 1 ## 5 57104553 1 ## 6 57104649 1 Sorting by AID and wave, we can easily identify the data structure by AID and wave. This lf dataset is what I call a “framework” of this data source, which is the one that will be used whenever combining or merging datasets. head(lf) ## AID wave ## 1 57100270 1 ## 11339 57100270 3 ## 2 57101310 1 ## 6505 57101310 2 ## 11340 57101310 3 ## 16221 57101310 4 Keep the number of cases (n = 25530) for your record. This number should be the number you expect whenever you merge or stack datasets. count(lf) ## n ## 1 25530 4.3.2 Time-variant variables from each wave In this practice, we will select and rename self-rated health (for all 5-wave) and appetite (only for \\(1^{st}\\) and \\(2^{nd}\\) waves) measures. srh1 &lt;- wave01 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H1GH1, &quot;a_poorappetite&quot; = H1FS2) srh2 &lt;- wave02 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H2GH1, &quot;a_poorappetite&quot; = H2GH22) srh3 &lt;- wave03 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H3GH1) srh4 &lt;- wave04 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H4GH1) srh5 &lt;- wave05 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H5ID1) Please note that how to name the “temporary” datasets. I found that using the combination of ‘variable name + wave’ minimizes any confusions later. The ‘rbind’ function requires all datasets have a same numbers of columns. ‘setDT’ and ‘fill=TRUE’ are the functions from a ‘data.table’ package that override this requirement. Now, we have created a long-form dataset (i.e., srh_TV) from five sets of cross-sectional datasets. srh_TV01 &lt;- rbind(setDT(srh1), setDT(srh2), setDT(srh3), setDT(srh4), setDT(srh5), fill=TRUE) srh_TV &lt;- srh_TV01[order(srh_TV01$AID, srh_TV01$wave), ] # 6504, 4834, 4882, 5114, 4196, 25530 head(srh_TV) ## AID wave a_srh a_poorappetite ## 1: 57100270 1 (3) (3) Good (0) (0) Never/rarely ## 2: 57100270 3 (1) (1) Excellent &lt;NA&gt; ## 3: 57101310 1 (4) (4) Fair (1) (1) Sometimes ## 4: 57101310 2 (4) (4) Fair (4) (4) Every day ## 5: 57101310 3 (2) (2) Very good &lt;NA&gt; ## 6: 57101310 4 (3) (3) Good &lt;NA&gt; 4.3.3 Time-invariant By definition, when a variable is time-invariant, only one measure from any variable should be applied to all other waves. In this example, we select sex from the first wave (because of completeness), which will be applied to the whole long-form dataset. demo_TI &lt;- wave01 %&gt;% select(AID, &quot;a_sex&quot; = BIO_SEX) 4.3.4 Merging datasets Once you have selected, created, and modified all required variables by waves, stacking all waves datasets will generate a long-form dataset per wave-person as long as you have keep AID and wave variables for all datasets. In this example, we’ve created three datasets - lf (a framework), demo_TI (time-invariant), and srh_TV (time-variant). Framework + time-invariant (i.e., lf (a framework) and demo_TI (time-invariant)) Final01 &lt;- merge(lf, demo_TI, by = c(&quot;AID&quot;)) head(Final01) ## AID wave a_sex ## 1 57100270 1 (2) (2) Female ## 2 57100270 3 (2) (2) Female ## 3 57101310 1 (2) (2) Female ## 4 57101310 2 (2) (2) Female ## 5 57101310 3 (2) (2) Female ## 6 57101310 4 (2) (2) Female Framework + time-invariant + time-variant (i.e., Final01 + srh_TV (time-variant)) Final02 &lt;- merge(Final01, srh_TV, by = c(&quot;AID&quot;, &quot;wave&quot;)) head(Final02) ## AID wave a_sex a_srh a_poorappetite ## 1 57100270 1 (2) (2) Female (3) (3) Good (0) (0) Never/rarely ## 2 57100270 3 (2) (2) Female (1) (1) Excellent &lt;NA&gt; ## 3 57101310 1 (2) (2) Female (4) (4) Fair (1) (1) Sometimes ## 4 57101310 2 (2) (2) Female (4) (4) Fair (4) (4) Every day ## 5 57101310 3 (2) (2) Female (2) (2) Very good &lt;NA&gt; ## 6 57101310 4 (2) (2) Female (3) (3) Good &lt;NA&gt; 4.3.5 Define event, enter, and exit The event can be defined as your outcomes. Depending on the nature of outcomes, it could be a multiple or repetitive events, requiring more complex survival modeling with more assumptions. Using lag/lead(wave) Final03 &lt;- Final02 %&gt;% group_by(AID) %&gt;% dplyr::mutate( enter = lag(wave), exit = wave ) %&gt;% ungroup() Final03$enter[Final03$wave == 1 &amp; is.na(Final03$enter)] &lt;- 0 Because we used wave as an example, it may look more complicated than necessary - for example, we may simply use enter = exit - 1. However, this lag/lead function is required when working with the actual date which interval is not always equal to 1. "],["survival-models-specification-estimation-and-interpretation.html", "Chapter 5 Survival models: specification, estimation, and interpretation 5.1 Nonparametric models 5.2 Semi-parametric models: Cox Regression 5.3 Logistic regression 5.4 Linear regression 5.5 Weibull model 5.6 Exponential model 5.7 Gompertz model 5.8 Graphs", " Chapter 5 Survival models: specification, estimation, and interpretation Let’s think some some feasible models addressing how the survival varies by sex, region, and infant mortality of the cohort, using oldmort01 dataset. Here are some possible models depending on the outcome: Descriptive models for survival time Linear or Poisson regression on the ‘survival time’, which can be defined as the time of death (i.e., ‘exit’). We may need to subset only those who died, potentially resulting in considerable loss of data. Logistic regression for the event, death. How would you incorporate “survival time” in this model? Semiparametric survival regression models parametric survival regression models 5.1 Nonparametric models Let’s fit Kaplan-Meier (KM) and Nelson-Aalen (NA) estimators using the oldmort01 dataset from the eha package. Kaplan-Meier (KM) survival estimator ## KM bysex_KM &lt;- survfit(Surv(enter, exit, event) ~ sex, data=oldmort01, conf.type=&quot;log-log&quot;) ## Tables bysex_KM ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = oldmort01, ## conf.type = &quot;log-log&quot;) ## ## records n events median 0.95LCL 0.95UCL ## sex=male 2884 1390 854 75.1 74.3 75.6 ## sex=female 3611 1833 1117 76.7 76.1 77.1 ##summary(bysex) summary(bysex_KM, times=c(60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110)) # add time points ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = oldmort01, ## conf.type = &quot;log-log&quot;) ## ## sex=male ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 60 1390 0 1.00000 0.00000 1.000000 1.0000 ## 65 1017 183 0.85817 0.00974 0.837865 0.8761 ## 70 697 168 0.70232 0.01352 0.674905 0.7279 ## 75 428 184 0.50490 0.01576 0.473598 0.5353 ## 80 191 171 0.28095 0.01561 0.250745 0.3119 ## 85 55 105 0.10892 0.01237 0.086192 0.1346 ## 90 12 34 0.03311 0.00841 0.019374 0.0526 ## 95 1 8 0.00392 0.00381 0.000404 0.0197 ## ## sex=female ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 60 1833 0 1.0000 0.00000 1.000000 1.00000 ## 65 1416 166 0.9022 0.00723 0.886989 0.91541 ## 70 1034 205 0.7610 0.01094 0.738691 0.78161 ## 75 669 238 0.5736 0.01343 0.546771 0.59938 ## 80 318 236 0.3477 0.01415 0.320058 0.37547 ## 85 115 167 0.1527 0.01192 0.130179 0.17687 ## 90 27 80 0.0380 0.00705 0.025840 0.05359 ## 95 5 21 0.0075 0.00333 0.002894 0.01662 ## 100 1 4 0.0015 0.00150 0.000153 0.00812 Nelson-Aalen (NA) estimator ## NA bysex_NA &lt;- survfit(Surv(enter, exit, event) ~ sex, data=oldmort01, conf.type=&quot;log-log&quot;, type=&quot;fh&quot;) # an option for NA estimator ## Tables bysex_NA ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = oldmort01, ## conf.type = &quot;log-log&quot;, type = &quot;fh&quot;) ## ## records n events median 0.95LCL 0.95UCL ## sex=male 2884 1390 854 75.1 74.3 75.6 ## sex=female 3611 1833 1117 76.7 76.1 77.1 ##summary(bysex) summary(bysex_NA, times=c(60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110)) # add time points ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = oldmort01, ## conf.type = &quot;log-log&quot;, type = &quot;fh&quot;) ## ## sex=male ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 60 1390 0 1.00000 0.00000 1.00000 1.0000 ## 65 1017 183 0.85822 0.00974 0.83793 0.8762 ## 70 697 168 0.70245 0.01352 0.67504 0.7280 ## 75 428 184 0.50514 0.01575 0.47385 0.5356 ## 80 191 171 0.28138 0.01561 0.25118 0.3123 ## 85 55 105 0.10962 0.01239 0.08685 0.1353 ## 90 12 34 0.03418 0.00849 0.02024 0.0538 ## 95 1 8 0.00583 0.00449 0.00101 0.0216 ## ## sex=female ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 60 1833 0 1.00000 0.00000 1.000000 1.00000 ## 65 1416 166 0.90220 0.00723 0.887022 0.91543 ## 70 1034 205 0.76103 0.01094 0.738776 0.78169 ## 75 669 238 0.57371 0.01342 0.546935 0.59953 ## 80 318 236 0.34799 0.01415 0.320348 0.37575 ## 85 115 167 0.15315 0.01193 0.130627 0.17734 ## 90 27 80 0.03863 0.00710 0.026391 0.05434 ## 95 5 21 0.00824 0.00347 0.003349 0.01759 ## 100 1 4 0.00228 0.00183 0.000381 0.00911 Overall survival and hazard curves for the population ## Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. with(oldmort01, plot(Surv(enter, exit, event), fun = &quot;cumhaz&quot;, main = &quot;Cumulativa hazards function&quot;, xlab = &quot;Duration&quot;)) with(oldmort01, plot(Surv(enter, exit, event), main = &quot;Survival function&quot;, xlab = &quot;Duration&quot;)) Comparison between Male and Female # Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. plot(bysex_KM, ylab = &quot;Survival probability&quot;, xlab = &quot;Time&quot;, mark.time = T, main=&quot;Kaplan-Meier survival curve&quot;) legend(&quot;topleft&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty=c(&quot;solid&quot;,&quot;dashed&quot;), col=c(&quot;black&quot;,&quot;red&quot;)) #abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) plot(bysex_NA, ylab = &quot;Cumulative hazard&quot;, xlab = &quot;Time&quot;, mark.time = T, fun=&quot;cumhaz&quot;, main=&quot;Nelson-Aalen cumulative hazard curve&quot;) legend(&quot;topleft&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty=c(&quot;solid&quot;,&quot;dashed&quot;), col=c(&quot;black&quot;,&quot;red&quot;)) #abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) For a better plot for comparisons library(ggfortify) library(ggplot2) autoplot(bysex_KM, ylab = &quot;Survival probabilities&quot;, xlab = &quot;Survival time&quot;, #mark.time = T, main=&quot;Kaplan-Meier survival curve estimate with 95% CIs&quot; ) 5.2 Semi-parametric models: Cox Regression 5.2.1 Model specification \\[h(t)=h_0 (t)\\exp(b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR})\\] 5.2.2 Estimation oldmort_cox &lt;- coxreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) print(summary(oldmort_cox), digits = 4) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.0001 ## male 0.406 0 1 (reference) ## female 0.594 -0.185 0.831 0.046 ## region 0.0013 ## town 0.111 0 1 (reference) ## industry 0.326 0.225 1.252 0.087 ## rural 0.563 0.069 1.071 0.087 ## imr.birth 15.162 0.005 1.005 0.007 0.5009 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -13563 ## LR test statistic 31.41 ## Degrees of freedom 4 ## Overall p-value 2.52611e-06 b_cox &lt;- coef(oldmort_cox) expb_cox &lt;- exp(coef(oldmort_cox)) # Plots par(mfrow = c(1, 2), las = 1) plot(oldmort_cox, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(oldmort_cox, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_cox)) ## sexfemale regionindustry regionrural imr.birth ## 0.8308988 1.2522007 1.0714696 1.0045585 5.2.3 Interpretations What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(\\exp(b_i) = 1\\), 2) \\(\\exp(b_i) &lt; 1\\), or 3) \\(\\exp(b_i) &gt; 1\\)? How would you compare \\(p(death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(p(death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(p(death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.3 Logistic regression To fit logistic regression, ‘death’ variable was created. oldmort01$death &lt;- ifelse(oldmort01$event == &quot;TRUE&quot;, 1, 0) 5.3.1 Model specification \\[ \\ln \\left( \\frac{p(y)}{1-p(y)} \\right) = b_0 + b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR}\\] 5.3.2 Estimation Logistic model was fitted as below. oldmort_log &lt;- glm(death ~ sex + region + imr.birth, data=oldmort01, family = binomial(link = &quot;logit&quot;)) summary(oldmort_log) ## ## Call: ## glm(formula = death ~ sex + region + imr.birth, family = binomial(link = &quot;logit&quot;), ## data = oldmort01) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9467 -0.8371 -0.8105 1.4506 1.6661 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.003482 0.166455 -6.029 1.65e-09 *** ## sexfemale 0.073055 0.054619 1.338 0.181050 ## regionindustry 0.378124 0.100516 3.762 0.000169 *** ## regionrural 0.111579 0.099098 1.126 0.260188 ## imr.birth -0.004146 0.007756 -0.534 0.593010 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 7972.9 on 6494 degrees of freedom ## Residual deviance: 7944.9 on 6490 degrees of freedom ## AIC: 7954.9 ## ## Number of Fisher Scoring iterations: 4 b_log = coef(oldmort_log) expb = exp(coef(oldmort_log)) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_log)) ## (Intercept) sexfemale regionindustry regionrural imr.birth ## 0.3666006 1.0757895 1.4595443 1.1180423 0.9958631 5.3.3 Interpretation What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(exp(b_i) = 1\\), 2) \\(exp(b_i) &lt; 1\\), or 3) \\(exp(b_i) &gt; 1\\)? How would you compare \\(p(death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(p(death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(p(death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.4 Linear regression 5.4.1 Model specification \\[ Y_{Time\\;to\\; death} = b_0 + b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR} \\] 5.4.2 Estimation To fit linear model, we need to subset data for the death and use ‘exit’ as an outcome. oldmort02 &lt;- oldmort01[oldmort01$death == 1,] oldmort_lm &lt;- glm(exit ~ sex + region + imr.birth, data=oldmort02, family = &quot;gaussian&quot;) summary(oldmort_lm) ## ## Call: ## glm(formula = exit ~ sex + region + imr.birth, family = &quot;gaussian&quot;, ## data = oldmort02) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -14.6605 -6.3367 -0.1812 5.4018 25.3607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 71.99777 1.12285 64.120 &lt; 2e-16 *** ## sexfemale 1.90403 0.35249 5.402 7.4e-08 *** ## regionindustry 2.07598 0.66429 3.125 0.00180 ** ## regionrural 1.80560 0.66531 2.714 0.00671 ** ## imr.birth -0.09621 0.05156 -1.866 0.06219 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 60.06622) ## ## Null deviance: 120769 on 1970 degrees of freedom ## Residual deviance: 118090 on 1966 degrees of freedom ## AIC: 13673 ## ## Number of Fisher Scoring iterations: 2 b_lm = coef(oldmort_lm) 5.4.3 Interpretation What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? How would you compare the time to death between two groups of people below? Is the effect additive or multiplicative? What is the estimated time to death for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated time to death for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.5 Weibull model 5.5.1 Model specification \\[h(t)=h_0 (t)\\exp(b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR})\\] The full hazard function for the Weibull PH model is \\[h(t)=\\exp(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)pt^{p-1}\\] Therefore, in terms of \\(S(t)\\), \\[ S(t)=\\exp(-(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)t^p) \\] \\(p \\; (0&lt;p)\\) is a shape parameter. 5.5.2 Estimation # Models oldmort_wei &lt;- phreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01, dist = &quot;weibull&quot;) # Table #print(summary(oldmort_wei), digits = 4) oldmort_wei ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + region + imr.birth, ## data = oldmort01, dist = &quot;weibull&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.406 0 1 (reference) ## female 0.594 -0.185 0.831 0.046 0.000 ## region ## town 0.111 0 1 (reference) ## industry 0.326 0.223 1.250 0.087 0.010 ## rural 0.563 0.065 1.067 0.087 0.456 ## imr.birth 15.162 0.005 1.005 0.007 0.452 ## ## log(scale) 4.362 0.019 0.000 ## log(shape) 2.083 0.027 0.000 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -7281.2 ## LR test statistic 31.82 ## Degrees of freedom 4 ## Overall p-value 2.08157e-06 b_wei &lt;- coef(oldmort_wei) expb_wei &lt;- exp(coef(oldmort_wei)) # Plots par(mfrow = c(1, 2), las = 1) plot(oldmort_wei, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(oldmort_wei, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_wei)) ## sexfemale regionindustry regionrural imr.birth log(scale) ## 0.8308202 1.2499929 1.0666983 1.0050654 78.4097920 ## log(shape) ## 8.0304270 5.5.3 Interpretations What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(\\exp(b_i) = 1\\), 2) \\(\\exp(b_i) &lt; 1\\), or 3) \\(\\exp(b_i) &gt; 1\\)? How would you compare \\(h(time\\;to\\;death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.6 Exponential model 5.6.1 Model specification \\[h(t)=h_0 (t)\\exp(b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR})\\] Exponential model is a specific case of Weibull family when \\(p\\)=1. The full hazard function is \\[h(t)=\\exp(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)pt^{p-1}=\\exp(b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)\\] Therefore, in terms of \\(S(t)\\), \\[ S(t)=\\exp(-(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)t^p)=\\exp(-(b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)t) \\] 5.6.2 Estimation # Models oldmort_exp &lt;- phreg(Surv(enter, exit, event) ~ sex + region + imr.birth, shape=1, data = oldmort01, dist = &quot;weibull&quot;) # Table #print(summary(oldmort_wei), digits = 4) oldmort_exp ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + region + imr.birth, ## data = oldmort01, dist = &quot;weibull&quot;, shape = 1) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.406 0 1 (reference) ## female 0.594 -0.100 0.905 0.046 0.029 ## region ## town 0.111 0 1 (reference) ## industry 0.326 0.395 1.484 0.086 0.000 ## rural 0.563 0.167 1.182 0.086 0.051 ## imr.birth 15.162 0.003 1.003 0.007 0.621 ## ## log(scale) 3.178 0.145 0.000 ## ## Shape is fixed at 1 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -7774.3 ## LR test statistic 39.77 ## Degrees of freedom 4 ## Overall p-value 4.82752e-08 b_exp &lt;- coef(oldmort_exp) expb_exp &lt;- exp(coef(oldmort_exp)) # Plots par(mfrow = c(1, 2), las = 1) plot(oldmort_exp, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(oldmort_exp, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_exp)) ## sexfemale regionindustry regionrural imr.birth log(scale) ## 0.9052028 1.4838456 1.1817648 1.0032832 23.9906740 5.6.3 Interpretations What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(\\exp(b_i) = 1\\), 2) \\(\\exp(b_i) &lt; 1\\), or 3) \\(\\exp(b_i) &gt; 1\\)? How would you compare \\(h(time\\;to\\;death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.7 Gompertz model 5.7.1 Model specification \\[h(t)=h_0 (t)\\exp(b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR})\\] Gompertz model is characterized by an exponentially increasing hazard function with fixed rate \\(r\\) (\\(-\\infty &lt; r &lt; \\infty\\)). when \\(r &lt; 0\\), the hazard function \\(h\\) is decreasing “too fast” to define a proper survival function, and \\(r=0\\) gives the exponential distribution as a special case. And for each fixed \\(r\\), the family of distributions indexed by \\(p &gt; 0\\) constitutes a proportional hazards family of distributions, and the corresponding regression model is written as Göran Broström, https://cran.r-project.org/web/packages/eha/vignettes/gompertz.html \\[h(t)=\\exp(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)pe^{rt}\\] 5.7.2 Estimation # Models oldmort_gomp &lt;- phreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01, dist = &quot;gompertz&quot;) # Table #print(summary(parm), digits = 4) oldmort_gomp ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + region + imr.birth, ## data = oldmort01, dist = &quot;gompertz&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.406 0 1 (reference) ## female 0.594 -0.188 0.829 0.046 0.000 ## region ## town 0.111 0 1 (reference) ## industry 0.326 0.222 1.248 0.087 0.011 ## rural 0.563 0.067 1.069 0.087 0.438 ## imr.birth 15.162 0.005 1.005 0.007 0.433 ## ## log(scale) 2.353 0.030 0.000 ## log(shape) -7.410 0.286 0.000 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -7280.6 ## LR test statistic 31.79 ## Degrees of freedom 4 ## Overall p-value 2.11245e-06 b_gomp &lt;- coef(oldmort_gomp) expb_gomp &lt;- exp(coef(oldmort_gomp)) # Plots par(mfrow = c(1, 2), las = 1) plot(oldmort_gomp, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(oldmort_gomp, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_gomp)) ## sexfemale regionindustry regionrural imr.birth log(scale) ## 8.286745e-01 1.248116e+00 1.069401e+00 1.005278e+00 1.051873e+01 ## log(shape) ## 6.053369e-04 5.7.3 Interpretations What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(\\exp(b_i) = 1\\), 2) \\(\\exp(b_i) &lt; 1\\), or 3) \\(\\exp(b_i) &gt; 1\\)? How would you compare \\(h(time\\;to\\;death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.8 Graphs The following figures summarize cumulative hazard curves by different survival models. # Plots par(mfrow = c(2, 2), las = 1) plot(oldmort_cox, fn = &quot;cum&quot;, main = &quot;Cox&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;&quot;, #xlim=c(0, 1) ylim=c(0, 10) ) plot(oldmort_wei, fn = &quot;cum&quot;, main = &quot;Weibull&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;&quot;, #xlim=c(0, 1) ylim=c(0, 10) ) plot(oldmort_exp, fn = &quot;cum&quot;, main = &quot;Exponential&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;&quot;, #xlim=c(0, 1) ylim=c(0, 10) ) plot(oldmort_gomp, fn = &quot;cum&quot;, main = &quot;Gompertz&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;&quot;, #xlim=c(0, 1) ylim=c(0, 10) ) "],["cox-proportional-hazard-modeling.html", "Chapter 6 Cox Proportional Hazard Modeling 6.1 Setup working datasets 6.2 Cox Proportional Hazard Models: Example 6.3 Interpretation 6.4 Proportional hazard (PH) assumption 6.5 Extended Cox model 6.6 Evaluating the Proportional hazard (PH) assumption 6.7 Why we call this model as semi-parametric model? 6.8 Why the Cox PH model is so popular 6.9 Estimation of the Cox PH model using Maximum likelihood (ML) 6.10 More about Hazard ratio 6.11 Adjusted Survival Curves using the Cox PH model", " Chapter 6 Cox Proportional Hazard Modeling 6.1 Setup working datasets 6.2 Cox Proportional Hazard Models: Example 6.2.1 Cox model specification \\[h_{(t,X)} = h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i), \\;\\; \\text{where}\\; X = (X_1, X_2, \\cdots, X_p)\\] The following result was obtained by using coxreg from the eha package. oldmort_cox &lt;- coxreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) print(summary(oldmort_cox), digits = 4) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.0001 ## male 0.406 0 1 (reference) ## female 0.594 -0.185 0.831 0.046 ## region 0.0013 ## town 0.111 0 1 (reference) ## industry 0.326 0.225 1.252 0.087 ## rural 0.563 0.069 1.071 0.087 ## imr.birth 15.162 0.005 1.005 0.007 0.5009 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -13563 ## LR test statistic 31.41 ## Degrees of freedom 4 ## Overall p-value 2.52611e-06 The same results can be obtained by using coxph from the survival package. oldmort_cox &lt;- coxph(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) print(summary(oldmort_cox), digits = 4) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ sex + region + imr.birth, ## data = oldmort01) ## ## n= 6495, number of events= 1971 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## sexfemale -0.185247 0.830899 0.045913 -4.035 5.47e-05 *** ## regionindustry 0.224903 1.252201 0.087093 2.582 0.00981 ** ## regionrural 0.069031 1.071470 0.086627 0.797 0.42552 ## imr.birth 0.004548 1.004558 0.006742 0.675 0.49994 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## sexfemale 0.8309 1.2035 0.7594 0.9091 ## regionindustry 1.2522 0.7986 1.0557 1.4853 ## regionrural 1.0715 0.9333 0.9042 1.2697 ## imr.birth 1.0046 0.9955 0.9914 1.0179 ## ## Concordance= 0.545 (se = 0.008 ) ## Likelihood ratio test= 31.41 on 4 df, p=3e-06 ## Wald test = 31.7 on 4 df, p=2e-06 ## Score (logrank) test = 31.8 on 4 df, p=2e-06 We would prefer to have \\(HR &gt; 1\\) than \\(HR &lt; 1\\) to ease interpretation. oldmort01$male &lt;- relevel(oldmort01$sex, ref = &quot;female&quot;) oldmort_cox &lt;- coxph(Surv(enter, exit, event) ~ male + region + imr.birth, data = oldmort01) print(summary(oldmort_cox), digits = 4) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + region + imr.birth, ## data = oldmort01) ## ## n= 6495, number of events= 1971 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## malemale 0.185247 1.203516 0.045913 4.035 5.47e-05 *** ## regionindustry 0.224903 1.252201 0.087093 2.582 0.00981 ** ## regionrural 0.069031 1.071470 0.086627 0.797 0.42552 ## imr.birth 0.004548 1.004558 0.006742 0.675 0.49994 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## malemale 1.204 0.8309 1.0999 1.317 ## regionindustry 1.252 0.7986 1.0557 1.485 ## regionrural 1.071 0.9333 0.9042 1.270 ## imr.birth 1.005 0.9955 0.9914 1.018 ## ## Concordance= 0.545 (se = 0.008 ) ## Likelihood ratio test= 31.41 on 4 df, p=3e-06 ## Wald test = 31.7 on 4 df, p=2e-06 ## Score (logrank) test = 31.8 on 4 df, p=2e-06 The following code will extract coefficients and model fit statistics. cox_coef &lt;- summary(oldmort_cox)$coefficients cox_fit &lt;- rbind( &quot;Wald&quot; = oldmort_cox$wald.test, &quot;Score(log_rank)&quot; = oldmort_cox$score ) knitr::kable(cox_coef, digits=2) coef exp(coef) se(coef) z Pr(&gt;|z|) malemale 0.19 1.20 0.05 4.03 0.00 regionindustry 0.22 1.25 0.09 2.58 0.01 regionrural 0.07 1.07 0.09 0.80 0.43 imr.birth 0.00 1.00 0.01 0.67 0.50 knitr::kable(cox_fit, digits=2) Wald 31.7 Score(log_rank) 31.8 6.2.2 Model 1: No covariates oldmort_cox01 &lt;- coxreg(Surv(enter, exit, event) ~ sex , data = oldmort01) cox_coef01 &lt;- as.data.frame(summary(oldmort_cox01)$coefficients) cox_fit01 &lt;- rbind( &quot;Wald&quot; = oldmort_cox01$wald.test, &quot;Score(log_rank)&quot; = oldmort_cox01$score ) 6.2.3 Model 2: Categorical covariate: region oldmort_cox02 &lt;- coxreg(Surv(enter, exit, event) ~ sex + region , data = oldmort01) cox_coef02 &lt;- as.data.frame(summary(oldmort_cox02)$coefficients) cox_fit02 &lt;- rbind( &quot;Wald&quot; = oldmort_cox02$wald.test, &quot;Score(log_rank)&quot; = oldmort_cox02$score ) 6.2.4 Model 3: Continuous covariate: imr.birth oldmort_cox03 &lt;- coxreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) cox_coef03 &lt;- as.data.frame(summary(oldmort_cox03)$coefficients) cox_fit03 &lt;- rbind( &quot;Wald&quot; = oldmort_cox03$wald.test, &quot;Score(log_rank)&quot; = oldmort_cox03$score ) cox_coef &lt;- cbind(setDT(cox_coef01), setDT(cox_coef02), setDT(cox_coef03)) ## Warning in as.data.table.list(x, keep.rownames = keep.rownames, check.names = ## check.names, : Item 2 has 3 rows but longest item has 4; recycled with ## remainder. cox_fit &lt;- cbind(cox_fit01, cox_fit02, cox_fit03) knitr::kable(cox_coef, digits=2) coef exp(coef) se(coef) z Wald p coef exp(coef) se(coef) z Wald p coef exp(coef) se(coef) z Wald p -0.19 0.82 0.05 -4.23 0 -0.19 0.83 0.05 -4.07 0.00 -0.19 0.83 0.05 -4.03 0.00 -0.19 0.82 0.05 -4.23 0 0.21 1.24 0.08 2.50 0.01 0.22 1.25 0.09 2.58 0.01 -0.19 0.82 0.05 -4.23 0 0.05 1.05 0.08 0.62 0.53 0.07 1.07 0.09 0.80 0.43 -0.19 0.82 0.05 -4.23 0 -0.19 0.83 0.05 -4.07 0.00 0.00 1.00 0.01 0.67 0.50 knitr::kable(cox_fit, digits=2) Score(log_rank) 17.96 31.34 31.8 #fcox_coef &lt;- flextable(head(cox_coef)) #fcox_coef &lt;- flextable(head(cox_coef)) #fcox_coef &lt;- add_body_row( # fcox_coef, # values = c(&quot;&quot;, &quot;Model 1&quot;, &quot;Model 2&quot;, &quot;Model 3&quot;), # colwidths = c(1, 3, 3, 3), top = TRUE #) #fcox_coef 6.3 Interpretation Comparisons between the crude model (i.e., no confounders) and adjusted models Often used to assess if confounding effect exists Report both even if there is no difference of the model fits for crude and adjusted models test statistics: difference of -2LL / difference of d.f.s, under \\(\\chi^2\\) distributions First, let’s examine the model fit statistics. Global statistical significance of the model: The output gives p-values for three alternative tests for overall significance of the model: The likelihood-ratio test, Wald test, and score logrank statistics. These three methods are asymptotically equivalent. For large enough \\(N\\), they will give similar results. For small \\(N\\), they may differ somewhat. The Likelihood ratio test has better behavior for small sample sizes, so it is generally preferred. Wald statistics \\(z = \\frac{coef}{se(coef)}\\) is normally distributed Likelihood ratio (LR) statistics -2 Log likelihood (-2LL) “In general, the LR and Wald statistics may not give exactly the same answer. Statisticians have shown that of the two test procedures, the LR statistic has better statistical properties, so when in doubt, you should use the LR test.”(Kleinbaum DG, Klein M. Survival Analysis. Springer New York; 2012. doi:10.1007/978-1-4419-6646-9) Score (logrank) test Concordance () Now, let’s examine coefficients. Note that there is no \\(\\beta_0\\) term coef: log(Hazard Ratio): A positive sign means that the hazard (risk of death) is higher, and thus the prognosis worse, for subjects with higher values of that variable. For the 0 and 1 variable, the Cox model gives the hazard ratio (HR) for the second group relative to the first group. exp(coef): Hazard ratio (HR) (\\(exp(0.1978)=1.2187\\)), the hazard for the test group is 1.2 times the hazard for the standard group. As other regression outputs, we have point estimates, ses, \\(p\\)-values, and confidence intervals. Statistical significance: The column marked “\\(z\\)” gives the Wald statistic value. It corresponds to the ratio of each regression coefficient to its standard error (\\(z\\) = coef/se(coef)). The wald statistic evaluates, whether the beta (\\(\\beta\\)) coefficient of a given variable is statistically significantly different from 0. \\(p\\)-value or CI? (Greenland, S., Senn, S.J., Rothman, K.J. et al. Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. Eur J Epidemiol 31, 337–350 (2016). https://doi.org/10.1007/s10654-016-0149-3) 6.4 Proportional hazard (PH) assumption \\(h_0 (t)\\): baseline hazard is a function of \\(t\\) but not \\(X\\)’s When all the \\(X\\)’s are equal to 0, than the formula reduces to the baseline hazard function, \\(h_0 (t)\\) as \\(e^0 = 1\\) When no \\(X\\)’s are in the model, than the formula reduces to the baseline hazard function, \\(h_0 (t)\\). \\(exp(\\sum_{i=1}^p \\beta_i X_i)\\): the exponential component is a function of \\(X\\)’s but not \\(t\\) (i.e., \\(X\\)’s are time-independent variables) A time-independent variable is defined to be any variable whose value for a given individual does not change over time. (e.g., sex, race/ethnicity) It may be appropriate to treat Age or Height as time-independent in the analysis if their values do not change much over time or if the effect of such variables on survival risk depends essentially on the value at only one measurement. Recall that \\[\\hat{HR} = \\frac{\\hat{h} (t, X^*)}{\\hat{h} (t, X)} = \\frac{h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i^*)}{h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i)} = \\frac{\\exp(\\sum_{i=1}^p \\beta_i X_i^*)}{\\exp(\\sum_{i=1}^p \\beta_i X_i)} = \\exp[{\\sum_{i=1}^p \\hat{\\beta_i}(X_i^* - X_i)}]\\] Notice that the baseline hazard function \\(h_0 (t)\\) appears in both the numerator and denominator of the hazard ratio and cancels out of the formula. The final expression for the hazard ratio therefore involves the estimated coefficients \\(\\hat{\\beta_i}\\) and the values of \\(X^*\\) and \\(X\\) for each variable. However, because the baseline hazard has canceled out, the final expression does not involve time \\(t\\). Thus, once the model is fitted and the values for \\(X^*\\) and \\(X\\) are specified, , which does not depend on time \\(t\\): \\[ \\hat{HR} = \\frac{\\hat{h} (t, X^*)}{\\hat{h} (t, X)} = exp[{\\sum_{i=1}^p \\hat{\\beta_i}(X_i^* - X_i)}] = \\theta\\;\\; \\text{therefore,} \\hat{h} (t, X^*) = \\hat{\\theta}\\hat{h} (t, X)\\] - - In the Cox PH model with 0 and 1 for {X_1}, \\(\\hat{\\theta}=e^{\\hat{\\beta}}\\) - When the PH assumption is in appropriate (e.g., the hazards cross), a Cox PH model is inappropriate and alternative model (e.g., extended Cox model) should be used 6.5 Extended Cox model It is possible to consider \\(X\\)’s which do involve \\(t\\), so that \\(X\\)s are called time-dependent variables. The extended Cox model no longer satisfies the proportional hazard assumption. 6.6 Evaluating the Proportional hazard (PH) assumption The Cox PH model assumes that the hazard ratio comparing any two specifications of predictors is constant over time. Equivalently, this means that the hazard for one individual is proportional to the hazard for any other individual, where the proportionality constant is independent of time. The PH assumption is not met if the graph of the hazards cross for two or more categories of a predictor of interest. However, even if the hazard functions do not cross, it is possible that the PH assumption is not met. Thus, rather than checking for crossing hazards, we must use other approaches to evaluate the reasonableness of the PH assumption. 6.6.1 Graphical evaluation Comparing estimated –ln(–ln) survivor curves over different (combinations of) categories of variables assessing the PH assumption for variables one-at-a-time, or 2) assessing the PH assumption after adjusting for other variables. Parallel curves, say comparing males with females, indicate that the PH assumption is satisfied A log–log survival curve is simply a transformation of an estimated survival curve that results from taking the natural log of an estimated survival probability . Mathematically, we write a log–log curve as \\(-ln(-ln \\hat{S})\\). Note that the log of a probability such as \\(\\hat{S}\\) is always a negative number. Because we can only take logs of positive numbers, we need to negate the first log before taking the second log. The value for \\(-ln(-ln \\hat{S})\\) may be positive or negative, either of which is acceptable by definition, \\(-ln(-ln \\hat{S})= -ln (\\int_0^t h(u)du)\\) The scale of an estimated survival curve (\\(\\hat{S}\\)) ranges between 0 and 1, whereas the corresponding scale for a \\(-ln(-ln \\hat{S})\\) ranges between \\(-\\infty\\) and \\(+\\infty\\) By empirical plots, we mean that do not assume an underlying Cox model. Alternatively, one could plot . If observed and predicted curves are “visually” parallel, then the PH assumption is reasonable. How much parallel is parallel? Too subjective decision: assume PH is OK unless strong evidence of non-parallelism many categories data: different categorizations may give different graphical pictures Assessing the PH assumption after adjusting for other variables: rather than using Kaplan–Meier curves, make a comparison using adjusted log–log survival curves under the PH assumption for one predictor adjusted for other predictors Comparing observed with predicted survivor curves If for each category of the predictor being assessed, the observed and expected plots are “close” to one another, we then can conclude that the PH assumption is satisfied. “how close is close?” par(mfrow=c(1,3)) plot(survfit(Surv(enter, exit, event) ~ male, data = oldmort01), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Sex&quot;) plot(survfit(Surv(enter, exit, event) ~ civ, data = oldmort01), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;CIV&quot;) plot(survfit(Surv(enter, exit, event) ~ region, data = oldmort01), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Region&quot;) par(mfrow=c(1,3)) plot(survfit(Surv(enter, exit, event) ~ sex, data = child), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Sex&quot;) plot(survfit(Surv(enter, exit, event) ~ socBranch, data = child), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Soc Branch&quot;) plot(survfit(Surv(enter, exit, event) ~ illeg, data = child), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Illegal&quot;) 6.6.2 Goodness-of-fit (GOF) A nonsignificant (i.e., large) \\(p\\)-value from large sample \\(z\\) or \\(\\chi^2\\) statistics , say greater than 0.10, suggests that the PH assumption is reasonable, whereas a small \\(p\\)-value, say less than 0.05, suggests that the variable being tested does not satisfy this assumption. More objective decision using a statistical test than graphical evaluation Schoenfeld residuals The idea behind the statistical test is that if the PH assumption holds for a particular covariate then the Schoenfeld residuals for that covariate will not be related to survival time. For each predictor in the model, Schoenfeld residuals are defined for every subject who has an event. For example, consider a Cox PH model with three predictors: sex, region, and imr.birth. Then there are three Schoenfeld residuals defined for each subject who has an event, one for each of the three predictors. Three step process Step 1. Run a Cox PH model and obtain Schoenfeld residuals for each predictor. Step 2. Create a variable that ranks the order of failures. The subject who has the first (earliest) event gets a value of 1, the next gets a value of 2, and so on. Step 3. Test the correlation between the variables created in the first and second steps. The null hypothesis is that the correlation between the Schoenfeld residuals and ranked failure time is zero Rejection of the null hypothesis leads to a conclusion that the PH assumption is violated However, 1) a \\(p\\)-value can be driven by sample size; 2) A gross violation of the null assumption may not be statistically significant if the sample is very small; and 3) conversely, a slight violation of the null assumption may be highly significant if the sample is very large. cox.gof &lt;- coxph(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) res.zph &lt;- cox.zph(cox.gof, transform = c(&quot;km&quot;,&quot;rank&quot;,&quot;idenityt&quot;)[2]) res.zph ## chisq df p ## sex 5.25 1 0.0220 ## region 9.83 2 0.0073 ## imr.birth 6.54 1 0.0105 ## GLOBAL 18.96 4 0.0008 plot(res.zph) 6.6.3 Time-dependent variable approaches The Cox model is extended to contain product (i.e., interaction) terms involving the time-independent variable being assessed and some function of time. If the coefficient of the product term turns out to be significant, we can conclude that the PH assumption is violated. Using the above one-at-a-time model, we assess the PH assumption by testing for the significance of the product term. The null hypothesis is therefore “d equal to zero.” Note that if the null hypothesis is true, the model reduces to a Cox PH model containing the single variable X. The test can be carried out using . To assess the PH assumption for several predictors simultaneously, the form of the extended model is \\[h(t,X) =h_0(t) exp\\left[\\sum_{i=1}^p (\\beta_i X_i + \\delta_i (X_i \\times g_i(t)))\\right], \\text{ where } g_i(t) \\text{ is a function of time for } i^{th} \\text{ predictor}\\] - This model contains the predictors being assessed as main effect terms and also as product terms with some function of time. Note that different predictors may require different functions of time; hence, the notation \\(g_i (t)\\) is used to define the time function for the \\(i^{th}\\) predictor - With the above model, we test for the PH assumption simultaneously by assessing the null hypothesis that all the \\(\\delta_i\\) coefficients are equal to zero. This requires a likelihood ratio chi-square statistic with \\(p\\) degrees of freedom, where \\(p\\) denotes the number of predictors being assessed. The LR statistic computes the difference between the log likelihood statistic (i.e., \\(-2\\; ln\\; L\\)) for the PH model and the log likelihood statistic for the extended Cox model. Note that under the null hypothesis, the model reduces to the Cox PH model. If the above test is found to be significant, then we can conclude that the PH assumption is not satisfied for at least one of the predictors in the model. To determine which predictor(s) do not satisfy the PH assumption, we could proceed by backward elimination of nonsignificant product terms until a final model is attained. The primary drawback of the use of an extended Cox model for assessing the PH assumption concerns the choice of the functions \\(g_i (t)\\) for the time-dependent product terms in the model. This choice is typically not clear-cut, and it is possible that different choices, such as \\(g(t)\\) equal to \\(t\\) versus log \\(t\\) versus a heaviside function, may result in different conclusions about whether the PH assumption is satisfied. 6.6.4 Testing for Influential Observations Testing for Influential Observations{} To test influential observations or outliers, we can visualize either the deviance residuals or the dfbeta values type: the type of residuals to present on Y axis. Allowed values include one of c(“martingale”, “deviance”, “score”, “schoenfeld”, “dfbeta”, “dfbetas”, “scaledsch”, “partial”). It’s also possible to check outliers by visualizing the deviance residuals. The deviance residual is a normalized transform of the martingale residual. These residuals should be roughtly symmetrically distributed about zero with a standard deviation of 1. Positive values correspond to individuals that “died too soon” compared to expected survival times. Negative values correspond to individual that “lived too long”. Very large or small values are outliers, which are poorly predicted by the model. survminer::ggcoxdiagnostics(oldmort_cox03, type = &quot;martingale&quot;, linear.predictions = FALSE, ggtheme = theme_bw()) ## Warning: `gather_()` was deprecated in tidyr 1.2.0. ## ℹ Please use `gather()` instead. ## ℹ The deprecated feature was likely used in the ## survminer package. ## Please report the issue at ## &lt;https://github.com/kassambara/survminer/issues&gt;. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see ## where this warning was generated. ## `geom_smooth()` using formula = &#39;y ~ x&#39; survminer::ggcoxdiagnostics(oldmort_cox03, type = &quot;schoenfeld&quot;, linear.predictions = FALSE, ggtheme = theme_bw()) ## `geom_smooth()` using formula = &#39;y ~ x&#39; survminer::ggcoxdiagnostics(oldmort_cox03, type = &quot;dfbeta&quot;, linear.predictions = FALSE, ggtheme = theme_bw()) ## `geom_smooth()` using formula = &#39;y ~ x&#39; survminer::ggcoxdiagnostics(oldmort_cox03, type = &quot;deviance&quot;, linear.predictions = FALSE, ggtheme = theme_bw()) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 6.6.5 Testing for Non-linearlity Nonlinearity is not an issue for categorical variables, so we only examine plots of martingale residuals and partial residuals against a continuous variable. Martingale residuals may present any value in the range (\\(-\\infty,\\; +1\\)): a value of martinguale residuals near 1 represents individuals that “died too soon”, large negative values correspond to individuals that “lived too long”. ggcoxfunctional(Surv(enter, exit, event) ~ imr.birth + log(imr.birth) + sqrt(imr.birth), data = oldmort01) ## Warning: arguments formula is deprecated; will be removed in the next version; ## please use fit instead. 6.7 Why we call this model as semi-parametric model? The formulation of a likelihood function is based on the distribution of the outcome. The Cox PH model does not impose any assumption on the distribution of the outcome, time to event. It simply uses the observed order of the failure time. (thus, it is a partial likelihood) If any distributional assumption was imposed, then it is a parametric survival model. No assumption on \\(h_0(t)\\) + proportional hazard assumption 6.8 Why the Cox PH model is so popular The Cox PH model is a model, so that the results from using the Cox model will closely approximate the results for the parametric model. Even though the baseline hazard is not specified, reasonably good estimates of regression coefficients, hazard ratios of interest, and adjusted survival curves can be obtained for a wide variety of data situations. We would prefer to use a parametric model if we were sure of the correct model. However, we may not be completely certain that a given parametric model is appropriate. When in doubt, the Cox model is a “safe” choice. Along with “robustness”, the model specification of the Cox PH model has several good properties. The exponential part of this product ensures that . (vs. a linear model with negative coefficients) The measure of effect, which is called a hazard ratio, is calculated . With a minimum of assumption, we can obtain the primary information about a hazard ratio and a survival curve. As compared to logistic model, the Cox PH model incorporate the survival time and censoring information. 6.9 Estimation of the Cox PH model using Maximum likelihood (ML) As with logistic regression, the ML estimates of the Cox model parameters are derived by maximizing a likelihood function, usually denoted as \\(L\\) (e.g., \\(L(\\beta)\\)). \\(L\\) is a partial likelihood (rather than a complete likelihood function): considers probabilities only for subjects who fail does not consider probabilities for subjects who are censored More specifically, the model breaks down each failure time to calculate each likelihood, and then get the product of several likelihoods \\[L = L_1 \\times L_2 \\times L_3 \\times \\cdots \\times L_k = \\prod_{j=1}^k L_j, \\text{ where } L_j= \\text{portion of } L \\text{for the } j^{th} \\text{ failure time given the risk set of } R(t_{(f)})\\] Once \\(L\\) is obtained, \\(\\ln L\\) is maximized by solving \\(\\frac{\\delta \\ln L}{\\delta \\beta_i} = 0\\) for \\((i=1, 2, \\cdots, p)\\) (# of parameters) over iteration 6.10 More about Hazard ratio -Hazard ratio = \\(e^{\\hat{\\beta}}\\) - In general, a hazard ratio (HR) is defined as the hazard for one individual divided by the hazard for a different individual. The two individuals being compared can be distinguished by their values for the set of predictors, that is, the \\(X\\)’s vs. \\(X^*\\)’s. Therefore, \\[ \\hat{HR} = \\frac{\\hat{h} (t, X^*)}{\\hat{h} (t, X)} = \\frac{h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i^*)}{h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i)} = \\frac{\\exp(\\sum_{i=1}^p \\beta_i X_i^*)}{\\exp(\\sum_{i=1}^p \\beta_i X_i)} = \\exp[{\\sum_{i=1}^p \\hat{\\beta_i}(X_i^* - X_i)}]\\] - Example: When \\(X_1\\) denotes (0, 1) exposure status, then \\(X_1^*=1\\), \\(X_1=0\\), thus \\[\\hat{HR} = \\exp[{\\sum_{i=1}^p \\hat{\\beta_i}(X_i^* - X_i)}] = \\exp[\\hat{\\beta_1}(1-0)]= \\exp(\\hat{\\beta_1})\\] - Thus, the \\(X\\)’s are typically coded so that group with the larger hazard corresponds to \\(X^*\\), and the group with the smaller hazard corresponds to\\(X\\). 6.11 Adjusted Survival Curves using the Cox PH model Two primary quantities we are interested in the survival model are estimated hazard ratios esitmated surival curves In the Cox PH model, Hazard function: \\(h(t, X) = h_0(t) \\exp[\\sum_{i=1}^p \\beta_i X_i]\\) Survival function: \\(S(t, X) = [S_0(t)]^{\\exp[\\sum_{i=1}^p \\beta_i X_i]}\\) Therefore, estimated functions are Estimated Hazard function: \\(\\hat{h}(t, X) = \\hat{h}_0(t) \\exp[\\sum_{i=1}^p \\hat{\\beta_i} X_i]\\) Estimated survival function: \\(\\hat{S}(t, X) = [\\hat{S}_0(t)]^{ \\exp[\\sum_{i=1}^p \\hat{\\beta_i} X_i]}\\) To fit the estimated curves, a set of values for \\(X_i\\) should be specified. Most software uses the of \\(X\\)s to calculate the adjusted for covariates. "],["add-health-project.html", "Chapter 7 Add Health Project 7.1 Library 7.2 Access datasets 7.3 Load 5-wave sample 7.4 Long-form dataset (wave-person) 7.5 Exploring variables in Add Health 7.6 Outcomes, exposures, and confounders 7.7 Demographic variables - time-invariant 7.8 Merging datasets 7.9 Data management, recoding, and so on 7.10 Analytic approach", " Chapter 7 Add Health Project 7.1 Library library(tidyverse) library(data.table) library(lme4) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack library(Matrix) 7.2 Access datasets Let’s use the public datasets available at https://www.icpsr.umich.edu/web/ICPSR/studies/21600?archive=ICPSR&amp;q=21600 7.3 Load 5-wave sample First, each RDA dataset will be loaded and then saved as WAVE0X. After assigning a WAVE variable, we will keep the WAVE0X dataset and WX datasets only. #1st wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0001/21600-0001-Data.rda&quot;) wave01 &lt;- da21600.0001 wave01$wave &lt;- 1 rm(da21600.0001) w1 = subset(wave01, select = c(AID, wave)) #2nd wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0005/21600-0005-Data.rda&quot;) wave02 &lt;- da21600.0005 wave02$wave &lt;- 2 rm(da21600.0005) w2 = subset(wave02, select = c(AID, wave)) #3rd wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0008/21600-0008-Data.rda&quot;) wave03 &lt;- da21600.0008 wave03$wave &lt;- 3 rm(da21600.0008) w3 = subset(wave03, select = c(AID, wave)) #4th wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0022/21600-0022-Data.rda&quot;) wave04 &lt;- da21600.0022 wave04$wave &lt;- 4 rm(da21600.0022) w4 = subset(wave04, select = c(AID, wave)) # 5th wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0032/21600-0032-Data.rda&quot;) wave05 &lt;- da21600.0032 wave05$wave &lt;- 5 rm(da21600.0032) w5 = subset(wave05, select = c(AID, wave)) The complete list of respondents can be obtained by aggregating all WX datasets and then getting the unique AID. aaa &lt;- rbind(w1, w2, w3, w4, w5) AH01 &lt;- unique(subset(rbind(w1, w2, w3, w4, w5), select = c(AID))) It looks like the first wave contains all respondents - no addtional respondents were added. To check this observation, we will see both AID from the first wave and All matched. test01 &lt;- cbind(wave01, AH01, by=&quot;AID&quot;) Because the n didn’t change, we confirmed that WAVE01 contains all respondents of the study. 7.4 Long-form dataset (wave-person) Aggregating WX datasets will generate a long-form dataset per wave-person. lf01 &lt;- rbind(w1, w2, w3, w4, w5) lf &lt;- lf01[order(lf01$AID, lf01$wave), ] # 6504, 4834, 4882, 5114, 4196, 25530 We will save LF for the later use. 7.5 Exploring variables in Add Health Use “Variables” or other documentations at https://www.icpsr.umich.edu/web/ICPSR/studies/21600?archive=ICPSR&amp;q=21600 7.6 Outcomes, exposures, and confounders Let’s assume we are interested in the BMI trajectory, which calculation requires both weight and height in each wave. We also keep and rename exposures and confounders. This process requires getting back and force to add, rename, and remove a set of variables. To note, whenever keeping variables in your datasets, add AID and wave as default variables. It is a good practice to keep the variable names consistent - for example, variables for adolescence will have “a_”, while those for adolescence’s parents will have “p_”, and those of adolescence’s offspring will have “0_”. By keeping all variable selection in one place, you can minimize any confusions in managing variables later. So, it looks like either lbs or kg, cm or inch was used for weight and height. Also, there are multiple variables for each weight or height, requiring your further study about which one is better than others. Here, I will simply go with the following variables, using BMI formula at https://www.cdc.gov/healthyweight/assessing/bmi/childrens_BMI/childrens_BMI_formula.html#:~:text=The%20formula%20for%20BMI%20is,to%20convert%20this%20to%20meters.&amp;text=When%20using%20English%20measurements%2C%20pounds,2%20to%20kg%2Fm2. bmiwgt1 &lt;- wave01 %&gt;% select(AID, wave, &quot;a_srh&quot; = H1GH1, &quot;a_wgt_lbs&quot; = H1GH60, &quot;a_hgt_ft&quot; = H1GH59A, &quot;a_hgt_in&quot; = H1GH59B, &quot;a_weightimage&quot; = H1GH28, &quot;a_poorappetite&quot; = H1FS2) summary(bmiwgt1) ## AID wave a_srh a_wgt_lbs ## 57100270: 1 Min. :1 (1) (1) Excellent:1847 Min. : 50.0 ## 57101310: 1 1st Qu.:1 (2) (2) Very good:2608 1st Qu.:118.0 ## 57103171: 1 Median :1 (3) (3) Good :1605 Median :135.0 ## 57103869: 1 Mean :1 (4) (4) Fair : 408 Mean :141.1 ## 57104553: 1 3rd Qu.:1 (5) (5) Poor : 28 3rd Qu.:160.0 ## 57104649: 1 Max. :1 NA&#39;s : 8 Max. :360.0 ## (Other) :6498 NA&#39;s :156 ## a_hgt_ft a_hgt_in ## (4) (4) 4 feet: 214 (04) (4) 4 inches: 693 ## (5) (5) 5 feet:5448 (06) (6) 6 inches: 669 ## (6) (6) 6 feet: 758 (03) (3) 3 inches: 629 ## NA&#39;s : 84 (02) (2) 2 inches: 569 ## (05) (5) 5 inches: 562 ## (Other) :3287 ## NA&#39;s : 95 ## a_weightimage a_poorappetite ## (1) (1) Very underweight : 128 (0) (0) Never/rarely :4192 ## (2) (2) Slightly underweight : 935 (1) (1) Sometimes :1744 ## (3) (3) About the right weight:3381 (2) (2) A lot of the time : 410 ## (4) (4) Slightly overweight :1808 (3) (3) Most/all of the time: 141 ## (5) (5) Very overweight : 238 NA&#39;s : 17 ## NA&#39;s : 14 ## bmiwgt2 &lt;- wave02 %&gt;% select(AID, wave, &quot;a_srh&quot; = H2GH1, &quot;a_wgt_lbs&quot; = H2GH53, &quot;a_hgt_ft&quot; = H2WS16HF, &quot;a_hgt_in&quot; = H2WS16HI, &quot;a_weightimage&quot; = H2GH30, &quot;a_poorappetite&quot; = H2GH22) summary(bmiwgt2) ## AID wave a_srh a_wgt_lbs ## 57101310: 1 Min. :2 (1) (1) Excellent:1434 Min. : 60.0 ## 57103869: 1 1st Qu.:2 (2) (2) Very good:1923 1st Qu.:120.0 ## 57104649: 1 Median :2 (3) (3) Good :1179 Median :140.0 ## 57104676: 1 Mean :2 (4) (4) Fair : 286 Mean :145.7 ## 57109625: 1 3rd Qu.:2 (5) (5) Poor : 10 3rd Qu.:163.0 ## 57110897: 1 Max. :2 NA&#39;s : 2 Max. :350.0 ## (Other) :4828 NA&#39;s :86 ## a_hgt_ft a_hgt_in ## (4) (4) 4 feet: 88 (06) (6) 6 inches: 537 ## (5) (5) 5 feet:4077 (05) (5) 5 inches: 462 ## (6) (6) 6 feet: 638 (04) (4) 4 inches: 461 ## NA&#39;s : 31 (07) (7) 7 inches: 459 ## (02) (2) 2 inches: 415 ## (Other) :2469 ## NA&#39;s : 31 ## a_weightimage a_poorappetite ## (1) (1) Very underweight : 56 (0) (0) Never :2312 ## (2) (2) Slightly underweight : 697 (1) (1) Just a few times :1834 ## (3) (3) About the right weight:2576 (2) (2) About once a week: 508 ## (4) (4) Slightly overweight :1338 (3) (3) Almost every day : 136 ## (5) (5) Very overweight : 162 (4) (4) Every day : 42 ## NA&#39;s : 5 NA&#39;s : 2 ## bmiwgt3 &lt;- wave03 %&gt;% select(AID, wave, &quot;a_srh&quot; = H3GH1, &quot;a_wgt_lbs&quot; = H3DA44, &quot;a_hgt_ft&quot; = H3HGT_F, &quot;a_hgt_in&quot; = H3HGT_I) summary(bmiwgt3) ## AID wave a_srh a_wgt_lbs ## 57100270: 1 Min. :3 (1) (1) Excellent:1601 Min. : 80.0 ## 57101310: 1 1st Qu.:3 (2) (2) Very good:2000 1st Qu.:136.0 ## 57103869: 1 Median :3 (3) (3) Good :1055 Median :160.0 ## 57104676: 1 Mean :3 (4) (4) Fair : 206 Mean :168.5 ## 57109625: 1 3rd Qu.:3 (5) (5) Poor : 20 3rd Qu.:190.0 ## 57111071: 1 Max. :3 Max. :430.0 ## (Other) :4876 NA&#39;s :111 ## a_hgt_ft a_hgt_in ## (4) (4) 4 feet: 84 (04) (4) 4 inches: 470 ## (5) (5) 5 feet:3913 (06) (6) 6 inches: 458 ## (6) (6) 6 feet: 732 (02) (2) 2 inches: 448 ## (7) (7) 7 feet: 1 (03) (3) 3 inches: 424 ## NA&#39;s : 152 (07) (7) 7 inches: 409 ## (Other) :2518 ## NA&#39;s : 155 bmiwgt4 &lt;- wave04 %&gt;% select(AID, wave, &quot;a_srh&quot; = H4GH1, &quot;a_wgt_lbs&quot; = H4GH6, &quot;a_hgt_ft&quot; = H4GH5F, &quot;a_hgt_in&quot; = H4GH5I) summary(bmiwgt4) ## AID wave a_srh a_wgt_lbs ## 57101310: 1 Min. :4 (1) (1) Excellent: 979 Min. : 18.0 ## 57103869: 1 1st Qu.:4 (2) (2) Very good:1963 1st Qu.:150.0 ## 57109625: 1 Median :4 (3) (3) Good :1683 Median :178.0 ## 57111071: 1 Mean :4 (4) (4) Fair : 434 Mean :184.1 ## 57113943: 1 3rd Qu.:4 (5) (5) Poor : 55 3rd Qu.:211.0 ## 57117542: 1 Max. :4 Max. :525.0 ## (Other) :5108 NA&#39;s :79 ## a_hgt_ft a_hgt_in ## Min. :4.000 Min. : 0.000 ## 1st Qu.:5.000 1st Qu.: 2.000 ## Median :5.000 Median : 5.000 ## Mean :5.172 Mean : 5.386 ## 3rd Qu.:5.000 3rd Qu.: 8.000 ## Max. :6.000 Max. :11.000 ## NA&#39;s :6 NA&#39;s :10 bmiwgt5 &lt;- wave05 %&gt;% select(AID, wave, &quot;a_srh&quot; = H5ID1, &quot;a_wgt_lbs&quot; = H5ID3, &quot;a_hgt_ft&quot; = H5ID2F, &quot;a_hgt_in&quot; = H5ID2I) summary(bmiwgt5) ## AID wave a_srh a_wgt_lbs a_hgt_ft ## 57101310: 1 Min. :5 Min. :1.000 Min. :100.0 Min. : 4.000 ## 57111071: 1 1st Qu.:5 1st Qu.:2.000 1st Qu.:155.0 1st Qu.: 5.000 ## 57111786: 1 Median :5 Median :2.000 Median :185.0 Median : 5.000 ## 57113943: 1 Mean :5 Mean :2.473 Mean :192.2 Mean : 5.379 ## 57117997: 1 3rd Qu.:5 3rd Qu.:3.000 3rd Qu.:220.0 3rd Qu.: 5.000 ## 57118381: 1 Max. :5 Max. :5.000 Max. :400.0 Max. :98.000 ## (Other) :4190 NA&#39;s :4 NA&#39;s :25 NA&#39;s :2 ## a_hgt_in ## Min. : 0.0 ## 1st Qu.: 3.0 ## Median : 5.0 ## Mean : 10.6 ## 3rd Qu.: 8.0 ## Max. :998.0 ## NA&#39;s :6 typeof(bmiwgt5$a_hgt_ft) ## [1] &quot;double&quot; The “Rbind” function requires all datasets have a same numbers of columns. “setDT” and “fill=TRUE” are the functions from a “data.table” package that override this requirement. Now, we have created a long-form dataset (i.e., vars) from five sets of cross-sectional datasets. vars01 &lt;- rbind(setDT(bmiwgt1), setDT(bmiwgt2), setDT(bmiwgt3), setDT(bmiwgt4), setDT(bmiwgt5), fill=TRUE) vars &lt;- vars01[order(vars01$AID, vars01$wave), ] # 6504, 4834, 4882, 5114, 4196, 25530 7.7 Demographic variables - time-invariant demo_TI &lt;- wave01 %&gt;% select(AID, &quot;a_sex&quot; = BIO_SEX) 7.8 Merging datasets The following code merges a long-form dataset (i.e., vars) and a time-invariant dataset (i.e., demo_TI). Final01 &lt;- merge(vars, demo_TI, by = c(&quot;AID&quot;)) summary(Final01) ## AID wave a_srh a_wgt_lbs ## 57101310: 5 Min. :1.00 (2) (2) Very good:8494 Min. : 18.0 ## 57111071: 5 1st Qu.:1.00 (1) (1) Excellent:5861 1st Qu.:130.0 ## 57113943: 5 Median :3.00 (3) (3) Good :5522 Median :155.0 ## 57118381: 5 Mean :2.83 2 :1503 Mean :164.3 ## 57118943: 5 3rd Qu.:4.00 3 :1393 3rd Qu.:190.0 ## 57121404: 5 Max. :5.00 (Other) :2743 Max. :525.0 ## (Other) :25500 NA&#39;s : 14 NA&#39;s :457 ## a_hgt_ft a_hgt_in ## (5) (5) 5 feet:13438 (06) (6) 6 inches: 1664 ## 5 : 7504 (04) (4) 4 inches: 1624 ## (6) (6) 6 feet: 2128 (03) (3) 3 inches: 1451 ## 6 : 1664 (02) (2) 2 inches: 1432 ## (4) (4) 4 feet: 386 (07) (7) 7 inches: 1430 ## (Other) : 135 (Other) :17632 ## NA&#39;s : 275 NA&#39;s : 297 ## a_weightimage a_poorappetite ## (1) (1) Very underweight : 184 (0) (0) Never/rarely : 4192 ## (2) (2) Slightly underweight : 1632 (0) (0) Never : 2312 ## (3) (3) About the right weight: 5957 (1) (1) Just a few times : 1834 ## (4) (4) Slightly overweight : 3146 (1) (1) Sometimes : 1744 ## (5) (5) Very overweight : 400 (2) (2) About once a week: 508 ## NA&#39;s :14211 (Other) : 729 ## NA&#39;s :14211 ## a_sex ## (1) (1) Male :11874 ## (2) (2) Female:13654 ## NA&#39;s : 2 ## ## ## ## 7.9 Data management, recoding, and so on 7.9.1 BMI Alright, the following code does not work…. Final01$a_wgt_flag &lt;- ifelse(Final01$a_wgt_lbs &lt; 50, 1, 0) Final01$a_wgt_flag &lt;- ifelse(430 &lt; Final01$a_wgt_lbs, 1, 0) Final01$a_hgt_flag &lt;- ifelse(as.integer(Final01$a_hgt_ft) &lt; 4, 1, 0) Final01$a_hgt_flag &lt;- ifelse(95 &lt; as.integer(Final01$a_hgt_ft), 1, 0) Final01$a_hgt_flag &lt;- ifelse(95 &lt; as.integer(Final01$a_hgt_in), 1, 0) summary(Final01) ## AID wave a_srh a_wgt_lbs ## 57101310: 5 Min. :1.00 (2) (2) Very good:8494 Min. : 18.0 ## 57111071: 5 1st Qu.:1.00 (1) (1) Excellent:5861 1st Qu.:130.0 ## 57113943: 5 Median :3.00 (3) (3) Good :5522 Median :155.0 ## 57118381: 5 Mean :2.83 2 :1503 Mean :164.3 ## 57118943: 5 3rd Qu.:4.00 3 :1393 3rd Qu.:190.0 ## 57121404: 5 Max. :5.00 (Other) :2743 Max. :525.0 ## (Other) :25500 NA&#39;s : 14 NA&#39;s :457 ## a_hgt_ft a_hgt_in ## (5) (5) 5 feet:13438 (06) (6) 6 inches: 1664 ## 5 : 7504 (04) (4) 4 inches: 1624 ## (6) (6) 6 feet: 2128 (03) (3) 3 inches: 1451 ## 6 : 1664 (02) (2) 2 inches: 1432 ## (4) (4) 4 feet: 386 (07) (7) 7 inches: 1430 ## (Other) : 135 (Other) :17632 ## NA&#39;s : 275 NA&#39;s : 297 ## a_weightimage a_poorappetite ## (1) (1) Very underweight : 184 (0) (0) Never/rarely : 4192 ## (2) (2) Slightly underweight : 1632 (0) (0) Never : 2312 ## (3) (3) About the right weight: 5957 (1) (1) Just a few times : 1834 ## (4) (4) Slightly overweight : 3146 (1) (1) Sometimes : 1744 ## (5) (5) Very overweight : 400 (2) (2) About once a week: 508 ## NA&#39;s :14211 (Other) : 729 ## NA&#39;s :14211 ## a_sex a_wgt_flag a_hgt_flag ## (1) (1) Male :11874 Min. :0e+00 Min. :0 ## (2) (2) Female:13654 1st Qu.:0e+00 1st Qu.:0 ## NA&#39;s : 2 Median :0e+00 Median :0 ## Mean :2e-04 Mean :0 ## 3rd Qu.:0e+00 3rd Qu.:0 ## Max. :1e+00 Max. :0 ## NA&#39;s :457 NA&#39;s :297 7.9.2 Sampling weights 7.9.3 Multiple imputation 7.10 Analytic approach The following models are demonstration only - mostly, the models themselves do not make sense. 7.10.1 A linear regression with the current dataset lmer(a_wgt_lbs ~ as.numeric(a_srh) + a_sex + (1 | AID), data=Final01) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: a_wgt_lbs ~ as.numeric(a_srh) + a_sex + (1 | AID) ## Data: Final01 ## REML criterion at convergence: 248599 ## Random effects: ## Groups Name Std.Dev. ## AID (Intercept) 33.69 ## Residual 26.88 ## Number of obs: 25069, groups: AID, 6492 ## Fixed Effects: ## (Intercept) as.numeric(a_srh) a_sex(2) (2) Female ## 160.802 5.422 -25.999 "]]
