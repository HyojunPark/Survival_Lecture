[["index.html", "SOC6280: Survival Analysis: Practice Chapter 1 About", " SOC6280: Survival Analysis: Practice Hyojun Park 2023-09-12 Chapter 1 About This is an additional lecture notes for Survival Analysis course, drawn from multiple textbooks and materials. "],["bias-assessment.html", "Chapter 2 Bias assessment 2.1 Bias due to omitted confounders 2.2 Overadjustment bias 2.3 Total effect 2.4 Overadjustment 2.5 Logistic models", " Chapter 2 Bias assessment 2.1 Bias due to omitted confounders \\[y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_2 + \\dots + \\epsilon_i; \\;\\; for \\;\\; i=1, \\dots, n\\] where the errors \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) with independent and identically distributed (i.i.d.) Let’s assume the following association is true (i.e., gold standard) without any selection bias, measurement bias, and other unmeasured confoundings. N &lt;- 100000 C &lt;- rnorm(N) X &lt;- .5 * C + rnorm(N) Y &lt;- .3 * C + .4 * X + rnorm(N) 2.1.1 Gold standard With the correct model specification (i.e., \\(C\\) as a confounder), we get an unbiased estimate of \\(X\\) on \\(Y\\). # Gold standard glm.unbiased &lt;- glm(Y~X + C, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + C, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.0002749 0.0031611 -0.087 0.931 ## X 0.3946589 0.0031560 125.050 &lt;2e-16 *** ## C 0.3000315 0.0035321 84.944 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.99923) ## ## Null deviance: 140182 on 99999 degrees of freedom ## Residual deviance: 99920 on 99997 degrees of freedom ## AIC: 283716 ## ## Number of Fisher Scoring iterations: 2 2.1.2 Misspecified model: a confounder, \\(C\\), was omitted from the model By omitting \\(C\\), the estimate of \\(X\\) was biased either “away from” or “towards to” the null # C was omitted glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.0009031 0.0032731 -0.276 0.783 ## X 0.5139919 0.0029263 175.647 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.071321) ## ## Null deviance: 140182 on 99999 degrees of freedom ## Residual deviance: 107130 on 99998 degrees of freedom ## AIC: 290681 ## ## Number of Fisher Scoring iterations: 2 2.1.3 Bias “away from” or “towards to” the null? N &lt;- 100000 C &lt;- rnorm(N) X &lt;- -.5 * C + rnorm(N) Y &lt;- -.3 * C + .4 * X + rnorm(N) # C was omitted glm.unbiased &lt;- glm(Y~X + C, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + C, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.006081 0.003159 -1.925 0.0542 . ## X 0.395894 0.003151 125.635 &lt;2e-16 *** ## C -0.303350 0.003536 -85.791 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.9977328) ## ## Null deviance: 140645 on 99999 degrees of freedom ## Residual deviance: 99770 on 99997 degrees of freedom ## AIC: 283566 ## ## Number of Fisher Scoring iterations: 2 glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.005787 0.003273 -1.768 0.077 . ## X 0.516742 0.002921 176.927 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.071159) ## ## Null deviance: 140645 on 99999 degrees of freedom ## Residual deviance: 107114 on 99998 degrees of freedom ## AIC: 290666 ## ## Number of Fisher Scoring iterations: 2 2.1.4 A \\(C\\) is not a confounder on \\(X\\) and \\(Y\\) N &lt;- 100000 C &lt;- rnorm(N) X &lt;- rnorm(N) Y &lt;- .4 * X + rnorm(N) 2.1.5 Correct model specification: Without \\(C\\) glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.002195 0.003155 -0.696 0.486 ## X 0.399377 0.003137 127.296 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.99524) ## ## Null deviance: 115649 on 99999 degrees of freedom ## Residual deviance: 99522 on 99998 degrees of freedom ## AIC: 283315 ## ## Number of Fisher Scoring iterations: 2 2.1.6 Misspecified model with \\(C\\) glm.unbiased &lt;- glm(Y~X + C, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + C, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.002178 0.003155 -0.690 0.49 ## X 0.399385 0.003137 127.298 &lt;2e-16 *** ## C -0.003138 0.003156 -0.994 0.32 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.9952401) ## ## Null deviance: 115649 on 99999 degrees of freedom ## Residual deviance: 99521 on 99997 degrees of freedom ## AIC: 283316 ## ## Number of Fisher Scoring iterations: 2 2.1.7 A \\(C\\) is a colloder on \\(X\\) and \\(Y\\) N &lt;- 100000 X &lt;- rnorm(N) Y &lt;- .7 * X + rnorm(N) C &lt;- 1.2 * X + .6 * Y + rnorm(N) 2.1.8 Correct model specification: Without \\(C\\) glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.002388 0.003162 0.755 0.45 ## X 0.699838 0.003164 221.203 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.9996525) ## ## Null deviance: 148877 on 99999 degrees of freedom ## Residual deviance: 99963 on 99998 degrees of freedom ## AIC: 283757 ## ## Number of Fisher Scoring iterations: 2 2.1.9 Misspecified model with \\(C\\) This is one of examples of selection bias. For example, let’s say, \\(X\\) is Education, \\(Y\\) is income, and \\(C\\) is social welfare program. People at lower education (i.e., high risk group in terms of exposure) and lower income (i.e., higher risk group in terms of outcome) are more likely to register social welfare program. If survey was conducted based on the registered social welfare program, the “estimated” association from this “disproportionally selected” respondents are likely biased. glm.unbiased &lt;- glm(Y~X + C, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + C, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.003167 0.002712 1.168 0.24296 ## X -0.015152 0.004648 -3.260 0.00111 ** ## C 0.441405 0.002329 189.491 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.735545) ## ## Null deviance: 148877 on 99999 degrees of freedom ## Residual deviance: 73552 on 99997 degrees of freedom ## AIC: 253078 ## ## Number of Fisher Scoring iterations: 2 2.2 Overadjustment bias Please note that this is not a comprehensive example; only reflect one aspect of potential overadjustement bias. Let’s assume a model with \\(M\\) as a mediator. N &lt;- 100000 X &lt;- rnorm(N) M &lt;- .5 * X + rnorm(N) Y &lt;- .3 * X + .4 * M + rnorm(N) 2.3 Total effect glm.unbiased &lt;- glm(Y~X, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.003265 0.003408 -0.958 0.338 ## X 0.498483 0.003400 146.634 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.161286) ## ## Null deviance: 141096 on 99999 degrees of freedom ## Residual deviance: 116126 on 99998 degrees of freedom ## AIC: 298745 ## ## Number of Fisher Scoring iterations: 2 2.4 Overadjustment glm.unbiased &lt;- glm(Y~X + M, family=&quot;gaussian&quot;) summary(glm.unbiased) ## ## Call: ## glm(formula = Y ~ X + M, family = &quot;gaussian&quot;) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.003290 0.003166 -1.039 0.299 ## X 0.300176 0.003528 85.084 &lt;2e-16 *** ## M 0.398780 0.003163 126.061 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.002054) ## ## Null deviance: 141096 on 99999 degrees of freedom ## Residual deviance: 100202 on 99997 degrees of freedom ## AIC: 283998 ## ## Number of Fisher Scoring iterations: 2 2.5 Logistic models 2.5.1 Sex as a Confounder, \\(C\\) MYY &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;Yes&quot;, Cancer = 1, freq = 5 ) MYN &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;Yes&quot;, Cancer = 0, freq = 8 ) MNY &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;No&quot;, Cancer = 1, freq = 45 ) MNN &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;No&quot;, Cancer = 0, freq = 72 ) FYY &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;Yes&quot;, Cancer = 1, freq = 25 ) FYN &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;Yes&quot;, Cancer = 0, freq = 10 ) FNY &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;No&quot;, Cancer = 1, freq = 25 ) FNN &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;No&quot;, Cancer = 0, freq = 10 ) Ex_confounder &lt;- rbind(MYY, MYN, MNY, MNN, FYY, FYN, FNY, FNN) Convert Freq table to raw data library(tidyr) raw_confounder &lt;- Ex_confounder %&gt;% uncount(freq) glm.unbiased &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_confounder) summary(glm.unbiased) ## ## Call: ## glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_confounder) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1582 0.1627 -0.972 0.3309 ## SmokingYes 0.6690 0.3397 1.970 0.0489 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 277.26 on 199 degrees of freedom ## Residual deviance: 273.28 on 198 degrees of freedom ## AIC: 277.28 ## ## Number of Fisher Scoring iterations: 4 Full model: glm_logit &lt;- glm(Cancer ~ Smoking + Sex , family=binomial(link = &quot;logit&quot;), data=raw_confounder) glm_logit ## ## Call: glm(formula = Cancer ~ Smoking + Sex, family = binomial(link = &quot;logit&quot;), ## data = raw_confounder) ## ## Coefficients: ## (Intercept) SmokingYes SexMale ## 9.163e-01 4.266e-15 -1.386e+00 ## ## Degrees of Freedom: 199 Total (i.e. Null); 197 Residual ## Null Deviance: 277.3 ## Residual Deviance: 257 AIC: 263 Stratified models ## For males raw_confounder_M &lt;- raw_confounder[ which(raw_confounder$Sex==&#39;Male&#39;), ] glm_logit_m &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_confounder_M) glm_logit_m ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_confounder_M) ## ## Coefficients: ## (Intercept) SmokingYes ## -4.700e-01 6.672e-16 ## ## Degrees of Freedom: 129 Total (i.e. Null); 128 Residual ## Null Deviance: 173.2 ## Residual Deviance: 173.2 AIC: 177.2 # For females raw_confounder_F &lt;- raw_confounder[ which(raw_confounder$Sex==&#39;Female&#39;), ] glm_logit_f &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_confounder_F) glm_logit_f ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_confounder_F) ## ## Coefficients: ## (Intercept) SmokingYes ## 9.163e-01 9.400e-16 ## ## Degrees of Freedom: 69 Total (i.e. Null); 68 Residual ## Null Deviance: 83.76 ## Residual Deviance: 83.76 AIC: 87.76 2.5.2 Sex as a Moderator, \\(M\\) MYY &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;Yes&quot;, Cancer = 1, freq = 5 ) MYN &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;Yes&quot;, Cancer = 0, freq = 4 ) MNY &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;No&quot;, Cancer = 1, freq = 45 ) MNN &lt;- data.frame(Sex = &quot;Male&quot;, Smoking = &quot;No&quot;, Cancer = 0, freq = 68 ) FYY &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;Yes&quot;, Cancer = 1, freq = 25 ) FYN &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;Yes&quot;, Cancer = 0, freq = 14 ) FNY &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;No&quot;, Cancer = 1, freq = 25 ) FNN &lt;- data.frame(Sex = &quot;Female&quot;, Smoking = &quot;No&quot;, Cancer = 0, freq = 14 ) Ex_moderator &lt;- rbind(MYY, MYN, MNY, MNN, FYY, FYN, FNY, FNN) Convert Freq table to raw data library(tidyr) raw_moderator &lt;- Ex_moderator %&gt;% uncount(freq) Full model: glm_logit &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_moderator) glm_logit ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_moderator) ## ## Coefficients: ## (Intercept) SmokingYes ## -0.1582 0.6690 ## ## Degrees of Freedom: 199 Total (i.e. Null); 198 Residual ## Null Deviance: 277.3 ## Residual Deviance: 273.3 AIC: 277.3 Stratified models ## For males raw_moderator_M &lt;- raw_moderator[ which(raw_moderator$Sex==&#39;Male&#39;), ] glm_logit_m &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_moderator_M) glm_logit_m ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_moderator_M) ## ## Coefficients: ## (Intercept) SmokingYes ## -0.4128 0.6360 ## ## Degrees of Freedom: 121 Total (i.e. Null); 120 Residual ## Null Deviance: 165.1 ## Residual Deviance: 164.3 AIC: 168.3 # For females raw_moderator_F &lt;- raw_moderator[ which(raw_moderator$Sex==&#39;Female&#39;), ] glm_logit_f &lt;- glm(Cancer ~ Smoking , family=binomial(link = &quot;logit&quot;), data=raw_moderator_F) glm_logit_f ## ## Call: glm(formula = Cancer ~ Smoking, family = binomial(link = &quot;logit&quot;), ## data = raw_moderator_F) ## ## Coefficients: ## (Intercept) SmokingYes ## 5.798e-01 -2.621e-16 ## ## Degrees of Freedom: 77 Total (i.e. Null); 76 Residual ## Null Deviance: 101.8 ## Residual Deviance: 101.8 AIC: 105.8 "],["survival-analyses-introduction.html", "Chapter 3 Survival Analyses: Introduction 3.1 Set packages and library 3.2 dataset 3.3 Nonparametric estimation 3.4 Proportional Hazards and Cox Regression 3.5 Parametric estimation", " Chapter 3 Survival Analyses: Introduction 3.1 Set packages and library 3.2 dataset The child dataset in eha package summary(child) # descriptive statistics ## id m.id sex socBranch ## Min. : 9 Min. : 55 male :13676 official: 610 ## 1st Qu.:249504 1st Qu.:248826 female:12898 farming :18641 ## Median :500126 Median :504920 business: 318 ## Mean :500080 Mean :501874 worker : 7005 ## 3rd Qu.:750266 3rd Qu.:752827 ## Max. :999976 Max. :999932 ## birthdate enter exit event illeg ## Min. :1850-01-01 Min. :0 Min. : 0.003 Min. :0.0000 no :24567 ## 1st Qu.:1861-01-05 1st Qu.:0 1st Qu.:15.000 1st Qu.:0.0000 yes: 2007 ## Median :1870-08-08 Median :0 Median :15.000 Median :0.0000 ## Mean :1869-06-09 Mean :0 Mean :12.231 Mean :0.2113 ## 3rd Qu.:1878-05-08 3rd Qu.:0 3rd Qu.:15.000 3rd Qu.:0.0000 ## Max. :1884-12-31 Max. :0 Max. :15.000 Max. :1.0000 ## m.age cohort ## Min. :15.83 Min. :-10.000 ## 1st Qu.:27.18 1st Qu.: 1.000 ## Median :31.79 Median : 10.000 ## Mean :32.03 Mean : 8.943 ## 3rd Qu.:36.74 3rd Qu.: 18.000 ## Max. :50.86 Max. : 24.000 ## SurvObj.start SurvObj.stop SurvObj.status ## Min. :0 Min. : 0.003000 Min. :0.0000000 ## 1st Qu.:0 1st Qu.:15.000000 1st Qu.:0.0000000 ## Median :0 Median :15.000000 Median :0.0000000 ## Mean :0 Mean :12.231114 Mean :0.2113344 ## 3rd Qu.:0 3rd Qu.:15.000000 3rd Qu.:0.0000000 ## Max. :0 Max. :15.000000 Max. :1.0000000 str(child) # structure ## &#39;data.frame&#39;: 26574 obs. of 12 variables: ## $ id : int 9 150 158 178 263 342 363 393 408 486 ... ## $ m.id : int 246606 377744 118277 715337 978617 282943 341341 840879 586140 564736 ... ## $ sex : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 1 1 1 1 2 1 1 1 2 2 ... ## $ socBranch: Factor w/ 4 levels &quot;official&quot;,&quot;farming&quot;,..: 2 2 4 2 4 2 2 2 2 2 ... ## $ birthdate: Date, format: &quot;1853-05-23&quot; &quot;1853-07-19&quot; ... ## $ enter : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exit : num 15 15 15 15 0.559 0.315 15 15 15 15 ... ## $ event : num 0 0 0 0 1 1 0 0 0 0 ... ## $ illeg : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ m.age : num 35 30.6 29.3 41.2 42.1 ... ## $ cohort : num -7 -7 1 12 -5 -5 5 6 7 -2 ... ## $ SurvObj : &#39;Surv&#39; num [1:26574, 1:3] (0,15.000+] (0,15.000+] (0,15.000+] (0,15.000+] ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:3] &quot;start&quot; &quot;stop&quot; &quot;status&quot; ## ..- attr(*, &quot;type&quot;)= chr &quot;counting&quot; head(child) # preview ## id m.id sex socBranch birthdate enter exit event illeg m.age ## 3 9 246606 male farming 1853-05-23 0 15.000 0 no 35.009 ## 42 150 377744 male farming 1853-07-19 0 15.000 0 no 30.609 ## 47 158 118277 male worker 1861-11-17 0 15.000 0 no 29.320 ## 54 178 715337 male farming 1872-11-16 0 15.000 0 no 41.183 ## 78 263 978617 female worker 1855-07-19 0 0.559 1 no 42.138 ## 102 342 282943 male farming 1855-09-29 0 0.315 1 no 32.931 ## cohort SurvObj ## 3 -7 (0,15.000+] ## 42 -7 (0,15.000+] ## 47 1 (0,15.000+] ## 54 12 (0,15.000+] ## 78 -5 (0, 0.559] ## 102 -5 (0, 0.315] 3.3 Nonparametric estimation 3.3.1 Data for nonparametric models The following code creates a set of vector for survival analysis. It contains 5 individuals’ survival time. \\(1\\) is an event (i.e., failure, death) and \\(0\\) is a cencored case. tt &lt;- c(7,6,6,5,2,4) cens &lt;- c(0,1,0,0,1,1) Surv(tt,cens) ## [1] 7+ 6 6+ 5+ 2 4 aaa &lt;- Surv(tt,cens) # demonstration only for checking how survival dataset was constructed aaa ## [1] 7+ 6 6+ 5+ 2 4 3.3.2 Kaplan-Meier estimator ## Models result.km &lt;- survfit(Surv(tt,cens)~1, conf.type=&quot;log-log&quot;) ## Table result.km ## Call: survfit(formula = Surv(tt, cens) ~ 1, conf.type = &quot;log-log&quot;) ## ## n events median 0.95LCL 0.95UCL ## [1,] 6 3 6 2 NA summary(result.km) ## Call: survfit(formula = Surv(tt, cens) ~ 1, conf.type = &quot;log-log&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 2 6 1 0.833 0.152 0.2731 0.975 ## 4 5 1 0.667 0.192 0.1946 0.904 ## 6 3 1 0.444 0.222 0.0662 0.785 ## Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. plot(result.km, ylab = &quot;Survival probability&quot;, xlab = &quot;Time&quot;, mark.time = T, main=&quot;KM survival curve&quot;) abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) plot(result.km, ylab = &quot;Cumulative hazard&quot;, xlab = &quot;Time&quot;, mark.time = T, fun=&quot;cumhaz&quot;, main=&quot;KM cumulative hazard curve&quot;) abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) 3.3.3 Nelson-Aalen estimator ## Models result.fh &lt;- survfit(Surv(tt,cens)~1, conf.type=&quot;log-log&quot;, type=&quot;fh&quot;) ## Table result.fh ## Call: survfit(formula = Surv(tt, cens) ~ 1, conf.type = &quot;log-log&quot;, ## type = &quot;fh&quot;) ## ## n events median 0.95LCL 0.95UCL ## [1,] 6 3 6 2 NA summary(result.fh) ## Call: survfit(formula = Surv(tt, cens) ~ 1, conf.type = &quot;log-log&quot;, ## type = &quot;fh&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 2 6 1 0.846 0.141 0.306 0.977 ## 4 5 1 0.693 0.180 0.229 0.913 ## 6 3 1 0.497 0.210 0.101 0.807 # Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. plot(result.fh, ylab = &quot;Survival probability&quot;, xlab = &quot;Time&quot;, mark.time = T, main=&quot;NA survival curve&quot;) abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) plot(result.fh, ylab = &quot;Cumulative hazard&quot;, xlab = &quot;Time&quot;, mark.time = T, fun=&quot;cumhaz&quot;, main=&quot;NA cumulative hazard curve&quot;) abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) 3.3.4 Comparisons by groups bysex &lt;- survfit(Surv(enter, exit, event) ~ sex, data=child, conf.type=&quot;log-log&quot;) ## Tables #bysex #summary(bysex) summary(bysex, times=c(0, 3, 6, 9, 12, 15)) # add time points ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = child, ## conf.type = &quot;log-log&quot;) ## ## sex=male ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 13676 0 1.000 0.00000 1.000 1.000 ## 3 11614 1924 0.859 0.00298 0.853 0.865 ## 6 10955 555 0.818 0.00331 0.811 0.824 ## 9 10653 240 0.800 0.00344 0.793 0.806 ## 12 10452 146 0.789 0.00351 0.782 0.795 ## 15 10269 120 0.780 0.00356 0.773 0.786 ## ## sex=female ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 12898 0 1.000 0.00000 1.000 1.000 ## 3 11152 1611 0.875 0.00292 0.869 0.880 ## 6 10578 501 0.835 0.00328 0.829 0.842 ## 9 10262 242 0.816 0.00343 0.809 0.823 ## 12 10079 129 0.806 0.00350 0.799 0.813 ## 15 9872 148 0.794 0.00358 0.787 0.801 ## plots plot(bysex, ylab = &quot;Survival probabilities&quot;, xlab = &quot;Survival time&quot;, #mark.time = T, main=&quot;Kaplan-Meier survival curve estimate with 95% CIs&quot; ) legend(&quot;topright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty=c(&quot;solid&quot;,&quot;dashed&quot;), col=c(&quot;black&quot;,&quot;red&quot;)) 3.3.5 Better KM figures library(ggfortify) library(ggplot2) autoplot(bysex, ylab = &quot;Survival probabilities&quot;, xlab = &quot;Survival time&quot;, #mark.time = T, main=&quot;Kaplan-Meier survival curve estimate with 95% CIs&quot; ) 3.3.6 Nonparametric models using a \\(child\\) dataset from eha ## Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. with(child, plot(Surv(enter, exit, event), fun = &quot;cumhaz&quot;, main = &quot;Cumulativa hazards function&quot;, xlab = &quot;Duration&quot;)) with(child, plot(Surv(enter, exit, event), main = &quot;Survival function&quot;, xlab = &quot;Duration&quot;)) 3.4 Proportional Hazards and Cox Regression cox01 &lt;- coxreg(Surv(enter, exit, event) ~ sex + socBranch + birthdate, data = child) print(summary(cox01), digits = 4) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.0019 ## male 0.510 0 1 (reference) ## female 0.490 -0.083 0.920 0.027 ## socBranch 0.0001 ## official 0.021 0 1 (reference) ## farming 0.710 -0.017 0.983 0.092 ## business 0.011 0.330 1.391 0.141 ## worker 0.258 0.099 1.104 0.094 ## birthdate 1869-07-13 -0.000 1.000 0.000 0.0000 ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -56481 ## LR test statistic 67.10 ## Degrees of freedom 5 ## Overall p-value 4.11227e-13 child$cohort &lt;- floor(toTime(child$birthdate)) # age cohort cox02 &lt;- coxreg(Surv(enter, exit, event) ~ sex + socBranch + cohort, data = child) print(summary(cox02), digits = 4) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.0018 ## male 0.510 0 1 (reference) ## female 0.490 -0.083 0.920 0.027 ## socBranch 0.0001 ## official 0.021 0 1 (reference) ## farming 0.710 -0.017 0.984 0.092 ## business 0.011 0.330 1.390 0.141 ## worker 0.258 0.099 1.104 0.094 ## cohort 1869.035 -0.008 0.992 0.001 0.0000 ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -56481 ## LR test statistic 66.79 ## Degrees of freedom 5 ## Overall p-value 4.75731e-13 range(child$cohort) ## [1] 1850 1884 child$cohort &lt;- child$cohort - 1860 cox03 &lt;- coxreg(Surv(enter, exit, event) ~ sex + socBranch + cohort, data = child) # Table summary(cox03) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.002 ## male 0.510 0 1 (reference) ## female 0.490 -0.083 0.920 0.027 ## socBranch 0.000 ## official 0.021 0 1 (reference) ## farming 0.710 -0.017 0.984 0.092 ## business 0.011 0.330 1.390 0.141 ## worker 0.258 0.099 1.104 0.094 ## cohort 9.035 -0.008 0.992 0.001 0.000 ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -56481 ## LR test statistic 66.79 ## Degrees of freedom 5 ## Overall p-value 4.75731e-13 # Plots par(mfrow = c(1, 2), las = 1) plot(cox03, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(cox03, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) 3.4.1 A visual check for a proportionality assumption library(survival) ## Create survival vector for fish dataset child$SurvObj &lt;- with(child, Surv(enter, exit, event)) par(mfrow = c(1, 2), las = 1) plot(survfit(SurvObj ~ sex, data=child), main = &quot;Proportional hazard by sex&quot;, ylab = &quot;Survival&quot;, col=c(&quot;black&quot;, &quot;red&quot;) ) plot(survfit(SurvObj ~ sex, data=child), fun = &quot;cloglog&quot;, ylab = &quot;Log-log survival&quot;, main = &quot;Proportional hazard by sex&quot;, col=c(&quot;black&quot;, &quot;red&quot;) ) library(survival) ## Create survival vector for fish dataset child$SurvObj &lt;- with(child, Surv(enter, exit, event)) par(mfrow = c(1, 2), las = 1) plot(survfit(SurvObj ~ socBranch, data=child), main = &quot;Proportional hazard by sex&quot;, ylab = &quot;Survival&quot;, col=c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;) ) plot(survfit(SurvObj ~ socBranch, data=child), fun = &quot;cloglog&quot;, ylab = &quot;Log-log survival&quot;, main = &quot;Proportional hazard by sex&quot;, col=c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;) ) 3.5 Parametric estimation 3.5.1 Weibull model # Models parm_weib &lt;- phreg(Surv(enter, exit, event) ~ sex + socBranch + cohort , dist = &quot;weibull&quot;, data = child) # Table #print(summary(parm), digits = 4) parm_weib ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + socBranch + ## cohort, data = child, dist = &quot;weibull&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.510 0 1 (reference) ## female 0.490 -0.083 0.920 0.027 0.002 ## socBranch ## official 0.021 0 1 (reference) ## farming 0.710 -0.026 0.975 0.092 0.780 ## business 0.011 0.332 1.393 0.141 0.019 ## worker 0.258 0.092 1.097 0.094 0.329 ## cohort 9.035 -0.008 0.992 0.001 0.000 ## ## log(scale) 5.887 0.228 0.000 ## log(shape) -0.880 0.013 0.000 ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -25131 ## LR test statistic 68.69 ## Degrees of freedom 5 ## Overall p-value 1.91736e-13 # Plots par(mfrow = c(1, 2), las = 1) plot(parm_weib, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(parm_weib, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) 3.5.2 Gompertz model # Models parm_gomp &lt;- phreg(Surv(enter, exit, event) ~ sex + socBranch + cohort , dist = &quot;gompertz&quot;, data = child) # Table #print(summary(parm), digits = 4) parm_gomp ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + socBranch + ## cohort, data = child, dist = &quot;gompertz&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.510 0 1 (reference) ## female 0.490 -0.087 0.916 NA NA ## socBranch ## official 0.021 0 1 (reference) ## farming 0.710 -0.064 0.938 NA NA ## business 0.011 0.349 1.417 NA NA ## worker 0.258 0.066 1.068 NA NA ## cohort 9.035 -0.008 0.992 NA NA ## ## log(scale) 401.049 NA NA ## log(shape) 397.124 NA NA ## ## Events 5616 ## Total time at risk 325030 ## Max. log. likelihood -28368 ## LR test statistic 78.89 ## Degrees of freedom 5 ## Overall p-value 1.44329e-15 # Plots par(mfrow = c(1, 2), las = 1) plot(parm_gomp, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(parm_gomp, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) "],["counting-process.html", "Chapter 4 Counting Process 4.1 Set packages and library 4.2 Example 4.3 Practice: AddHealth Public datasets", " Chapter 4 Counting Process In this section, we will cover 1) survival data structure (i.e., counting process) and 2) modeling survival data. 4.1 Set packages and library 4.2 Example “The oldmort dataset in eha package contains life histories of people followed from their 60th birthday to their 100th, or until death, born between June 28, 1765 and December 31, 1820 in Skellefteå. The variable enter is age at start of the given interval, exit contains the age at the end of the interval. We need to calculate follow-up time since age 60 - 60 is subtracted from enter and exit. The variable event is an indicator of death at the duration given by exit.” https://www.rdocumentation.org/packages/eha/versions/2.8.5/topics/oldmort [Göran Broström, http://ehar.se/r/ehar2/parametric.html] Here are the summary of the oldmort dataset. library(eha) oldmort01 &lt;- oldmort summary(oldmort01) # descriptive statistics ## id enter exit event ## Min. :765000603 Min. :60.00 Min. : 60.00 Mode :logical ## 1st Qu.:797001170 1st Qu.:60.00 1st Qu.: 63.88 FALSE:4524 ## Median :804001545 Median :60.07 Median : 68.51 TRUE :1971 ## Mean :803652514 Mean :64.07 Mean : 69.89 ## 3rd Qu.:812001564 3rd Qu.:66.88 3rd Qu.: 74.73 ## Max. :826002672 Max. :94.51 Max. :100.00 ## ## birthdate m.id f.id sex ## Min. :1765 Min. : 6039 Min. : 2458 male :2884 ## 1st Qu.:1797 1st Qu.:766000610 1st Qu.:763000610 female:3611 ## Median :1805 Median :775000742 Median :772000649 ## Mean :1804 Mean :771271398 Mean :762726961 ## 3rd Qu.:1812 3rd Qu.:783000743 3rd Qu.:780001077 ## Max. :1820 Max. :802000669 Max. :797001468 ## NA&#39;s :3155 NA&#39;s :3310 ## civ ses.50 birthplace imr.birth region ## unmarried: 557 middle : 233 parish:3598 Min. : 4.348 town : 657 ## married :3638 unknown:2565 region:1503 1st Qu.:12.709 industry:2214 ## widow :2300 upper : 55 remote:1394 Median :14.234 rural :3624 ## farmer :1562 Mean :15.209 ## lower :2080 3rd Qu.:17.718 ## Max. :31.967 ## str(oldmort01) # structure ## &#39;data.frame&#39;: 6495 obs. of 13 variables: ## $ id : int 765000603 765000669 768000648 770000562 770000707 771000617 771000619 771000638 771000670 772000622 ... ## $ enter : num 94.5 94.3 91.1 89 90 ... ## $ exit : num 95.8 95.8 91.9 89.6 90.2 ... ## $ event : logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ birthdate : num 1765 1766 1769 1771 1770 ... ## $ m.id : int NA NA NA NA NA NA NA NA NA NA ... ## $ f.id : int NA NA NA NA NA NA NA NA NA NA ... ## $ sex : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 2 2 2 2 2 2 1 2 2 2 ... ## $ civ : Factor w/ 3 levels &quot;unmarried&quot;,&quot;married&quot;,..: 3 1 3 3 3 3 3 3 3 3 ... ## $ ses.50 : Factor w/ 5 levels &quot;middle&quot;,&quot;unknown&quot;,..: 2 2 2 2 1 2 5 2 2 2 ... ## $ birthplace: Factor w/ 3 levels &quot;parish&quot;,&quot;region&quot;,..: 3 1 1 1 2 1 2 1 1 1 ... ## $ imr.birth : num 22.2 17.7 12.7 16.9 12 ... ## $ region : Factor w/ 3 levels &quot;town&quot;,&quot;industry&quot;,..: 3 2 3 2 3 3 3 2 3 2 ... head(oldmort01) # preview ## id enter exit event birthdate m.id f.id sex civ ses.50 ## 1 765000603 94.510 95.813 TRUE 1765.490 NA NA female widow unknown ## 2 765000669 94.266 95.756 TRUE 1765.734 NA NA female unmarried unknown ## 3 768000648 91.093 91.947 TRUE 1768.907 NA NA female widow unknown ## 4 770000562 89.009 89.593 TRUE 1770.991 NA NA female widow unknown ## 5 770000707 89.998 90.211 TRUE 1770.002 NA NA female widow middle ## 6 771000617 88.429 89.762 TRUE 1771.571 NA NA female widow unknown ## birthplace imr.birth region ## 1 remote 22.20000 rural ## 2 parish 17.71845 industry ## 3 parish 12.70903 rural ## 4 parish 16.90544 industry ## 5 region 11.97183 rural ## 6 parish 13.08594 rural To check how this dataset is constructed, we will need to identify any duplicated id. dup01 &lt;- data.frame(table(oldmort01$id)) dup02 &lt;- dup01[order(-dup01$Freq), ] In the following example, please check When is a new record for the same id created? What are the time-invariant variables? What are the time-variant variables? What does it mean by “TRUE” or “FALSE” in event? How does the time of enter and exit connected with each other? What would happen if there is a gap between two records? dup03 &lt;- oldmort01[oldmort01$id %in% c(&quot;789000771&quot;, &quot;796001158&quot;), ] dup03 ## id enter exit event birthdate m.id f.id sex civ ses.50 ## 536 789000771 70.570 72.059 FALSE 1789.430 NA NA female married unknown ## 537 789000771 72.059 79.391 FALSE 1789.430 NA NA female married unknown ## 538 789000771 79.391 79.947 FALSE 1789.430 NA NA female widow unknown ## 539 789000771 80.010 83.750 FALSE 1789.430 NA NA female widow unknown ## 540 789000771 84.358 87.274 TRUE 1789.430 NA NA female unmarried unknown ## 1472 796001158 63.531 64.020 FALSE 1796.469 NA NA male married farmer ## 1473 796001158 64.020 65.020 FALSE 1796.469 NA NA male widow farmer ## 1474 796001158 65.020 70.019 FALSE 1796.469 NA NA male widow farmer ## 1475 796001158 70.019 80.021 FALSE 1796.469 NA NA male widow farmer ## 1476 796001158 80.021 83.531 FALSE 1796.469 NA NA male widow farmer ## birthplace imr.birth region ## 536 region 19.92337 rural ## 537 region 19.92337 rural ## 538 region 19.92337 rural ## 539 region 19.92337 rural ## 540 region 19.92337 rural ## 1472 parish 19.92337 rural ## 1473 parish 19.92337 rural ## 1474 parish 19.92337 rural ## 1475 parish 19.92337 rural ## 1476 parish 19.92337 rural 4.3 Practice: AddHealth Public datasets There are many ways to construct long-form datasets with counting process. The following procedure is just one way to achieve the goal. Here are a couple of things to construct a long-form dataset with counting process. In practice, measuring outcomes, exposures, confounders, and other variables involves a separate procedure for each one of variables. I personally prefer to divide each measurement as time-variant and time-invariant datasets, respectively. Two variables should be ALWAYS included in every single dataset you are working on - AID and wave (or any other Time variable). Datasets with time-invariant variables can be merged by AID, while those with time-variant variables need to be merged by AID and wave. Time-varying variables will be assigned a single variable name. Let’s say we are to use self-rated health with a variable name of SRH for five waves. The dataset should contain AID, wave, and SRH. The SRH in each wave should be assigned the same name, SRH, and the wave information will be on wave. This way, you can simply “stack up” all 5-wave data to construct the long-form datasets. First, each rda dataset will be loaded and then saved as WAVE0X. After assigning a wave variable for each of them, we will keep the WAVE0X dataset and WX datasets only. #1st wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0001/21600-0001-Data.rda&quot;) wave01 &lt;- da21600.0001 wave01$wave &lt;- 1 rm(da21600.0001) w1 = subset(wave01, select = c(AID, wave)) #2nd wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0005/21600-0005-Data.rda&quot;) wave02 &lt;- da21600.0005 wave02$wave &lt;- 2 rm(da21600.0005) w2 = subset(wave02, select = c(AID, wave)) #3rd wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0008/21600-0008-Data.rda&quot;) wave03 &lt;- da21600.0008 wave03$wave &lt;- 3 rm(da21600.0008) w3 = subset(wave03, select = c(AID, wave)) #4th wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0022/21600-0022-Data.rda&quot;) wave04 &lt;- da21600.0022 wave04$wave &lt;- 4 rm(da21600.0022) w4 = subset(wave04, select = c(AID, wave)) # 5th wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0032/21600-0032-Data.rda&quot;) wave05 &lt;- da21600.0032 wave05$wave &lt;- 5 rm(da21600.0032) w5 = subset(wave05, select = c(AID, wave)) The complete list of respondents can be obtained by aggregating all WX datasets and then getting the unique AID. The numbers of cases for both occasions are the same. It looks like the first wave contains all respondents - no additional respondents were added. To check this observation, we will see both AID from the first wave and All matched. Because the n didn’t change, we confirmed that WAVE01 contains all respondents of the study. AH01 &lt;- unique(subset(rbind(w1, w2, w3, w4, w5), select = c(AID))) test01 &lt;- cbind(wave01, AH01, by=&quot;AID&quot;) 4.3.1 Generate a complete framework with AID and wave (Optional) I personally prefer working with a “complete framework” containing all AID and wave. lf01 &lt;- rbind(w1, w2, w3, w4, w5) lf &lt;- lf01[order(lf01$AID, lf01$wave), ] Here is the merged (“stacked”) dataset. head(lf01) ## AID wave ## 1 57100270 1 ## 2 57101310 1 ## 3 57103171 1 ## 4 57103869 1 ## 5 57104553 1 ## 6 57104649 1 Sorting by AID and wave, we can easily identify the data structure by AID and wave. This lf dataset is what I call a “framework” of this data source, which is the one that will be used whenever combining or merging datasets. head(lf) ## AID wave ## 1 57100270 1 ## 11339 57100270 3 ## 2 57101310 1 ## 6505 57101310 2 ## 11340 57101310 3 ## 16221 57101310 4 Keep the number of cases (n = 25530) for your record. This number should be the number you expect whenever you merge or stack datasets. count(lf) ## n ## 1 25530 4.3.2 Time-variant variables from each wave In this practice, we will select and rename self-rated health (for all 5-wave) and appetite (only for \\(1^{st}\\) and \\(2^{nd}\\) waves) measures. srh1 &lt;- wave01 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H1GH1, &quot;a_poorappetite&quot; = H1FS2) srh2 &lt;- wave02 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H2GH1, &quot;a_poorappetite&quot; = H2GH22) srh3 &lt;- wave03 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H3GH1) srh4 &lt;- wave04 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H4GH1) srh5 &lt;- wave05 %&gt;% dplyr::select(AID, wave, &quot;a_srh&quot; = H5ID1) Please note that how to name the “temporary” datasets. I found that using the combination of ‘variable name + wave’ minimizes any confusions later. The ‘rbind’ function requires all datasets have a same numbers of columns. ‘setDT’ and ‘fill=TRUE’ are the functions from a ‘data.table’ package that override this requirement. Now, we have created a long-form dataset (i.e., srh_TV) from five sets of cross-sectional datasets. srh_TV01 &lt;- rbind(setDT(srh1), setDT(srh2), setDT(srh3), setDT(srh4), setDT(srh5), fill=TRUE) srh_TV &lt;- srh_TV01[order(srh_TV01$AID, srh_TV01$wave), ] # 6504, 4834, 4882, 5114, 4196, 25530 head(srh_TV) ## AID wave a_srh a_poorappetite ## 1: 57100270 1 (3) (3) Good (0) (0) Never/rarely ## 2: 57100270 3 (1) (1) Excellent &lt;NA&gt; ## 3: 57101310 1 (4) (4) Fair (1) (1) Sometimes ## 4: 57101310 2 (4) (4) Fair (4) (4) Every day ## 5: 57101310 3 (2) (2) Very good &lt;NA&gt; ## 6: 57101310 4 (3) (3) Good &lt;NA&gt; 4.3.3 Time-invariant By definition, when a variable is time-invariant, only one measure from any variable should be applied to all other waves. In this example, we select sex from the first wave (because of completeness), which will be applied to the whole long-form dataset. demo_TI &lt;- wave01 %&gt;% select(AID, &quot;a_sex&quot; = BIO_SEX) 4.3.4 Merging datasets Once you have selected, created, and modified all required variables by waves, stacking all waves datasets will generate a long-form dataset per wave-person as long as you have keep AID and wave variables for all datasets. In this example, we’ve created three datasets - lf (a framework), demo_TI (time-invariant), and srh_TV (time-variant). Framework + time-invariant (i.e., lf (a framework) and demo_TI (time-invariant)) Final01 &lt;- merge(lf, demo_TI, by = c(&quot;AID&quot;)) head(Final01) ## AID wave a_sex ## 1 57100270 1 (2) (2) Female ## 2 57100270 3 (2) (2) Female ## 3 57101310 1 (2) (2) Female ## 4 57101310 2 (2) (2) Female ## 5 57101310 3 (2) (2) Female ## 6 57101310 4 (2) (2) Female Framework + time-invariant + time-variant (i.e., Final01 + srh_TV (time-variant)) Final02 &lt;- merge(Final01, srh_TV, by = c(&quot;AID&quot;, &quot;wave&quot;)) head(Final02) ## AID wave a_sex a_srh a_poorappetite ## 1 57100270 1 (2) (2) Female (3) (3) Good (0) (0) Never/rarely ## 2 57100270 3 (2) (2) Female (1) (1) Excellent &lt;NA&gt; ## 3 57101310 1 (2) (2) Female (4) (4) Fair (1) (1) Sometimes ## 4 57101310 2 (2) (2) Female (4) (4) Fair (4) (4) Every day ## 5 57101310 3 (2) (2) Female (2) (2) Very good &lt;NA&gt; ## 6 57101310 4 (2) (2) Female (3) (3) Good &lt;NA&gt; 4.3.5 Define event, enter, and exit The event can be defined as your outcomes. Depending on the nature of outcomes, it could be a multiple or repetitive events, requiring more complex survival modeling with more assumptions. Using lag/lead(wave) Final03 &lt;- Final02 %&gt;% group_by(AID) %&gt;% dplyr::mutate( enter = lag(wave), exit = wave ) %&gt;% ungroup() Final03$enter[Final03$wave == 1 &amp; is.na(Final03$enter)] &lt;- 0 Because we used wave as an example, it may look more complicated than necessary - for example, we may simply use enter = exit - 1. However, this lag/lead function is required when working with the actual date which interval is not always equal to 1. "],["survival-models-specification-estimation-and-interpretation.html", "Chapter 5 Survival models: specification, estimation, and interpretation 5.1 Nonparametric models 5.2 Semi-parametric models: Cox Regression 5.3 Logistic regression 5.4 Linear regression 5.5 Weibull model 5.6 Exponential model 5.7 Gompertz model 5.8 Graphs", " Chapter 5 Survival models: specification, estimation, and interpretation Let’s think some some feasible models addressing how the survival varies by sex, region, and infant mortality of the cohort, using oldmort01 dataset. Here are some possible models depending on the outcome: Descriptive models for survival time Linear or Poisson regression on the ‘survival time’, which can be defined as the time of death (i.e., ‘exit’). We may need to subset only those who died, potentially resulting in considerable loss of data. Logistic regression for the event, death. How would you incorporate “survival time” in this model? Semiparametric survival regression models parametric survival regression models 5.1 Nonparametric models Let’s fit Kaplan-Meier (KM) and Nelson-Aalen (NA) estimators using the oldmort01 dataset from the eha package. Kaplan-Meier (KM) survival estimator ## KM bysex_KM &lt;- survfit(Surv(enter, exit, event) ~ sex, data=oldmort01, conf.type=&quot;log-log&quot;) ## Tables bysex_KM ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = oldmort01, ## conf.type = &quot;log-log&quot;) ## ## records n events median 0.95LCL 0.95UCL ## sex=male 2884 1390 854 75.1 74.3 75.6 ## sex=female 3611 1833 1117 76.7 76.1 77.1 ##summary(bysex) summary(bysex_KM, times=c(60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110)) # add time points ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = oldmort01, ## conf.type = &quot;log-log&quot;) ## ## sex=male ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 60 1390 0 1.00000 0.00000 1.000000 1.0000 ## 65 1017 183 0.85817 0.00974 0.837865 0.8761 ## 70 697 168 0.70232 0.01352 0.674905 0.7279 ## 75 428 184 0.50490 0.01576 0.473598 0.5353 ## 80 191 171 0.28095 0.01561 0.250745 0.3119 ## 85 55 105 0.10892 0.01237 0.086192 0.1346 ## 90 12 34 0.03311 0.00841 0.019374 0.0526 ## 95 1 8 0.00392 0.00381 0.000404 0.0197 ## ## sex=female ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 60 1833 0 1.0000 0.00000 1.000000 1.00000 ## 65 1416 166 0.9022 0.00723 0.886989 0.91541 ## 70 1034 205 0.7610 0.01094 0.738691 0.78161 ## 75 669 238 0.5736 0.01343 0.546771 0.59938 ## 80 318 236 0.3477 0.01415 0.320058 0.37547 ## 85 115 167 0.1527 0.01192 0.130179 0.17687 ## 90 27 80 0.0380 0.00705 0.025840 0.05359 ## 95 5 21 0.0075 0.00333 0.002894 0.01662 ## 100 1 4 0.0015 0.00150 0.000153 0.00812 Nelson-Aalen (NA) estimator ## NA bysex_NA &lt;- survfit(Surv(enter, exit, event) ~ sex, data=oldmort01, conf.type=&quot;log-log&quot;, type=&quot;fh&quot;) # an option for NA estimator ## Tables bysex_NA ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = oldmort01, ## conf.type = &quot;log-log&quot;, type = &quot;fh&quot;) ## ## records n events median 0.95LCL 0.95UCL ## sex=male 2884 1390 854 75.1 74.3 75.6 ## sex=female 3611 1833 1117 76.7 76.1 77.1 ##summary(bysex) summary(bysex_NA, times=c(60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110)) # add time points ## Call: survfit(formula = Surv(enter, exit, event) ~ sex, data = oldmort01, ## conf.type = &quot;log-log&quot;, type = &quot;fh&quot;) ## ## sex=male ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 60 1390 0 1.00000 0.00000 1.00000 1.0000 ## 65 1017 183 0.85822 0.00974 0.83793 0.8762 ## 70 697 168 0.70245 0.01352 0.67504 0.7280 ## 75 428 184 0.50514 0.01575 0.47385 0.5356 ## 80 191 171 0.28138 0.01561 0.25118 0.3123 ## 85 55 105 0.10962 0.01239 0.08685 0.1353 ## 90 12 34 0.03418 0.00849 0.02024 0.0538 ## 95 1 8 0.00583 0.00449 0.00101 0.0216 ## ## sex=female ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 60 1833 0 1.00000 0.00000 1.000000 1.00000 ## 65 1416 166 0.90220 0.00723 0.887022 0.91543 ## 70 1034 205 0.76103 0.01094 0.738776 0.78169 ## 75 669 238 0.57371 0.01342 0.546935 0.59953 ## 80 318 236 0.34799 0.01415 0.320348 0.37575 ## 85 115 167 0.15315 0.01193 0.130627 0.17734 ## 90 27 80 0.03863 0.00710 0.026391 0.05434 ## 95 5 21 0.00824 0.00347 0.003349 0.01759 ## 100 1 4 0.00228 0.00183 0.000381 0.00911 Overall survival and hazard curves for the population ## Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. with(oldmort01, plot(Surv(enter, exit, event), fun = &quot;cumhaz&quot;, main = &quot;Cumulativa hazards function&quot;, xlab = &quot;Duration&quot;)) with(oldmort01, plot(Surv(enter, exit, event), main = &quot;Survival function&quot;, xlab = &quot;Duration&quot;)) Comparison between Male and Female # Plots par(mfrow = c(1, 2))# Two panels, &quot;one row, two columns&quot;. plot(bysex_KM, ylab = &quot;Survival probability&quot;, xlab = &quot;Time&quot;, mark.time = T, main=&quot;Kaplan-Meier survival curve&quot;) legend(&quot;topleft&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty=c(&quot;solid&quot;,&quot;dashed&quot;), col=c(&quot;black&quot;,&quot;red&quot;)) #abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) plot(bysex_NA, ylab = &quot;Cumulative hazard&quot;, xlab = &quot;Time&quot;, mark.time = T, fun=&quot;cumhaz&quot;, main=&quot;Nelson-Aalen cumulative hazard curve&quot;) legend(&quot;topleft&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty=c(&quot;solid&quot;,&quot;dashed&quot;), col=c(&quot;black&quot;,&quot;red&quot;)) #abline(h = 0.5, col = &quot;sienna&quot;, lty = 3) For a better plot for comparisons library(ggfortify) library(ggplot2) autoplot(bysex_KM, ylab = &quot;Survival probabilities&quot;, xlab = &quot;Survival time&quot;, #mark.time = T, main=&quot;Kaplan-Meier survival curve estimate with 95% CIs&quot; ) 5.2 Semi-parametric models: Cox Regression 5.2.1 Model specification \\[h(t)=h_0 (t)\\exp(b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR})\\] 5.2.2 Estimation oldmort_cox &lt;- coxreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) print(summary(oldmort_cox), digits = 4) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.0001 ## male 0.406 0 1 (reference) ## female 0.594 -0.185 0.831 0.046 ## region 0.0013 ## town 0.111 0 1 (reference) ## industry 0.326 0.225 1.252 0.087 ## rural 0.563 0.069 1.071 0.087 ## imr.birth 15.162 0.005 1.005 0.007 0.5009 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -13563 ## LR test statistic 31.41 ## Degrees of freedom 4 ## Overall p-value 2.52611e-06 b_cox &lt;- coef(oldmort_cox) expb_cox &lt;- exp(coef(oldmort_cox)) # Plots par(mfrow = c(1, 2), las = 1) plot(oldmort_cox, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(oldmort_cox, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_cox)) ## sexfemale regionindustry regionrural imr.birth ## 0.8308988 1.2522007 1.0714696 1.0045585 5.2.3 Interpretations What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(\\exp(b_i) = 1\\), 2) \\(\\exp(b_i) &lt; 1\\), or 3) \\(\\exp(b_i) &gt; 1\\)? How would you compare \\(p(death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(p(death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(p(death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.3 Logistic regression To fit logistic regression, ‘death’ variable was created. oldmort01$death &lt;- ifelse(oldmort01$event == &quot;TRUE&quot;, 1, 0) 5.3.1 Model specification \\[ \\ln \\left( \\frac{p(y)}{1-p(y)} \\right) = b_0 + b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR}\\] 5.3.2 Estimation Logistic model was fitted as below. oldmort_log &lt;- glm(death ~ sex + region + imr.birth, data=oldmort01, family = binomial(link = &quot;logit&quot;)) summary(oldmort_log) ## ## Call: ## glm(formula = death ~ sex + region + imr.birth, family = binomial(link = &quot;logit&quot;), ## data = oldmort01) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.003482 0.166455 -6.029 1.65e-09 *** ## sexfemale 0.073055 0.054619 1.338 0.181050 ## regionindustry 0.378124 0.100516 3.762 0.000169 *** ## regionrural 0.111579 0.099098 1.126 0.260188 ## imr.birth -0.004146 0.007756 -0.534 0.593010 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 7972.9 on 6494 degrees of freedom ## Residual deviance: 7944.9 on 6490 degrees of freedom ## AIC: 7954.9 ## ## Number of Fisher Scoring iterations: 4 b_log = coef(oldmort_log) expb = exp(coef(oldmort_log)) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_log)) ## (Intercept) sexfemale regionindustry regionrural imr.birth ## 0.3666006 1.0757895 1.4595443 1.1180423 0.9958631 5.3.3 Interpretation What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(exp(b_i) = 1\\), 2) \\(exp(b_i) &lt; 1\\), or 3) \\(exp(b_i) &gt; 1\\)? How would you compare \\(p(death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(p(death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(p(death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.4 Linear regression 5.4.1 Model specification \\[ Y_{Time\\;to\\; death} = b_0 + b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR} \\] 5.4.2 Estimation To fit linear model, we need to subset data for the death and use ‘exit’ as an outcome. oldmort02 &lt;- oldmort01[oldmort01$death == 1,] oldmort_lm &lt;- glm(exit ~ sex + region + imr.birth, data=oldmort02, family = &quot;gaussian&quot;) summary(oldmort_lm) ## ## Call: ## glm(formula = exit ~ sex + region + imr.birth, family = &quot;gaussian&quot;, ## data = oldmort02) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 71.99777 1.12285 64.120 &lt; 2e-16 *** ## sexfemale 1.90403 0.35249 5.402 7.4e-08 *** ## regionindustry 2.07598 0.66429 3.125 0.00180 ** ## regionrural 1.80560 0.66531 2.714 0.00671 ** ## imr.birth -0.09621 0.05156 -1.866 0.06219 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 60.06622) ## ## Null deviance: 120769 on 1970 degrees of freedom ## Residual deviance: 118090 on 1966 degrees of freedom ## AIC: 13673 ## ## Number of Fisher Scoring iterations: 2 b_lm = coef(oldmort_lm) 5.4.3 Interpretation What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? How would you compare the time to death between two groups of people below? Is the effect additive or multiplicative? What is the estimated time to death for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated time to death for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.5 Weibull model 5.5.1 Model specification \\[h(t)=h_0 (t)\\exp(b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR})\\] The full hazard function for the Weibull PH model is \\[h(t)=\\exp(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)pt^{p-1}\\] Therefore, in terms of \\(S(t)\\), \\[ S(t)=\\exp(-(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)t^p) \\] \\(p \\; (0&lt;p)\\) is a shape parameter. 5.5.2 Estimation # Models oldmort_wei &lt;- phreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01, dist = &quot;weibull&quot;) # Table #print(summary(oldmort_wei), digits = 4) oldmort_wei ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + region + imr.birth, ## data = oldmort01, dist = &quot;weibull&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.406 0 1 (reference) ## female 0.594 -0.185 0.831 0.046 0.000 ## region ## town 0.111 0 1 (reference) ## industry 0.326 0.223 1.250 0.087 0.010 ## rural 0.563 0.065 1.067 0.087 0.456 ## imr.birth 15.162 0.005 1.005 0.007 0.452 ## ## log(scale) 4.362 0.019 0.000 ## log(shape) 2.083 0.027 0.000 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -7281.2 ## LR test statistic 31.82 ## Degrees of freedom 4 ## Overall p-value 2.08157e-06 b_wei &lt;- coef(oldmort_wei) expb_wei &lt;- exp(coef(oldmort_wei)) # Plots par(mfrow = c(1, 2), las = 1) plot(oldmort_wei, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(oldmort_wei, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_wei)) ## sexfemale regionindustry regionrural imr.birth log(scale) ## 0.8308202 1.2499929 1.0666983 1.0050654 78.4097920 ## log(shape) ## 8.0304270 5.5.3 Interpretations What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(\\exp(b_i) = 1\\), 2) \\(\\exp(b_i) &lt; 1\\), or 3) \\(\\exp(b_i) &gt; 1\\)? How would you compare \\(h(time\\;to\\;death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.6 Exponential model 5.6.1 Model specification \\[h(t)=h_0 (t)\\exp(b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR})\\] Exponential model is a specific case of Weibull family when \\(p\\)=1. The full hazard function is \\[h(t)=\\exp(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)pt^{p-1}=\\exp(b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)\\] Therefore, in terms of \\(S(t)\\), \\[ S(t)=\\exp(-(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)t^p)=\\exp(-(b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)t) \\] 5.6.2 Estimation # Models oldmort_exp &lt;- phreg(Surv(enter, exit, event) ~ sex + region + imr.birth, shape=1, data = oldmort01, dist = &quot;weibull&quot;) # Table #print(summary(oldmort_wei), digits = 4) oldmort_exp ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + region + imr.birth, ## data = oldmort01, dist = &quot;weibull&quot;, shape = 1) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.406 0 1 (reference) ## female 0.594 -0.100 0.905 0.046 0.029 ## region ## town 0.111 0 1 (reference) ## industry 0.326 0.395 1.484 0.086 0.000 ## rural 0.563 0.167 1.182 0.086 0.051 ## imr.birth 15.162 0.003 1.003 0.007 0.621 ## ## log(scale) 3.178 0.145 0.000 ## ## Shape is fixed at 1 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -7774.3 ## LR test statistic 39.77 ## Degrees of freedom 4 ## Overall p-value 4.82752e-08 b_exp &lt;- coef(oldmort_exp) expb_exp &lt;- exp(coef(oldmort_exp)) # Plots par(mfrow = c(1, 2), las = 1) plot(oldmort_exp, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(oldmort_exp, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_exp)) ## sexfemale regionindustry regionrural imr.birth log(scale) ## 0.9052028 1.4838456 1.1817648 1.0032832 23.9906740 5.6.3 Interpretations What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(\\exp(b_i) = 1\\), 2) \\(\\exp(b_i) &lt; 1\\), or 3) \\(\\exp(b_i) &gt; 1\\)? How would you compare \\(h(time\\;to\\;death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.7 Gompertz model 5.7.1 Model specification \\[h(t)=h_0 (t)\\exp(b_1\\times D_f + b_2 \\times D_{ind} + b_3 \\times D_{rural} + b_4 \\times X_{IMR})\\] Gompertz model is characterized by an exponentially increasing hazard function with fixed rate \\(r\\) (\\(-\\infty &lt; r &lt; \\infty\\)). when \\(r &lt; 0\\), the hazard function \\(h\\) is decreasing “too fast” to define a proper survival function, and \\(r=0\\) gives the exponential distribution as a special case. And for each fixed \\(r\\), the family of distributions indexed by \\(p &gt; 0\\) constitutes a proportional hazards family of distributions, and the corresponding regression model is written as Göran Broström, https://cran.r-project.org/web/packages/eha/vignettes/gompertz.html \\[h(t)=\\exp(b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)pe^{rt}\\] 5.7.2 Estimation # Models oldmort_gomp &lt;- phreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01, dist = &quot;gompertz&quot;) # Table #print(summary(parm), digits = 4) oldmort_gomp ## Call: ## phreg(formula = Surv(enter, exit, event) ~ sex + region + imr.birth, ## data = oldmort01, dist = &quot;gompertz&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## sex ## male 0.406 0 1 (reference) ## female 0.594 -0.188 0.829 0.046 0.000 ## region ## town 0.111 0 1 (reference) ## industry 0.326 0.222 1.248 0.087 0.011 ## rural 0.563 0.067 1.069 0.087 0.438 ## imr.birth 15.162 0.005 1.005 0.007 0.433 ## ## log(scale) 2.353 0.030 0.000 ## log(shape) -7.410 0.286 0.000 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -7280.6 ## LR test statistic 31.79 ## Degrees of freedom 4 ## Overall p-value 2.11245e-06 b_gomp &lt;- coef(oldmort_gomp) expb_gomp &lt;- exp(coef(oldmort_gomp)) # Plots par(mfrow = c(1, 2), las = 1) plot(oldmort_gomp, fn = &quot;sur&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;Survival&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) plot(oldmort_gomp, fn = &quot;cum&quot;, main = &quot;&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;hazard&quot;, #xlim=c(0, 1) #ylim=c(ymin, ymax) ) To ease interpretation, we exponentiate coefficients (and CIs). exp(coef(oldmort_gomp)) ## sexfemale regionindustry regionrural imr.birth log(scale) ## 8.286745e-01 1.248116e+00 1.069401e+00 1.005278e+00 1.051873e+01 ## log(shape) ## 6.053369e-04 5.7.3 Interpretations What is the metric of \\(y\\) and \\(b_i\\), respectively? Interpret \\(b_0, b_1,\\) and \\(b_2\\), respectively. What is the difference between coefficients and \\(\\exp\\)(coefficients)? Specify the metric. What is the interpretation when 1) \\(b_i = 0\\), 2) \\(b_i &lt; 0\\), or 3) \\(b_i &gt; 0\\)? What is the interpretation when 1) \\(\\exp(b_i) = 1\\), 2) \\(\\exp(b_i) &lt; 1\\), or 3) \\(\\exp(b_i) &gt; 1\\)? How would you compare \\(h(time\\;to\\;death)\\) between two groups of people below? Is the effect additive or multiplicative? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 0, and IMR = 0 vs. those who with sex = 1, region = 0, and IMR = 0? What is the estimated \\(h(time\\;to\\;death)\\) for those who with sex = 0, region = 2, and IMR = 90 vs. those who with sex = 1, region = 2, and IMR = 90? 5.8 Graphs The following figures summarize cumulative hazard curves by different survival models. # Plots par(mfrow = c(2, 2), las = 1) plot(oldmort_cox, fn = &quot;cum&quot;, main = &quot;Cox&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;&quot;, #xlim=c(0, 1) ylim=c(0, 10) ) plot(oldmort_wei, fn = &quot;cum&quot;, main = &quot;Weibull&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;&quot;, #xlim=c(0, 1) ylim=c(0, 10) ) plot(oldmort_exp, fn = &quot;cum&quot;, main = &quot;Exponential&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;&quot;, #xlim=c(0, 1) ylim=c(0, 10) ) plot(oldmort_gomp, fn = &quot;cum&quot;, main = &quot;Gompertz&quot;, #xlab=&quot;Duration (year)&quot;, ylab=&quot;&quot;, #xlim=c(0, 1) ylim=c(0, 10) ) "],["cox-proportional-hazard-modeling.html", "Chapter 6 Cox Proportional Hazard Modeling 6.1 Setup working datasets 6.2 Cox Proportional Hazard Models: Example 6.3 Interpretation 6.4 Proportional hazard (PH) assumption 6.5 Extended Cox model 6.6 Evaluating the Proportional hazard (PH) assumption", " Chapter 6 Cox Proportional Hazard Modeling 6.1 Setup working datasets library(tidyverse) library(knitr) library(kableExtra) library(eha) library(survival) library(data.table) library(flextable) library(survminer) library(ggfortify) library(ggplot2) library(data.table) library(coxme) library(broom) oldmort01 &lt;- oldmort 6.2 Cox Proportional Hazard Models: Example 6.2.1 Cox model specification \\[h_{(t,X)} = h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i), \\;\\; \\text{where}\\; X = (X_1, X_2, \\cdots, X_p)\\] The following result was obtained by using coxreg from the eha package. oldmort_cox &lt;- coxreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) print(summary(oldmort_cox), digits = 4) ## Covariate Mean Coef Rel.Risk S.E. LR p ## sex 0.0001 ## male 0.406 0 1 (reference) ## female 0.594 -0.185 0.831 0.046 ## region 0.0013 ## town 0.111 0 1 (reference) ## industry 0.326 0.225 1.252 0.087 ## rural 0.563 0.069 1.071 0.087 ## imr.birth 15.162 0.005 1.005 0.007 0.5009 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -13563 ## LR test statistic 31.41 ## Degrees of freedom 4 ## Overall p-value 2.52611e-06 The same results can be obtained by using coxph from the survival package. oldmort_cox &lt;- coxph(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) print(summary(oldmort_cox), digits = 4) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ sex + region + imr.birth, ## data = oldmort01) ## ## n= 6495, number of events= 1971 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## sexfemale -0.185247 0.830899 0.045913 -4.035 5.47e-05 *** ## regionindustry 0.224903 1.252201 0.087093 2.582 0.00981 ** ## regionrural 0.069031 1.071470 0.086627 0.797 0.42552 ## imr.birth 0.004548 1.004558 0.006742 0.675 0.49994 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## sexfemale 0.8309 1.2035 0.7594 0.9091 ## regionindustry 1.2522 0.7986 1.0557 1.4853 ## regionrural 1.0715 0.9333 0.9042 1.2697 ## imr.birth 1.0046 0.9955 0.9914 1.0179 ## ## Concordance= 0.545 (se = 0.008 ) ## Likelihood ratio test= 31.41 on 4 df, p=3e-06 ## Wald test = 31.7 on 4 df, p=2e-06 ## Score (logrank) test = 31.8 on 4 df, p=2e-06 We would prefer to have \\(HR &gt; 1\\) than \\(HR &lt; 1\\) to ease interpretation. oldmort01$male &lt;- relevel(oldmort01$sex, ref = &quot;female&quot;) oldmort_cox &lt;- coxph(Surv(enter, exit, event) ~ male + region + imr.birth, data = oldmort01) print(summary(oldmort_cox), digits = 4) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + region + imr.birth, ## data = oldmort01) ## ## n= 6495, number of events= 1971 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## malemale 0.185247 1.203516 0.045913 4.035 5.47e-05 *** ## regionindustry 0.224903 1.252201 0.087093 2.582 0.00981 ** ## regionrural 0.069031 1.071470 0.086627 0.797 0.42552 ## imr.birth 0.004548 1.004558 0.006742 0.675 0.49994 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## malemale 1.204 0.8309 1.0999 1.317 ## regionindustry 1.252 0.7986 1.0557 1.485 ## regionrural 1.071 0.9333 0.9042 1.270 ## imr.birth 1.005 0.9955 0.9914 1.018 ## ## Concordance= 0.545 (se = 0.008 ) ## Likelihood ratio test= 31.41 on 4 df, p=3e-06 ## Wald test = 31.7 on 4 df, p=2e-06 ## Score (logrank) test = 31.8 on 4 df, p=2e-06 The following code will extract coefficients and model fit statistics. cox_coef &lt;- summary(oldmort_cox)$coefficients cox_fit &lt;- rbind( &quot;Wald&quot; = oldmort_cox$wald.test, &quot;Score(log_rank)&quot; = oldmort_cox$score ) knitr::kable(cox_coef, digits=2) coef exp(coef) se(coef) z Pr(&gt;|z|) malemale 0.19 1.20 0.05 4.03 0.00 regionindustry 0.22 1.25 0.09 2.58 0.01 regionrural 0.07 1.07 0.09 0.80 0.43 imr.birth 0.00 1.00 0.01 0.67 0.50 knitr::kable(cox_fit, digits=2) Wald 31.7 Score(log_rank) 31.8 6.2.2 Model 1: No covariates oldmort_cox01 &lt;- coxreg(Surv(enter, exit, event) ~ sex , data = oldmort01) cox_coef01 &lt;- as.data.frame(summary(oldmort_cox01)$coefficients) cox_fit01 &lt;- rbind( &quot;Wald&quot; = oldmort_cox01$wald.test, &quot;Score(log_rank)&quot; = oldmort_cox01$score ) 6.2.3 Model 2: Categorical covariate: region oldmort_cox02 &lt;- coxreg(Surv(enter, exit, event) ~ sex + region , data = oldmort01) cox_coef02 &lt;- as.data.frame(summary(oldmort_cox02)$coefficients) cox_fit02 &lt;- rbind( &quot;Wald&quot; = oldmort_cox02$wald.test, &quot;Score(log_rank)&quot; = oldmort_cox02$score ) 6.2.4 Model 3: Continuous covariate: imr.birth oldmort_cox03 &lt;- coxreg(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) cox_coef03 &lt;- as.data.frame(summary(oldmort_cox03)$coefficients) cox_fit03 &lt;- rbind( &quot;Wald&quot; = oldmort_cox03$wald.test, &quot;Score(log_rank)&quot; = oldmort_cox03$score ) cox_coef &lt;- cbind(setDT(cox_coef01), setDT(cox_coef02), setDT(cox_coef03)) ## Warning in as.data.table.list(x, keep.rownames = keep.rownames, check.names = ## check.names, : Item 2 has 3 rows but longest item has 4; recycled with ## remainder. cox_fit &lt;- cbind(cox_fit01, cox_fit02, cox_fit03) knitr::kable(cox_coef, digits=2) coef exp(coef) se(coef) z Wald p coef exp(coef) se(coef) z Wald p coef exp(coef) se(coef) z Wald p -0.19 0.82 0.05 -4.23 0 -0.19 0.83 0.05 -4.07 0.00 -0.19 0.83 0.05 -4.03 0.00 -0.19 0.82 0.05 -4.23 0 0.21 1.24 0.08 2.50 0.01 0.22 1.25 0.09 2.58 0.01 -0.19 0.82 0.05 -4.23 0 0.05 1.05 0.08 0.62 0.53 0.07 1.07 0.09 0.80 0.43 -0.19 0.82 0.05 -4.23 0 -0.19 0.83 0.05 -4.07 0.00 0.00 1.00 0.01 0.67 0.50 knitr::kable(cox_fit, digits=2) Score(log_rank) 17.96 31.34 31.8 #fcox_coef &lt;- flextable(head(cox_coef)) #fcox_coef &lt;- flextable(head(cox_coef)) #fcox_coef &lt;- add_body_row( # fcox_coef, # values = c(&quot;&quot;, &quot;Model 1&quot;, &quot;Model 2&quot;, &quot;Model 3&quot;), # colwidths = c(1, 3, 3, 3), top = TRUE #) #fcox_coef 6.3 Interpretation Comparisons between the crude model (i.e., no confounders) and adjusted models Often used to assess if confounding effect exists Report both even if there is no difference of the model fits for crude and adjusted models test statistics: difference of -2LL / difference of d.f.s, under \\(\\chi^2\\) distributions First, let’s examine the model fit statistics. Global statistical significance of the model: The output gives p-values for three alternative tests for overall significance of the model: The likelihood-ratio test, Wald test, and score logrank statistics. These three methods are asymptotically equivalent. For large enough \\(N\\), they will give similar results. For small \\(N\\), they may differ somewhat. The Likelihood ratio test has better behavior for small sample sizes, so it is generally preferred. Wald statistics \\(z = \\frac{coef}{se(coef)}\\) is normally distributed Likelihood ratio (LR) statistics -2 Log likelihood (-2LL) “In general, the LR and Wald statistics may not give exactly the same answer. Statisticians have shown that of the two test procedures, the LR statistic has better statistical properties, so when in doubt, you should use the LR test.”(Kleinbaum DG, Klein M. Survival Analysis. Springer New York; 2012. doi:10.1007/978-1-4419-6646-9) Score (logrank) test Concordance () Now, let’s examine coefficients. Note that there is no \\(\\beta_0\\) term coef: log(Hazard Ratio): A positive sign means that the hazard (risk of death) is higher, and thus the prognosis worse, for subjects with higher values of that variable. For the 0 and 1 variable, the Cox model gives the hazard ratio (HR) for the second group relative to the first group. exp(coef): Hazard ratio (HR) (\\(exp(0.1978)=1.2187\\)), the hazard for the test group is 1.2 times the hazard for the standard group. As other regression outputs, we have point estimates, ses, \\(p\\)-values, and confidence intervals. Statistical significance: The column marked “\\(z\\)” gives the Wald statistic value. It corresponds to the ratio of each regression coefficient to its standard error (\\(z\\) = coef/se(coef)). The wald statistic evaluates, whether the beta (\\(\\beta\\)) coefficient of a given variable is statistically significantly different from 0. \\(p\\)-value or CI? (Greenland, S., Senn, S.J., Rothman, K.J. et al. Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. Eur J Epidemiol 31, 337–350 (2016). https://doi.org/10.1007/s10654-016-0149-3) 6.4 Proportional hazard (PH) assumption Mathematical expression1 \\[ h_x(t) = \\phi^x h_o(t),\\;\\; t&gt;0,\\;\\; x=0,1,\\;\\; \\phi&gt;0 \\] when \\(x=0\\), \\(h_0(t) = h_0(t)\\) and when \\(x=1\\), \\(h_0(t) = \\phi h_0(t)\\) When \\(\\beta = \\log(\\phi)\\), \\[ h_x(t) = h(t; x) = e^{\\beta x} h_o(t),\\;\\; t&gt;0,\\;\\; x=0,1,\\;\\; -\\infty &lt; \\beta &lt; \\infty \\] For multiple groups ($= (_1, _2, , _k) $), \\[ h_x(t) = h(t; x) = e^{x_1 \\beta_1 + x_2 + \\beta_2 + \\cdots + x_k \\beta_k} h_o(t) = h_0 (t)e^{x \\beta},\\;\\; t&gt;0,\\;\\; x=0,1,\\;\\; -\\infty &lt; \\beta &lt; \\infty \\] \\(h_0 (t)\\): baseline hazard is a function of \\(t\\) but not \\(X\\)’s When all the \\(X\\)’s are equal to 0, than the formula reduces to the baseline hazard function, \\(h_0 (t)\\) as \\(e^0 = 1\\) When no \\(X\\)’s are in the model, than the formula reduces to the baseline hazard function, \\(h_0 (t)\\). \\(exp(\\sum_{i=1}^p \\beta_i X_i)\\): the exponential component is a function of \\(X\\)’s but not \\(t\\) (i.e., \\(X\\)’s are time-independent variables) A time-independent variable is defined to be any variable whose value for a given individual does not change over time. (e.g., sex, race/ethnicity) It may be appropriate to treat Age or Height as time-independent in the analysis if their values do not change much over time or if the effect of such variables on survival risk depends essentially on the value at only one measurement. Recall that \\[\\hat{HR} = \\frac{\\hat{h} (t, X^*)}{\\hat{h} (t, X)} = \\frac{h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i^*)}{h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i)} = \\frac{\\exp(\\sum_{i=1}^p \\beta_i X_i^*)}{\\exp(\\sum_{i=1}^p \\beta_i X_i)} = \\exp[{\\sum_{i=1}^p \\hat{\\beta_i}(X_i^* - X_i)}]\\] Notice that the baseline hazard function \\(h_0 (t)\\) appears in both the numerator and denominator of the hazard ratio and cancels out of the formula. The final expression for the hazard ratio therefore involves the estimated coefficients \\(\\hat{\\beta_i}\\) and the values of \\(X^*\\) and \\(X\\) for each variable. However, because the baseline hazard has canceled out, the final expression does not involve time \\(t\\). Thus, once the model is fitted and the values for \\(X^*\\) and \\(X\\) are specified, , which does not depend on time \\(t\\): \\[ \\hat{HR} = \\frac{\\hat{h} (t, X^*)}{\\hat{h} (t, X)} = exp[{\\sum_{i=1}^p \\hat{\\beta_i}(X_i^* - X_i)}] = \\theta\\;\\; \\text{therefore,} \\hat{h} (t, X^*) = \\hat{\\theta}\\hat{h} (t, X)\\] - - In the Cox PH model with 0 and 1 for {X_1}, \\(\\hat{\\theta}=e^{\\hat{\\beta}}\\) - When the PH assumption is in appropriate (e.g., the hazards cross), a Cox PH model is inappropriate and alternative model (e.g., extended Cox model) should be used 6.5 Extended Cox model It is possible to consider \\(X\\)’s which do involve \\(t\\), so that \\(X\\)s are called time-dependent variables. The extended Cox model no longer satisfies the proportional hazard assumption. 6.6 Evaluating the Proportional hazard (PH) assumption The Cox PH model assumes that the hazard ratio comparing any two specifications of predictors is constant over time. Equivalently, this means that the hazard for one individual is proportional to the hazard for any other individual, where the proportionality constant is independent of time. The PH assumption is not met if the graph of the hazards cross for two or more categories of a predictor of interest. However, even if the hazard functions do not cross, it is possible that the PH assumption is not met. Thus, rather than checking for crossing hazards, we must use other approaches to evaluate the reasonableness of the PH assumption. 6.6.1 Graphical evaluation Comparing estimated –ln(–ln) survivor curves over different (combinations of) categories of variables assessing the PH assumption for variables one-at-a-time, or 2) assessing the PH assumption after adjusting for other variables. Parallel curves, say comparing males with females, indicate that the PH assumption is satisfied A log–log survival curve is simply a transformation of an estimated survival curve that results from taking the natural log of an estimated survival probability . Mathematically, we write a log–log curve as \\(-ln(-ln \\hat{S})\\). Note that the log of a probability such as \\(\\hat{S}\\) is always a negative number. Because we can only take logs of positive numbers, we need to negate the first log before taking the second log. The value for \\(-ln(-ln \\hat{S})\\) may be positive or negative, either of which is acceptable by definition, \\(-ln(-ln \\hat{S})= -ln (\\int_0^t h(u)du)\\) The scale of an estimated survival curve (\\(\\hat{S}\\)) ranges between 0 and 1, whereas the corresponding scale for a \\(-ln(-ln \\hat{S})\\) ranges between \\(-\\infty\\) and \\(+\\infty\\) By empirical plots, we mean that do not assume an underlying Cox model. Alternatively, one could plot . If observed and predicted curves are “visually” parallel, then the PH assumption is reasonable. How much parallel is parallel? Too subjective decision: assume PH is OK unless strong evidence of non-parallelism many categories data: different categorizations may give different graphical pictures Assessing the PH assumption after adjusting for other variables: rather than using Kaplan–Meier curves, make a comparison using adjusted log–log survival curves under the PH assumption for one predictor adjusted for other predictors Comparing observed with predicted survivor curves If for each category of the predictor being assessed, the observed and expected plots are “close” to one another, we then can conclude that the PH assumption is satisfied. “how close is close?” par(mfrow=c(1,3)) plot(survfit(Surv(enter, exit, event) ~ male, data = oldmort01), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Sex&quot;) plot(survfit(Surv(enter, exit, event) ~ civ, data = oldmort01), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;CIV&quot;) plot(survfit(Surv(enter, exit, event) ~ region, data = oldmort01), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Region&quot;) par(mfrow=c(1,3)) plot(survfit(Surv(enter, exit, event) ~ sex, data = child), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Sex&quot;) plot(survfit(Surv(enter, exit, event) ~ socBranch, data = child), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Soc Branch&quot;) plot(survfit(Surv(enter, exit, event) ~ illeg, data = child), col=c(&quot;black&quot;, &quot;red&quot;) , fun = &quot;cloglog&quot; , xlab = &quot;log(time)&quot; , ylab = &quot;log-log(survival)&quot; , main = &quot;Illegal&quot;) 6.6.2 Goodness-of-fit (GOF) A nonsignificant (i.e., large) \\(p\\)-value from large sample \\(z\\) or \\(\\chi^2\\) statistics , say greater than 0.10, suggests that the PH assumption is reasonable, whereas a small \\(p\\)-value, say less than 0.05, suggests that the variable being tested does not satisfy this assumption. More objective decision using a statistical test than graphical evaluation Schoenfeld residuals The idea behind the statistical test is that if the PH assumption holds for a particular covariate then the Schoenfeld residuals for that covariate will not be related to survival time. For each predictor in the model, Schoenfeld residuals are defined for every subject who has an event. For example, consider a Cox PH model with three predictors: sex, region, and imr.birth. Then there are three Schoenfeld residuals defined for each subject who has an event, one for each of the three predictors. Three step process Step 1. Run a Cox PH model and obtain Schoenfeld residuals for each predictor. Step 2. Create a variable that ranks the order of failures. The subject who has the first (earliest) event gets a value of 1, the next gets a value of 2, and so on. Step 3. Test the correlation between the variables created in the first and second steps. The null hypothesis is that the correlation between the Schoenfeld residuals and ranked failure time is zero Rejection of the null hypothesis leads to a conclusion that the PH assumption is violated However, 1) a \\(p\\)-value can be driven by sample size; 2) A gross violation of the null assumption may not be statistically significant if the sample is very small; and 3) conversely, a slight violation of the null assumption may be highly significant if the sample is very large. cox.gof &lt;- coxph(Surv(enter, exit, event) ~ sex + region + imr.birth, data = oldmort01) res.zph &lt;- cox.zph(cox.gof, transform = c(&quot;km&quot;,&quot;rank&quot;,&quot;idenityt&quot;)[2]) res.zph ## chisq df p ## sex 5.25 1 0.0220 ## region 9.83 2 0.0073 ## imr.birth 6.54 1 0.0105 ## GLOBAL 18.96 4 0.0008 plot(res.zph) 6.6.3 Time-dependent variable approaches The Cox model is extended to contain product (i.e., interaction) terms involving the time-independent variable being assessed and some function of time. If the coefficient of the product term turns out to be significant, we can conclude that the PH assumption is violated. Using the above one-at-a-time model, we assess the PH assumption by testing for the significance of the product term. The null hypothesis is therefore “d equal to zero.” Note that if the null hypothesis is true, the model reduces to a Cox PH model containing the single variable X. The test can be carried out using . To assess the PH assumption for several predictors simultaneously, the form of the extended model is \\[h(t,X) =h_0(t) exp\\left[\\sum_{i=1}^p (\\beta_i X_i + \\delta_i (X_i \\times g_i(t)))\\right], \\text{ where } g_i(t) \\text{ is a function of time for } i^{th} \\text{ predictor}\\] - This model contains the predictors being assessed as main effect terms and also as product terms with some function of time. Note that different predictors may require different functions of time; hence, the notation \\(g_i (t)\\) is used to define the time function for the \\(i^{th}\\) predictor - With the above model, we test for the PH assumption simultaneously by assessing the null hypothesis that all the \\(\\delta_i\\) coefficients are equal to zero. This requires a likelihood ratio chi-square statistic with \\(p\\) degrees of freedom, where \\(p\\) denotes the number of predictors being assessed. The LR statistic computes the difference between the log likelihood statistic (i.e., \\(-2\\; ln\\; L\\)) for the PH model and the log likelihood statistic for the extended Cox model. Note that under the null hypothesis, the model reduces to the Cox PH model. If the above test is found to be significant, then we can conclude that the PH assumption is not satisfied for at least one of the predictors in the model. To determine which predictor(s) do not satisfy the PH assumption, we could proceed by backward elimination of nonsignificant product terms until a final model is attained. The primary drawback of the use of an extended Cox model for assessing the PH assumption concerns the choice of the functions \\(g_i (t)\\) for the time-dependent product terms in the model. This choice is typically not clear-cut, and it is possible that different choices, such as \\(g(t)\\) equal to \\(t\\) versus log \\(t\\) versus a heaviside function, may result in different conclusions about whether the PH assumption is satisfied. 6.6.4 Testing for Influential Observations Testing for Influential Observations{} To test influential observations or outliers, we can visualize either the deviance residuals or the dfbeta values type: the type of residuals to present on Y axis. Allowed values include one of c(“martingale”, “deviance”, “score”, “schoenfeld”, “dfbeta”, “dfbetas”, “scaledsch”, “partial”). It’s also possible to check outliers by visualizing the deviance residuals. The deviance residual is a normalized transform of the martingale residual. These residuals should be roughtly symmetrically distributed about zero with a standard deviation of 1. Positive values correspond to individuals that “died too soon” compared to expected survival times. Negative values correspond to individual that “lived too long”. Very large or small values are outliers, which are poorly predicted by the model. survminer::ggcoxdiagnostics(oldmort_cox03, type = &quot;martingale&quot;, linear.predictions = FALSE, ggtheme = theme_bw()) ## Warning: `gather_()` was deprecated in tidyr 1.2.0. ## ℹ Please use `gather()` instead. ## ℹ The deprecated feature was likely used in the survminer ## package. ## Please report the issue at ## &lt;https://github.com/kassambara/survminer/issues&gt;. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where ## this warning was generated. ## `geom_smooth()` using formula = &#39;y ~ x&#39; survminer::ggcoxdiagnostics(oldmort_cox03, type = &quot;schoenfeld&quot;, linear.predictions = FALSE, ggtheme = theme_bw()) ## `geom_smooth()` using formula = &#39;y ~ x&#39; survminer::ggcoxdiagnostics(oldmort_cox03, type = &quot;dfbeta&quot;, linear.predictions = FALSE, ggtheme = theme_bw()) ## `geom_smooth()` using formula = &#39;y ~ x&#39; survminer::ggcoxdiagnostics(oldmort_cox03, type = &quot;deviance&quot;, linear.predictions = FALSE, ggtheme = theme_bw()) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 6.6.5 Testing for Non-linearlity Nonlinearity is not an issue for categorical variables, so we only examine plots of martingale residuals and partial residuals against a continuous variable. Martingale residuals may present any value in the range (\\(-\\infty,\\; +1\\)): a value of martinguale residuals near 1 represents individuals that “died too soon”, large negative values correspond to individuals that “lived too long”. ggcoxfunctional(Surv(enter, exit, event) ~ imr.birth + log(imr.birth) + sqrt(imr.birth), data = oldmort01) ## Warning: arguments formula is deprecated; will be removed in the next version; ## please use fit instead. http://ehar.se/r/ehar2/proportional-hazards-and-cox-regression.html#cox-regression-models↩︎ "],["accounting-for-heterogeneity.html", "Chapter 7 Accounting for Heterogeneity 7.1 Setup working datasets 7.2 An introductory Example2 7.3 Working with heterogeneity3 7.4 Stratification 7.5 Frailty models5", " Chapter 7 Accounting for Heterogeneity 7.1 Setup working datasets library(tidyverse) library(knitr) library(kableExtra) library(eha) library(survival) library(data.table) library(flextable) library(survminer) library(ggfortify) library(ggplot2) library(data.table) library(coxme) library(broom) oldmort01 &lt;- oldmort oldmort01$male &lt;- relevel(oldmort01$sex, ref = &quot;female&quot;) 7.2 An introductory Example2 Let us assume that in a follow-up study, the cohort is not homogeneous but instead consists of two equally sized groups with differing hazard rates. Assume further that we have no indication of which group an individual belongs to, and that members of both groups follow an exponential life length distribution: \\[\\begin{equation*} \\begin{split} h_1(t) &amp;= \\lambda_1 \\\\ h_2(t) &amp;= \\lambda_2 \\\\ \\end{split} \\qquad t &gt; 0. \\end{equation*}\\] This implies that the corresponding survival functions \\(S_1\\) and \\(S_2\\) are \\[\\begin{equation*} \\begin{split} S_1(t) &amp;= e^{-\\lambda_1 t} \\\\ S_2(t) &amp;= e^{-\\lambda_2 t} \\\\ \\end{split} \\qquad t &gt; 0, \\end{equation*}\\] and a randomly chosen individual will follow the “population mortality” \\(S\\), which is a mixture of the two distributions: \\[\\begin{equation*} S(t) = \\frac{1}{2} S_1(t) + \\frac{1}{2} S_2(t), \\quad t &gt; 0. \\end{equation*}\\] Let us calculate the hazard function for this mixture. We start by finding the density function \\(f\\): \\[\\begin{equation*} f(t) = -\\frac{dS(x)}{dx} = \\frac{1}{2}\\left(\\lambda_1 e^{-\\lambda_1 t} + \\lambda_2 e^{-\\lambda_2 t} \\right), \\quad t &gt; 0. \\end{equation*}\\] Then, by the definition of \\(h\\) we get \\[\\begin{equation} h(t) = \\frac{f(t)}{S(t)} = \\omega(t) \\lambda_1 + \\big(1 - \\omega(t)\\big) \\lambda_2, \\quad t &gt; 0, \\tag{7.1} \\end{equation}\\] with \\[\\begin{equation*} \\omega(t) = \\frac{e^{-\\lambda_1 t}}{e^{-\\lambda_1 t} + e^{-\\lambda_2 t}} \\end{equation*}\\] It is easy to see that \\[\\begin{equation*} \\omega(t) \\rightarrow \\left\\{ \\begin{array}{ll} 0, &amp; \\lambda_1 &gt; \\lambda_2 \\\\ \\frac{1}{2}, &amp; \\lambda_1 = \\lambda_2 \\\\ 1, &amp; \\lambda_1 &lt; \\lambda_2 \\end{array} \\right. , \\quad \\mbox{as } t \\rightarrow \\infty, \\end{equation*}\\] implying that \\[\\begin{equation*} h(t) \\rightarrow \\min(\\lambda_1, \\lambda_2), \\quad t \\rightarrow \\infty, \\end{equation*}\\] see Figure 7.1. Figure 7.1: Population hazard function (solid line). The dashed lines are the hazard functions of each group, constant at 1 and 2. The important point here is that it is impossible to tell from data alone whether the population is homogeneous, with all individuals following the same hazard function given by equation (7.1), or if it in fact consists of two groups, each following a constant hazard rate. Therefore, individual frailty models like \\(h_i(t) = Z_i h(t), \\quad i = 1, \\ldots, n\\), where \\(Z_i\\) is the “frailty” for individual No. \\(i\\), and \\(Z_1, \\ldots, Z_n\\) are independent and identically distributed (iid) are less useful. A heuristic explanation to all this is the dynamics of the problem: We follow a population (cohort) over time, and the composition of it changes over time. The weaker individuals die first, and the proportion stronger will steadily grow as time goes by. Another terminology is to distinguish between individual and population hazards. In Figure 7.1 the solid line is the population hazard, and the dashed lines represent the two kinds of individual hazards present. Of course, in a truly homogeneous population, these two concepts coincide. 7.3 Working with heterogeneity3 Suppose that we collect data measuring time (variable \\(time\\)) from the onset of risk at time zero until occurrence of an event of interest (variable \\(fail\\)) on patients from different hospitals (variable \\(hospital\\)). We want to study patients’ survival as a function of some risk factors, say age and gender (variable \\(age\\) and \\(gender\\)). We can estimate the effect of predictors on survival by fitting a Cox model. \\[h(t) = h_0(t) \\exp(age\\times x_1 + gender\\times x_2)\\] In this model, we ignore the fact that patients come from different hospitals and therefore assumed that hospitals have no effect on the results. If we believe that there might be a group effect (e.g., the effect of a hospital), we should take it into account in the analysis. There are various ways of adjusting for group effects (i.e., subjects are correlated we mean that subjects’ failure times are correlated or they are heterogenous). Each depends on the nature of the grouping of subjects and on the assumptions we are willing to make about the effect of grouping on subjects’ survival. Stratified model Suppose we identified a fixed number of hospitals and then sampled our patients within each hospital; that is, we stratified on hospitals in our sampling design. Then we can adjust for the homogeneity of patients within a stratum (a hospital) using a stratified Cox model. \\[h_g(t) = h_{0g}(t)\\exp(age\\times x_1 + gender\\times x_2), \\;\\; where \\;\\;g=1, \\cdots, n\\] The same logic applies to the situation when we believe that there is possible dependence among patients within a hospital. Subjects might be correlated, either because of how we sampled our data or because of some other reasons specific to the nature of the grouping, or we want to allow baseline hazards to be different for each hospital rather than constraining them to be multiplicative version of each other. If your main focus is on the effect of other predictors (e.g., age and gender), you may benefit from accounting for the group-specific effects in a more general way by stratifying on the group. Random effect model Alternately, we can model correlation by assuming that it is induced by an unobserved hospital-level random effect, or frailty, and by specifying the distribution of this random effect (only for parametric model). The effect of a hospital is assumed to be random and to have a multiplicative effect on the hazard function. Here the effect of a hospital is directly incorporated into the hazard function, resulting in a different model specification for the survival data: a shared frailty model. As such, both point estimates and their standard errors will change. For example, in the gamma distribution, the effect of a hospital is governed by a mean of 1 and variace of \\(\\theta\\). If the estimated \\(\\hat{\\theta}\\) is not significantly different from zero, we ignore the correlation due to hospitals is ignored. \\[h(t) = h_0(t) \\exp(age\\times x_1 + gender\\times x_2) \\;\\;with\\;\\; frailty(hospital)\\] Fixed effect model Suppose we are only interested in the effect of our observed hospitals rather than in making inferences about the effect of all hospitals based on the observed random sample of hospitals. In this case, the effects of all hospitals are treated as fixed, and we estimate it by including in the model. We assume that the hospitals have a direct multiplicative effect on the hazard function. That is, all patients share the sam baseline hazard function, and the effect of a hospital multiplies this baseline hazard function up or down depending on the sign of the estimated coefficients for the hospital indicator. \\[h(t) = h_0(t) \\exp(age\\times x_1 + gender\\times x_2 + hospital \\times x_3)\\] Interaction with stratification You may include an interaction term “hospital*age”, which will result in a different model: the effect of a hospital is absorbed in the baseline hazard but the effect of \\(age\\) is allowed to vary with hospitals. \\[h_g(t) = h_{0g}(t) \\exp(age\\times x_1 + gender\\times x_2 + hospital \\times age \\times x_3), \\;\\; where\\;\\; g=1, \\cdots, n\\] In sum, there is no definitive recommendation on how to account for the group effect and on which model is the most appropriate when analyzing data. Robust standard error (aka empirical standard error, sandwich estimator) A widely used technique for adjusting for the correlation among outcomes on the same subject is called robust estimation (also referred to as empirical estimation). This technique essentially involves adjusting the estimated variances of regression coefficients obtained for a fitted model to account for misspecification of the correlation structure assumed - Crude model oldmort_coxst &lt;- coxph(Surv(enter, exit, event) ~ male + region + imr.birth, data = oldmort01) summary(oldmort_coxst) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + region + imr.birth, ## data = oldmort01) ## ## n= 6495, number of events= 1971 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## malemale 0.185247 1.203516 0.045913 4.035 5.47e-05 *** ## regionindustry 0.224903 1.252201 0.087093 2.582 0.00981 ** ## regionrural 0.069031 1.071470 0.086627 0.797 0.42552 ## imr.birth 0.004548 1.004558 0.006742 0.675 0.49994 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## malemale 1.204 0.8309 1.0999 1.317 ## regionindustry 1.252 0.7986 1.0557 1.485 ## regionrural 1.071 0.9333 0.9042 1.270 ## imr.birth 1.005 0.9955 0.9914 1.018 ## ## Concordance= 0.545 (se = 0.008 ) ## Likelihood ratio test= 31.41 on 4 df, p=3e-06 ## Wald test = 31.7 on 4 df, p=2e-06 ## Score (logrank) test = 31.8 on 4 df, p=2e-06 - Robust standard error oldmort_coxst &lt;- coxph(Surv(enter, exit, event) ~ male + region + imr.birth, id = id, robust = TRUE, data = oldmort01) summary(oldmort_coxst) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + region + imr.birth, ## data = oldmort01, robust = TRUE, id = id) ## ## n= 6495, number of events= 1971 ## ## coef exp(coef) se(coef) robust se z Pr(&gt;|z|) ## malemale 0.185247 1.203516 0.045913 0.046314 4.000 6.34e-05 *** ## regionindustry 0.224903 1.252201 0.087093 0.086125 2.611 0.00902 ** ## regionrural 0.069031 1.071470 0.086627 0.084892 0.813 0.41613 ## imr.birth 0.004548 1.004558 0.006742 0.006904 0.659 0.51006 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## malemale 1.204 0.8309 1.0991 1.318 ## regionindustry 1.252 0.7986 1.0577 1.482 ## regionrural 1.071 0.9333 0.9072 1.265 ## imr.birth 1.005 0.9955 0.9911 1.018 ## ## Concordance= 0.545 (se = 0.008 ) ## Likelihood ratio test= 31.41 on 4 df, p=3e-06 ## Wald test = 31.68 on 4 df, p=2e-06 ## Score (logrank) test = 31.8 on 4 df, p=2e-06, Robust = 31.52 p=2e-06 ## ## (Note: the likelihood ratio and score tests assume independence of ## observations within a cluster, the Wald and robust score tests do not). 7.4 Stratification Stratification means that data is split up in groups called strata, and a separate partial likelihood function is created for each stratum, but with common values on the regression parameters corresponding to the common explanatory variables. In the estimation, these partial likelihoods are multiplied together, and the product is treated as a likelihood function. Thus, there is one restriction on the parameters, they are the same across strata. There are typically two reasons for stratification. First, if the proportionality assumption does not hold for a factor covariate, a way out is to stratify along it. Second, a factor may have too many levels, so that it is inappropriate to treat is as an ordinary factor. This argument is similar to the one about using a frailty model. Stratification is also a useful tool with matched data. When a factor does not produce proportional hazards between categories, stratify on the categories.4 7.4.1 Generalized stratified models \\[h_g(t,X)=h_0g (t) \\exp[\\beta_1X_1+\\beta_2X_2+ \\cdots+\\beta_p x_p]\\] \\(g=1,2,\\dots,k^*,\\) strata defined from \\(Z^*\\), which has \\(k^*\\) categories \\(Z^*\\) is not included in the model \\(X_i\\)s are included in the model Hazard ratio is same for each stratum A simple way to eliminate the effect of clustering is to stratify on the clusters. The drawback with a stratified analysis is that it is not possible to estimate the effect of covariates that are constant within clusters. Notice also that the hazard functions for groups (e.g., males and females) differ only insofar as they have different baseline hazard functions, namely, \\(h_{01}(t)\\) for females and \\(h_{02}(t)\\) for males. However, the coefficients \\(\\beta_i\\)s are the same for both female and male models. 7.4.2 group_by analysis (dplyr) group_by &lt;- oldmort01 %&gt;% group_by(region) %&gt;% do(oldmort_coxst = coxph(Surv(enter, exit, event) ~ male + imr.birth, data = .)) group_by$oldmort_coxst ## [[1]] ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + imr.birth, ## data = .) ## ## coef exp(coef) se(coef) z p ## malemale 0.681102 1.976054 0.159655 4.266 1.99e-05 ## imr.birth -0.004887 0.995125 0.017667 -0.277 0.782 ## ## Likelihood ratio test=17.33 on 2 df, p=0.0001729 ## n= 657, number of events= 173 ## ## [[2]] ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + imr.birth, ## data = .) ## ## coef exp(coef) se(coef) z p ## malemale 0.19435 1.21452 0.07365 2.639 0.00832 ## imr.birth 0.03092 1.03140 0.01120 2.761 0.00575 ## ## Likelihood ratio test=14.66 on 2 df, p=0.0006558 ## n= 2214, number of events= 759 ## ## [[3]] ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + imr.birth, ## data = .) ## ## coef exp(coef) se(coef) z p ## malemale 0.096529 1.101342 0.062874 1.535 0.125 ## imr.birth -0.010157 0.989894 0.009451 -1.075 0.282 ## ## Likelihood ratio test=3.32 on 2 df, p=0.1906 ## n= 3624, number of events= 1039 7.4.3 strata in coxph oldmort_coxst &lt;- coxph(Surv(enter, exit, event) ~ male + imr.birth + strata(region), data = oldmort01) summary(oldmort_coxst) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + imr.birth + ## strata(region), data = oldmort01) ## ## n= 6495, number of events= 1971 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## malemale 0.178905 1.195907 0.045963 3.892 9.93e-05 *** ## imr.birth 0.003723 1.003730 0.006769 0.550 0.582 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## malemale 1.196 0.8362 1.0929 1.309 ## imr.birth 1.004 0.9963 0.9905 1.017 ## ## Concordance= 0.528 (se = 0.008 ) ## Likelihood ratio test= 15.55 on 2 df, p=4e-04 ## Wald test = 15.68 on 2 df, p=4e-04 ## Score (logrank) test = 15.72 on 2 df, p=4e-04 7.4.4 strata in coxph with interaction terms Interaction terms for region can be included directly in the model formula by including product terms using the : operator. oldmort_coxst &lt;- coxph(Surv(enter, exit, event) ~ male + imr.birth + male:region + strata(region), data = oldmort01) summary(oldmort_coxst) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + imr.birth + ## male:region + strata(region), data = oldmort01) ## ## n= 6495, number of events= 1971 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## malemale 0.684144 1.982075 0.159427 4.291 1.78e-05 *** ## imr.birth 0.004434 1.004444 0.006755 0.656 0.511585 ## malefemale:regionindustry 0.483570 1.621854 0.175642 2.753 0.005902 ** ## malemale:regionindustry NA NA 0.000000 NA NA ## malefemale:regionrural 0.593993 1.811206 0.171391 3.466 0.000529 *** ## malemale:regionrural NA NA 0.000000 NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## malemale 1.982 0.5045 1.4502 2.709 ## imr.birth 1.004 0.9956 0.9912 1.018 ## malefemale:regionindustry 1.622 0.6166 1.1495 2.288 ## malemale:regionindustry NA NA NA NA ## malefemale:regionrural 1.811 0.5521 1.2944 2.534 ## malemale:regionrural NA NA NA NA ## ## Concordance= 0.528 (se = 0.008 ) ## Likelihood ratio test= 27.26 on 4 df, p=2e-05 ## Wald test = 28.43 on 4 df, p=1e-05 ## Score (logrank) test = 29.12 on 4 df, p=7e-06 Suppose we wish to estimate the hazard ratio for male = 1 vs. male = 0 for region = 2. This hazard ratio can be estimated by exponentiating the coefficient for male plus 2 times the coefficient for the male*region interaction term. This expression is obtained by substituting the appropriate values into the hazard in both the numerator (for male = 1) and denominator (for male = 0). \\[HR=\\frac{h_0(t) \\exp[\\beta_1 (1) + \\beta_2 imr.birth + \\beta_3 (1)(2) + \\beta_4 (2)]}{h_0(t) \\exp[\\beta_1 (0) + \\beta_2 imr.birth + \\beta_3 (0)(2) + \\beta_4 (2)]}=\\exp(\\beta_1 + 2 \\beta_3)\\] The resulting hazard ratio, \\(\\exp(\\beta_1 + 2 \\beta_2)\\), is an exponentiated linear combination of parameters. Unfortunately, R does not have a lincom command that Stata provides or an estimate statement that SAS provides in order to calculate a linear combination of parameter estimates. However an approach that can be used in any statistical software package for such a situation is to recode the variable(s) of interest such that the desired estimate is no longer a linear combination of parameter estimates. 7.4.5 Weibull PH models with or without stratification Without stratification Weib.PH_st &lt;- phreg(Surv(enter, exit, event) ~ male + imr.birth, data = oldmort01, dist=&quot;weibull&quot;, param=&quot;survreg&quot;) b_Weib.PH_st = coef(Weib.PH_st) Weib.PH_st ## Call: ## phreg(formula = Surv(enter, exit, event) ~ male + imr.birth, ## data = oldmort01, dist = &quot;weibull&quot;, param = &quot;survreg&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## male ## female 0.594 0 1 (reference) ## male 0.406 0.192 1.212 0.046 0.000 ## imr.birth 15.162 0.005 1.005 0.006 0.446 ## ## log(scale) 4.371 0.012 0.000 ## log(shape) 2.091 0.027 0.000 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -7287.9 ## LR test statistic 18.30 ## Degrees of freedom 2 ## Overall p-value 0.000106031 With stratification Weib.PH_st &lt;- phreg(Surv(enter, exit, event) ~ male + imr.birth + strata(region), data = oldmort01, dist=&quot;weibull&quot;, param=&quot;survreg&quot;) b_Weib.PH_st = coef(Weib.PH_st) Weib.PH_st ## Call: ## phreg(formula = Surv(enter, exit, event) ~ male + imr.birth + ## strata(region), data = oldmort01, dist = &quot;weibull&quot;, param = &quot;survreg&quot;) ## ## Covariate W.mean Coef Exp(Coef) se(Coef) Wald p ## male ## female 0.594 0 1 (reference) ## male 0.406 0.181 1.198 0.046 0.000 ## imr.birth 15.162 0.005 1.005 0.007 0.445 ## ## log(scale):1 4.383 0.020 0.000 ## log(shape):1 1.954 0.106 0.000 ## log(scale):2 4.352 0.015 0.000 ## log(shape):2 2.009 0.046 0.000 ## log(scale):3 4.379 0.012 0.000 ## log(shape):3 2.158 0.035 0.000 ## ## Events 1971 ## Total time at risk 37824 ## Max. log. likelihood -7276.8 ## LR test statistic 16.30 ## Degrees of freedom 2 ## Overall p-value 0.000288105 Here are some differences between two models: Compared to the Weibull PH model without stratification, the stratified model includes additional set of parameters of log(scale) and log(shape). More specifically, both log(scale) and log(shape) will be estimated for each stratum. Only one set of coefficients of clinic and dose was estimated. this means that the stratified model assume the effects of variables are the same across strata. In other words, “we allow both the scale and shape of the hazard to vary with strata, yet we constrained the effects of included variables to be the same for each stratum.” plot(Weib.PH_st) 7.5 Frailty models5 Frailty models in survival analysis correspond to hierarchical models in linear or generalized linear models. They are also called mixed effects models. They contain an extra random component designed to account for individual(or subgroup)-level differences in the hazard otherwise unaccounted for by the model. The frailty, \\(\\alpha\\), is a multiplicative effect on the hazard assumed to follow some distribution. The hazard function conditional on the frailty can be expressed as \\(h(t|\\alpha)=\\alpha [h(t)]\\). 7.5.1 Simple Frailty Model Vaupel et al. (1979) described an individual frailty model, \\[h(t;x,Z)=h_0(t)Z e^{\\beta x}, \\;\\; t&gt;0,\\] where \\(Z\\) is assumed to be drawn independently for each individual. Hazard rates for “random survivors” are not proportional, but converging (to each other) if the frailty distribution has finite variance. Thus, the problem may be less pronounced in AFT than in PH regression. 7.5.2 Shared Frailty Model Frailty models work best when there is a natural grouping of the data, so that observations from the same group are dependent (share the same frailty), while two individual survival times from different groups can be regarded as independent. Such a model may be described as \\[h_i(t;x)=h_{i0}(t) e^{\\beta x}, \\;\\;i=1,\\dots, s; \\;\\;t&gt;0,\\] which simply is a stratified Cox regression model. By assuming \\[h_{i0}(t)=Z_i h_0(t),\\;\\; i=1,\\dots,s;\\;\\;t&gt;0,\\] the traditional multivariate frailty model emerges. Here it is assumed that \\(Z_1,\\dots,Z_s\\) are independent and identically distributed (\\(iid\\)), usually with a lognormal distribution. From what we get, with \\(U_i = \\log(Z_i)\\), \\[h_i(t;x)=h_0(t) e^{\\beta x +U_i}, \\;\\;i=1,…,s;\\;\\;t&gt;0.\\] In this formulation, \\(U_1,\\dots, U_s\\) are \\(iid\\) normal with mean zero and unknown variance \\(\\sigma^2\\). Another popular choice of distribution for the \\(Z:s\\) is the gamma distribution. 7.5.3 Cox model specification \\[ h(t) = h_0(t) \\exp{(X\\beta)} \\] 7.5.4 Mixed effects Cox model specification \\[ h(t) = h_0(t) \\exp{(X\\beta+Zb)},\\;\\;\\; b \\sim G(0, \\sum(\\theta)) \\] where \\(h_0\\) is unspecified baseline hazard function, \\(X\\) and \\(Z\\) are the design matrices for the fixed and random effects, respectively, \\(\\beta\\) is the vector of fixed-effect coefficients and \\(b\\) is the vector of random effects coefficents. The random effects distribution \\(G\\) is modeled as Gaussian with mean 0 and a variance matrax \\(\\sum\\), which in turn depends a vector of parameters \\(\\theta\\). 7.5.5 Models without considering fraility R offers three choices for the distribution of the frailty: the gamma, Gaussian, and \\(t\\) distributions. The variance (\\(\\theta\\)) of the frailty component is a parameter typically estimated by the model. If \\(\\theta\\) = 0, then there is no frailty. First, we rerun a stratified Cox model without frailty. oldmort_coxst &lt;- coxph(Surv(enter, exit, event) ~ male + imr.birth + strata(region), data = oldmort01) summary(oldmort_coxst) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + imr.birth + ## strata(region), data = oldmort01) ## ## n= 6495, number of events= 1971 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## malemale 0.178905 1.195907 0.045963 3.892 9.93e-05 *** ## imr.birth 0.003723 1.003730 0.006769 0.550 0.582 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## malemale 1.196 0.8362 1.0929 1.309 ## imr.birth 1.004 0.9963 0.9905 1.017 ## ## Concordance= 0.528 (se = 0.008 ) ## Likelihood ratio test= 15.55 on 2 df, p=4e-04 ## Wald test = 15.68 on 2 df, p=4e-04 ## Score (logrank) test = 15.72 on 2 df, p=4e-04 Next we illustrate how to include a frailty component in this model. oldmort_coxst &lt;- coxph(Surv(enter, exit, event) ~ male + imr.birth + strata(region) + frailty(id, distribution = &quot;gamma&quot;), data = oldmort01) ## Warning in coxpenal.fit(X, Y, istrat, offset, init = init, control, weights = ## weights, : Inner loop failed to coverge for iterations 2 summary(oldmort_coxst) ## Call: ## coxph(formula = Surv(enter, exit, event) ~ male + imr.birth + ## strata(region) + frailty(id, distribution = &quot;gamma&quot;), data = oldmort01) ## ## n= 6495, number of events= 1971 ## ## coef se(coef) se2 Chisq DF p ## malemale 0.178913 0.045964 0.045963 15.15 1.0 9.9e-05 ## imr.birth 0.003723 0.006769 0.006769 0.30 1.0 5.8e-01 ## frailty(id, distribution 0.10 0.1 4.9e-01 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## malemale 1.196 0.8362 1.0929 1.309 ## imr.birth 1.004 0.9963 0.9905 1.017 ## ## Iterations: 5 outer, 42 Newton-Raphson ## Variance of random effect= 5e-05 I-likelihood = -11764.4 ## Degrees of freedom for terms= 1.0 1.0 0.1 ## Concordance= 0.553 (se = 0.008 ) ## Likelihood ratio test= 15.74 on 2.1 df, p=4e-04 The term + frailty(id, distribution=“gamma”) is included in the model formula. The first argument of the frailty function is the variable id and indicates that the unmeasured heterogeneity (the frailty) is at the individual level. The second argument indicates that the distribution of the random component is the gamma distribution. Under the table of parameter estimates the output indicates that the variance of random effect is very small. The p-value for the frailty component indicates that the frailty component is not significant. We conclude that the variance of the random component is zero for this model (i.e., there is no frailty). 7.5.6 coxme package for frailty models 7.5.6.1 Nested in each individual The package coxme works with frailty (e.g., (1 | id)), accounting for multiple observations in each id. oldmort_coxst &lt;- coxme(Surv(enter, exit, event) ~ male + region + imr.birth + (1 | id), data = oldmort01) summary(oldmort_coxst) ## Cox mixed-effects model fit by maximum likelihood ## Data: oldmort01 ## events, n = 1971, 6495 ## Iterations= 5 23 ## NULL Integrated Fitted ## Log-likelihood -13578.6 -13562.88 -13539.4 ## ## Chisq df p AIC BIC ## Integrated loglik 31.44 5.00 7.6668e-06 21.44 -6.49 ## Penalized loglik 78.40 27.36 8.0644e-07 23.68 -129.15 ## ## Model: Surv(enter, exit, event) ~ male + region + imr.birth + (1 | id) ## Fixed coefficients ## coef exp(coef) se(coef) z p ## malemale 0.187097661 1.205745 0.046200912 4.05 5.1e-05 ## regionindustry 0.225769128 1.253286 0.087555314 2.58 9.9e-03 ## regionrural 0.068923216 1.071354 0.087071018 0.79 4.3e-01 ## imr.birth 0.004661296 1.004672 0.006782026 0.69 4.9e-01 ## ## Random effects ## Group Variable Std Dev Variance ## id Intercept 0.10983436 0.01206359 The components of the resutls are6 The total number of observations and the total number of events (deaths) in the data set. The computational effort, summarized as the number of iterations for the optim routine and the underlying Newton-Raphson iterations used. The log partial likelihood for a model with no covariates or random effects, the fitted partial likelihood, and the value with the random effects integrated out. We will normally be interested in the null and integrated values. (The log values are printed, but labeled as PL and IPL for brevity). Likelihood ratio tests based on the integrated and penalized views of the model, along with penalized values. The AIC penalizes by twice the effective degrees of freedom, and the BIC by log(d) times the effective degrees of freedom, where d is the number of events. A summary of the fixed effects A summary of the variances of the random effects One feature of the mixed effects Cox model is that the standard deviation of the random effect is directly interpretable. The random effects \\(b_j\\) for each individual \\(j\\) are in the risk score, a value of .11 for instance (one standard deviation above the mean) corresponds to a relative risk of exp(.11) = 1.12, an almost 12% higher risk of death for subjects at that individual. 7.5.6.2 Nested in each mother m.id is a identifier for each mother. oldmort_coxst &lt;- coxme(Surv(enter, exit, event) ~ male + region + imr.birth + (1 | m.id), data = oldmort01) summary(oldmort_coxst) ## Cox mixed-effects model fit by maximum likelihood ## Data: oldmort01 ## events, n = 888, 3340 (3155 observations deleted due to missingness) ## Iterations= 2 10 ## NULL Integrated Fitted ## Log-likelihood -5702.662 -5694.263 -5693.911 ## ## Chisq df p AIC BIC ## Integrated loglik 16.8 5.00 0.0048989 6.8 -17.15 ## Penalized loglik 17.5 4.35 0.0021189 8.8 -12.04 ## ## Model: Surv(enter, exit, event) ~ male + region + imr.birth + (1 | m.id) ## Fixed coefficients ## coef exp(coef) se(coef) z p ## malemale 0.13528873 1.1448673 0.06840477 1.98 0.048 ## regionindustry 0.16109275 1.1747939 0.13624607 1.18 0.240 ## regionrural -0.08711187 0.9165746 0.14598296 -0.60 0.550 ## imr.birth -0.02119861 0.9790245 0.01447452 -1.46 0.140 ## ## Random effects ## Group Variable Std Dev Variance ## m.id Intercept 0.0199927885 0.0003997116 Because of missing cases in m.id, the results above are not comparable. To make both models, nested in each individual vs. nested in each mother, we select cases with m.id is not missing. oldmort01_mid &lt;- oldmort01[!is.na(oldmort01$m.id), ] Then fit the models, respectively. oldmort_coxst &lt;- coxme(Surv(enter, exit, event) ~ male + region + imr.birth + (1 | id), data = oldmort01_mid) summary(oldmort_coxst) ## Cox mixed-effects model fit by maximum likelihood ## Data: oldmort01_mid ## events, n = 888, 3340 ## Iterations= 5 24 ## NULL Integrated Fitted ## Log-likelihood -5702.662 -5694.064 -5618.221 ## ## Chisq df p AIC BIC ## Integrated loglik 17.20 5.00 4.1412e-03 7.20 -16.75 ## Penalized loglik 168.88 78.25 1.3031e-08 12.38 -362.37 ## ## Model: Surv(enter, exit, event) ~ male + region + imr.birth + (1 | id) ## Fixed coefficients ## coef exp(coef) se(coef) z p ## malemale 0.14578584 1.1569484 0.07085215 2.06 0.04 ## regionindustry 0.16556153 1.1800556 0.14095526 1.17 0.24 ## regionrural -0.08807818 0.9156893 0.15076497 -0.58 0.56 ## imr.birth -0.02125968 0.9789647 0.01493296 -1.42 0.15 ## ## Random effects ## Group Variable Std Dev Variance ## id Intercept 0.30065228 0.09039179 oldmort_coxst &lt;- coxme(Surv(enter, exit, event) ~ male + region + imr.birth + (1 | m.id), data = oldmort01_mid) summary(oldmort_coxst) ## Cox mixed-effects model fit by maximum likelihood ## Data: oldmort01_mid ## events, n = 888, 3340 ## Iterations= 2 10 ## NULL Integrated Fitted ## Log-likelihood -5702.662 -5694.263 -5693.911 ## ## Chisq df p AIC BIC ## Integrated loglik 16.8 5.00 0.0048989 6.8 -17.15 ## Penalized loglik 17.5 4.35 0.0021189 8.8 -12.04 ## ## Model: Surv(enter, exit, event) ~ male + region + imr.birth + (1 | m.id) ## Fixed coefficients ## coef exp(coef) se(coef) z p ## malemale 0.13528873 1.1448673 0.06840477 1.98 0.048 ## regionindustry 0.16109275 1.1747939 0.13624607 1.18 0.240 ## regionrural -0.08711187 0.9165746 0.14598296 -0.60 0.550 ## imr.birth -0.02119861 0.9790245 0.01447452 -1.46 0.140 ## ## Random effects ## Group Variable Std Dev Variance ## m.id Intercept 0.0199927885 0.0003997116 7.5.6.3 Individuals are nested in each mother Here is the code, but do not run - it takes forever. # oldmort_coxst &lt;- coxme(Surv(enter, exit, event) ~ male + region + imr.birth + (1 | id/m.id), # data = oldmort01_mid) #summary(oldmort_coxst) http://ehar.se/r/ehar2/multivariate-survival-models.html#an-introductory-example↩︎ Cleves MA. An Introduction to Survival Analysis Using Stata. 3rd ed. Stata Press; 2010.↩︎ http://ehar.se/r/ehar2/more-on-cox-regression.html#strat_6↩︎ This section is a summarized excerpt from Goran Brostrom, Event History Analysis with R and Kleinbaum and Klein, Survival Analysis, and https://cran.r-project.org/web/packages/coxme/vignettes/coxme.pdf↩︎ https://cran.r-project.org/web/packages/coxme/vignettes/coxme.pdf↩︎ "],["more-topics-on-survival-models.html", "Chapter 8 More Topics on Survival Models 8.1 Setup working datasets 8.2 Why we call Cox model as semi-parametric model? 8.3 Why the Cox PH model is so popular 8.4 Adjusted Survival Curves using the Cox PH model 8.5 Survival model selection7 8.6 Likelihood and partial likelihood 8.7 Tied or Discrete Data Analysis10 8.8 The discrete method", " Chapter 8 More Topics on Survival Models 8.1 Setup working datasets library(tidyverse) library(knitr) library(kableExtra) library(eha) library(survival) library(data.table) library(flextable) library(survminer) library(ggfortify) library(ggplot2) library(data.table) library(coxme) library(broom) oldmort01 &lt;- oldmort oldmort01$male &lt;- relevel(oldmort01$sex, ref = &quot;female&quot;) 8.2 Why we call Cox model as semi-parametric model? The formulation of a likelihood function is based on the distribution of the outcome. The Cox PH model does not impose any assumption on the distribution of the outcome, time to event. It simply uses the observed order of the failure time. (thus, it is a partial likelihood) If any distributional assumption was imposed, then it is a parametric survival model. No assumption on \\(h_0(t)\\) + proportional hazard assumption 8.3 Why the Cox PH model is so popular The Cox PH model is a robust model, so that the results from using the Cox model will closely approximate the results for the correct parametric model. Even though the baseline hazard is not specified, reasonably good estimates of regression coefficients, hazard ratios of interest, and adjusted survival curves can be obtained for a wide variety of data situations. We would prefer to use a parametric model if we were sure of the correct model. However, we may not be completely certain that a given parametric model is appropriate. When in doubt, the Cox model is a “safe” choice. Along with “robustness”, the model specification of the Cox PH model has several good properties. The exponential part of this product ensures that the fitted model will always give estimated hazards that are non-negative. (vs. a linear model with negative coefficients) The measure of effect, which is called a hazard ratio, is calculated without having to estimate the baseline hazard function. With a minimum of assumption, we can obtain the primary information about a hazard ratio and a survival curve. As compared to logistic model, the Cox PH model incorporate the survival time and censoring information. 8.4 Adjusted Survival Curves using the Cox PH model Two primary quantities we are interested in the survival model are estimated hazard ratios esitmated surival curves In the Cox PH model, Hazard function: \\(h(t, X) = h_0(t) \\exp[\\sum_{i=1}^p \\beta_i X_i]\\) Survival function: \\(S(t, X) = [S_0(t)]^{\\exp[\\sum_{i=1}^p \\beta_i X_i]}\\) Therefore, estimated functions are Estimated Hazard function: \\(\\hat{h}(t, X) = \\hat{h}_0(t) \\exp[\\sum_{i=1}^p \\hat{\\beta_i} X_i]\\) Estimated survival function: \\(\\hat{S}(t, X) = [\\hat{S}_0(t)]^{ \\exp[\\sum_{i=1}^p \\hat{\\beta_i} X_i]}\\) To fit the estimated curves, a set of values for \\(X_i\\) should be specified. Most software uses the mean value (rather than median) of \\(X\\)s to calculate the adjusted for covariates. 8.5 Survival model selection7 Akaike’s information criterion (AIC) provides an approach for comparing the fit of models with different underlying distributions, making use of the -2 log likelihood statistic The AIC statistic is calculated as: -2 log likelihood + 2\\(p\\) (where \\(p\\) is the number of parameters in the model). A smaller AIC statistic suggests a better fit. The addition of 2 times \\(p\\) can be thought of as a penalty if nonpredictive parameters are added to the model. Nested vs. non-nested models (The likelihood ratio test for the nested model is considered a superior method to the AIC for comparing non-nested models) Figure: “AIC Example” Likelihood ratio (LR) test: compute the difference between the log likelihood statistic of the reduced model (with fewer parameters to estimate) and the log likelihood statistic of the full model (with more parameters to estimate). In general, the LR statistic can be written in the form \\(-2 ln L_R\\) minus \\(-2 ln L_F\\), where \\(R\\) denotes the reduced model and \\(F\\) denotes the full model. The test statistic has a \\(\\chi ^2\\) distribution with \\(p\\) degrees of freedom, where \\(p\\) denotes the number of additional parameters being assessed. The exponential distribution is characterized by the fact that it lacks memory. In other words, items whose life lengths follow an exponential distribution do not age; no matter how old they are, if they are alive they are as good as new. This concept is not useful when it comes to human lives, but the life lengths of electronic components are often modeled by the exponential distribution in reliability theory. If the exponential distribution is not useful in describing human lives, it may be so for short segments of life. At least it will be a good approximation if the segment is short enough. This is the idea behind the piecewise constant hazards distribution. Its definition involves a partition of time (age) axis, and one positive constant (the hazard level) corresponding to each interval. Note that the last interval will be open, with infinite length; only a finite number of cut points are allowed. The definition of the hazard function \\(h(x)\\) becomes, with the cuts denoted \\(t=(t_1 &lt; \\cdots &lt; t_n)\\) and the levels denoted \\(h=(h_1, \\dots, h_{n+1})\\): \\(h(t;t,h)= h_1 (t \\ge t_1);\\) \\(h_i (t_{i_1} &lt; t \\ge t_i, i=2,\\dots,n,);\\) \\(h_{n+1} (t_n&lt;t)\\) In this definition, the number of levels must be exactly one more than the number of cut points. Note that, despite the fact that the hazard function is not continuous, the other functions are. They are not differentiable at the cut points, though. The piecewise constant distribution is very flexible. It can be made arbitrarily close to any continuous distribution by increasing the number of cut points and choose the levels appropriately. Parametric proportional hazards modeling with the pch distribution is a serious competitor to the Cox regression model, especially with large data sets. Piecewise constant hazards function library(eha) fit_pch &lt;- eha::pchreg(Surv(enter, exit, event) ~ male + imr.birth + region, cuts =seq(60, 100, by = 5), data = oldmort01) fit_np &lt;- coxreg(Surv(enter, exit, event) ~ male + imr.birth + region, data = oldmort01) compHaz(fit_pch, fit_np) The Weibull distribution is a very popular parametric model for survival data, described in detail by Waloddi Weibull (Weibull 1951), known earlier. It is one of the so-called extreme-value distributions, and as such very useful in reliability theory. It is becoming popular in demographic applications, but in mortality studies it is wise to avoid it for old age mortality (the hazard grows too slow) and mortality in ages 0–15 years of age (U-shaped hazards, which the Weibull model doesn’t allow). The lognormal distribution is connected to the normal distribution through the exponential function: If \\(X\\) is normally distributed, then \\(Y = \\exp(X)\\) is lognormally distributed. Conversely, if \\(Y\\) is lognormally distributed, then \\(X = \\log(Y)\\) is normally distributed. The lognormal distribution has the interesting property that the hazard function is first increasing, then decreasing, in contrast to the Weibull distribution which only allows for monotone (increasing or decreasing) hazard functions. The loglogistic distribution is very close to the lognormal, but have heavier tail to the right. Its advantage over the lognormal is that the hazard function has closed form. The Gompertz distribution is useful for modeling old age mortality. The hazard function is exponentially increasing. The Gompertz distribution was generalized by (Makeham 1860) (the Gompertz—Makeham Distribution). The generalization consists of adding a positive constant to the Gompertz hazard function. The Gamma distribution is another generalization of the exponential distribution. It is popular in modeling shared frailty. When you have no idea of what the baseline hazard looks like, use Cox regression. Exponential regression can be used to fit models in whihc the hazard varies with time, and that may be a reasonable thing to do, expecially if you want to berify the fit of another parametric model. For instance, pretend that you habe strong reason to beliebe that the formulation outgh to be Weibull. Even after fitting a Weibull model, you could use the exponential model with dummy variables for interbals to verify that the Weibull fit was reasonable. Which test should we use?8 There is no simple answer, so let us instead understand how to determine the answer in particular cases. The advantage of the modeling-the-effect approaches is that you can control for the effects of other variables. For instance, we would know that patients vary in age, and we would know age also affects outcome. In a carefully controlled experiment, we could ignore that effect because the average ages (and the distribution of age) of the control and experimental groups would be the same. The disadvantage of the modeling-the-effect approaches is that you could model the effect incorrectly in two ways. You could model incorrectly the effect of other variables, or you could mismodel the effect itself, for example, by stating its functional form incorrectly. Effects of the form “apply the treatment and get an overall improvement” are often not simple. Effects can vary with other covariates (being perhaps larger for males than for females), and effects can vary with time, whcih is to say, aspects that change over time and that are not measured. For instance, a treatment might involve surgery, after whcih there may be a greater risk to be followed by a lesser risk in the future. It is because of these concerns that looking at graphs is useful, whether you are engaging in parametric or semiparametric modeling (although, when doing semiparametric modeling, you can only indirectly look at the hazard function by looking at the cumulative hazard or survival function). In most real circumstances, you will be forced into parametric or semiparametric analysis. Nonparametric analysis is useful when the experiment has been carefully controlled, although even controlled experiments are sometimes not adequately controlled. Nonparametric analysis is always a useful starting point. In nonexperimental situations in the presence of covariates, you do this more as a data description technique rather than in hopes of producing any final analysis that you can believe. You, as a researcher, should be able to describe the survival experience, say, as reflected in a graph of the survivor function or cumulative hazard function for your data, ignoring the complications of confounding variables and the like. Before disentangling reality, you need to be able to describe the reality that your are starting with. So, our position is that you will likely be forced into parameterizing the effect. This is perhaps due more to our past analysis experiences. In a well-designed, controlled experiment, however, there is nothing wrong with not parameterizing the effect and stopping at nonparametric analysis. If you do need to continue, should you parameterize the hazard function? On this issue, different researchers fell differently. We are favorable disposed to parametric analysis when you have good reason to believe that the hazard function ought to follow a certain shape. Imposing a hazard function is an excellent way of improving the efficiency of your estimates and helping to avoid being misled by the fortuity of chance. On the other hand, when you do not have a good deductive reasons to know the shape of the hazard, you should use semiparametric analysis. When choosing between a semiparametric and parametric analysis, you much also take into consideration what information you are trying to obtain. In all you care about are hazard ratios (parameter effects) in a PH model, then you are probably better off with a semiparametric analysis. If you are interested in predicting the time to failure, however, some sort of parametric assumption as to the hazard is necessary. Here even if you do not have deductive knowledge as to the shape of the hazard, you can try all functional forms, to compare various functional forms of the hazard. You can use the piecewise exponential model to “nonparametrically” check the validity of any parametric form you wish to posit. 8.6 Likelihood and partial likelihood 8.6.1 Estimation of the Cox PH model using Maximum likelihood (ML) As with logistic regression, the ML estimates of the Cox model parameters are derived by maximizing a likelihood function, usually denoted as \\(L\\) (e.g., \\(L(\\beta)\\)). \\(L\\) is a partial likelihood (rather than a complete likelihood function): considers probabilities only for subjects who fail does not consider probabilities for subjects who are censored More specifically, the model breaks down each failure time to calculate each likelihood, and then get the product of several likelihoods \\[L = L_1 \\times L_2 \\times L_3 \\times \\cdots \\times L_k = \\prod_{j=1}^k L_j, \\text{ where } L_j= \\text{portion of } L \\text{for the } j^{th} \\text{ failure time given the risk set of } R(t_{(f)})\\] Once \\(L\\) is obtained, \\(\\ln L\\) is maximized by solving \\(\\frac{\\delta \\ln L}{\\delta \\beta_i} = 0\\) for \\((i=1, 2, \\cdots, p)\\) (# of parameters) over iteration 8.6.2 More about Hazard ratio Hazard ratio = \\(e^{\\hat{\\beta}}\\) In general, a hazard ratio (HR) is defined as the hazard for one individual divided by the hazard for a different individual. The two individuals being compared can be distinguished by their values for the set of predictors, that is, the \\(X\\)’s vs. \\(X^*\\)’s. Therefore, \\[ \\hat{HR} = \\frac{\\hat{h} (t, X^*)}{\\hat{h} (t, X)} = \\frac{h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i^*)}{h_0 (t) \\exp(\\sum_{i=1}^p \\beta_i X_i)} = \\frac{\\exp(\\sum_{i=1}^p \\beta_i X_i^*)}{\\exp(\\sum_{i=1}^p \\beta_i X_i)} = \\exp[{\\sum_{i=1}^p \\hat{\\beta_i}(X_i^* - X_i)}]\\] - Example: When \\(X_1\\) denotes (0, 1) exposure status, then \\(X_1^*=1\\), \\(X_1=0\\), thus \\[\\hat{HR} = \\exp[{\\sum_{i=1}^p \\hat{\\beta_i}(X_i^* - X_i)}] = \\exp[\\hat{\\beta_1}(1-0)]= \\exp(\\hat{\\beta_1})\\] - As with an odds ratio, it is easier to interpret an HR that exceeds the null value of 1 than an HR that is less than 1. Thus, the \\(X\\)’s are typically coded so that group with the larger hazard corresponds to \\(X^*\\), and the group with the smaller hazard corresponds to\\(X\\). 8.6.3 Maximum likelihood estimation Maximum likelihood estimation (MLE, ML) is a general approach to estimate that has become popular in many different areas of application. There are two reasons for this popularity. ML produces estimatros that have good large-sample properties. Provided that certain regularity conditions are met, ML estimators are consistent, asymptotically efficient, and asymptotically normal. Consistency: the estimates converge in probability to the true values as the sample gets larger, implying that the estimates will be approximately unbiased in large samples Asymptotically efficient: In large samples, the estimates will have standard errors that are approximately at least as small as those for any other estimation method Asymptotically normal: the sampling distribution of the estimates will be approximately normal in large samples, implying that we can use the normal and chi-square distributions to compute confidence intervals and p-values. It is often straightforward to derive ML estimators when there are no other obvious possibilities. One case is that ML handles nicely is data with censored observations. (OLS will leads to larger standard errors and there is little available theory to justify the construction of hypothesis tests or confidence intervals) The basic principle of ML is to choose as estimates those values that will maximize the probability of observing what we have, in fact, observed. The first step is write down a formula for the probability of the data as a function of the unknown parameters (i.e., constructing the likelihood function) The second step is to find the values of the unknown parameters that maek the value of this formula as large as possible (i.e., maximization) MLE Assume that we have \\(n\\) independent individuals \\((j=1,2,\\dots,n)\\). For each individual \\(i\\), the data consist of three parts: \\(t_i\\), \\(\\delta_i\\), and \\(x_i\\), where \\(t_i\\) is the time of the event or the time of censoring; \\(\\delta_i\\) is an indicator variable with a value of 1 if \\(t_i\\) is uncensored or 0 if right censored; and \\(x_i = [1\\; x_{i1}\\; \\dots \\;x_{ik}]\\) is a vector of covariates values (the 1 is for the intercept) (for simplicity, we treat them as fixed rather than random) Suppose that all the observations are uncensored. Because we are assuming independence, it follows that the probability of the entire data is found by taking the product of the probabilities of the data for every individual. Because \\(t_i\\) is assumed to be measured on a continuum, the probability that it will take on any specific value is 0. Instead, we represent the probability of each observation by the probability density function (p.d.f.), \\(f(t_i)\\). Thus, the probability (or likelihood) of the data is given by the following expression, where \\(\\prod\\) indicates repeated multiplication: \\[L=\\prod_{i=1}^{n} f_i(t_i)\\] Note that \\(f_i\\) is subscripted to indicate that each individual has a different p.d.f. that depends on the covariates. To proceed further, we need to substitute an expression for \\(f_i(t_i)\\) that involves covariates and the unknown parameters. Before we do this, however, let’s see how this likelihood is altered if we have censored cases. If an individual is censored at time \\(t_i\\), all we know is that the individual’s event time is greater than \\(t_i\\). But the probability of an event time greater than \\(t_i\\) is given by the survivor function \\(S(t)\\) evaluated at time \\(t_i\\). Now suppose that we have \\(r\\) uncensored observations and \\(n-r\\) censored observations. If we arrange the data so that all the uncensored cases come first, we can write the likelihood as \\[L=\\prod_{i=1}^{r}f_i(t_i) \\prod_{i=r+1}^{n} S_i(t_i)\\] where, again, we subscript the survivor function to indicate that it depends on the covariates. Using the censoring indicator \\(\\delta\\), we can equivalently write this as \\[L=\\prod_{i=1}^{n}[f_i(t_i)]^{\\delta_i} [S_i(t_i)]^{1-\\delta_i}\\] Here \\(\\delta_i\\) acts as a switch, turning the appropriate function on or off, depending on whether the observation is censored. As a result, we do not need to order the observations by censoring status. This last expression applies to all the models with right-censored data, shows how consored and uncensored cases are combined in ML estimation. Once we choose a particular model, we can substitute appropriate expressions for the p.d.f. and the survivor function. For example, the exponential model is \\(f_i(t_i) = \\lambda_i e^{-\\lambda_i t_i}\\) and \\(S_i(t_i)=e^{-\\lambda_i t_i}\\), where \\(\\lambda_i=\\exp(-\\beta x_i)\\) and \\(x_i\\) is a vector of coefficients. Substituting, we get \\[L=\\prod_{i=1}^{n} [\\lambda_i e^{-\\lambda_i t_i}]^{\\delta_i}[e^{-\\lambda_i t_i}]^{1-\\delta_i}=\\prod\\lambda_i^{\\delta_i}e^{-\\lambda_i t_i}\\] Although this expression can be maximized directly, it is generally easier to work with the natural logarithm of the likelihood function because products get converted into sums and exponents become coefficients. Because the logarithm is an increasing function, whatever maximizes the logarithm also maximizes the original function. Taking the logarithm of the likelihood, we get \\[\\log L= \\sum_{i=1}^{n} \\delta_i \\log \\lambda_i - \\sum_{i=1}^{n} \\lambda_i t_i = -\\beta \\sum_{i=1}^{n} \\delta_i x_i - \\sum_{i=1}^{n} t_i e^{-\\beta x_i}\\] Now we are ready for step 2, finding values of \\(\\beta\\) that make this expression as large as possible. There are many different methods for maximizing functions like this. One well-known approach is to find the derivative of the function with respect to \\(\\beta\\), set the derivative equal to 0, and then solve for \\(\\beta\\). Taking the derivative and setting it equal to 0 gives us \\[\\sum_{i=1}^{n} \\delta_i x_i = \\sum_{i=1}^{n} x_i t_i e^{-\\beta x_i}\\] Because \\(x_i\\) is a vector, this is actually a system of \\(k+1\\) equations, one for each element of \\(\\beta\\). While these equations are not terribly complicated, the problem is that they involve nonlinear functions of \\(\\beta\\). Consequently, except in special cases (like a single dichotomous \\(x\\) variable), there is no explicit solution. Instead, we have to rely on iterative methods, which amount to successive approximations to the solution until the approximations converge to the correct value. Again, there are many different methods for doing this. All give the same solution, but they differ in such factors as speed of convergence, sensitivity to starting values, and computational difficulty at each iteration. (e.g., the Newton-Raphson algorithm) 8.6.4 Partial likelihood estimation \\[h_i(t)=h_0(t)\\exp(\\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n )\\] The likelihood function for the proportional hazards model of this equation can be factored into two parts: one part depends on both \\(h_0(t)\\) and \\(\\beta=[\\beta_1, \\beta_2,\\dots,\\beta_n]^{&#39;}\\), the vector of coefficients the other part depends on \\(\\beta\\) \\(=[\\beta_1, \\beta_2,\\dots,\\beta_n]^{&#39;}\\) alone What partial likelihood does, in effect, is discard the first part and treat the second part - the partial likelihood function - as though it were an ordinary likelihood function. You get estimates by finding values of \\(\\beta\\) that maximize the partial likelihood. Because there is some information about \\(\\beta\\) in the discarded portion of the likelihood function, the resulting estimates are not fully efficient. There standard errors are larger than they would be if you used the entire likelihood function to obtain the estimates. In most cases, however, the loss of efficiency is quite small. What we gain in return is robustness because the estimates have good properties regardless of the actual shape of the baseline hazard function. To be specific, partial likelihood estimates still have two of the three standard properties of ML estimates: they are consistent and asymptotically normal. In other words, in large samples they are approximately unbiased and their sampling distribution is approximately normal. Another interesting property of partial likelihood estimates is that they depend only on the ranks of the event times, not their numerical values. This implies that any monotonic transformation of the event times will leave the coefficient estimates unchanged. For example, we could add a constant to everyone’s event time, multiply the result by a constant, take the logarithm, and then take the square root - all without producing the slightest change in the coefficients or their standard errors. Now let’s take a closer look at how the partial likelihood works. Using the same notation as MLE, Assume that we have \\(n\\) independent individuals \\((j=1,2,\\dots,n)\\). For each individual \\(i\\), the data consist of three parts: \\(t_i\\), \\(\\delta_i\\), and \\(x_i\\), where \\(t_i\\) is the time of the event or the time of censoring; \\(\\delta_i\\) is an indicator variable with a value of 1 if \\(t_i\\) is uncensored or 0 if right censored; and \\(x_i = [1 x_{i1} \\dots x_{ik}\\) is a vector of covariates values (the 1 is for the intercept) (for simplicity, we treat them as fixed rather than random) we can write the partial likelihoods as a product of the likelihoods for all the events that are observed. Thus if \\(J\\) is the number of events, \\[PL=\\prod_{j=1}^{J}L_j\\] where \\(L_j\\) is the likelihood for the \\(J^{th}\\) event. Next we need to see how the individual \\(L_J\\)s are constructed. This is best explained by way of an example. First, we arrange data in ascending order by survival time, which is convenient for constructing the partial likelihood. Let’s say that the first death occurred to patient 1 in month 5. To construct the partial likelihood (\\(L_1\\)) for this event, we ask the following quetion: Given that a death occurred in month 5, what is the probability that it happened to patient 1 rather than to one of the other patients? The answer is the hazard for patient1 at month 5 divided by the sum of the hazards for all the patients who were at risk of death in that same month. At month 5, let all other 45 patients were at risk of death, so the probability is \\[L_1=\\frac{h_1(5)}{h_1(5)+h_2(5)+\\dots +h_{45}(5)}\\] The second death occurred to patient 2 in month 8. Again we ask, given that a death occurred in month 8, what is the probability that it occurred to patient 2 rather than to one of other patients ar risk? Patient 1 is no longer at risk of death bacuase she already died. So \\(L_2\\) has the same form as \\(L_1\\), but the hazard for patient 1 is removed from the denominator: \\[L_2=\\frac{h_2(8)}{h_2(8)+h_3(8)+\\dots +h_{45}(8)}\\] The set of all individuals who are at risk at a given point in time is often referred to as the risk set. At time 8, the risk set consists of patients 2 through 45, inclusive. We continue in this way for each successive death, deleting from the denominator the hazards for all those who have already died. Also deleted from the denominator are those who have been censored at an earlier point in time. Until now, we made no assumptions about the form of the hazard function. Now, we invoke the proportional hazards model and substitute the expression for the hazard into the expression for \\(L_1\\), \\[L_1=\\frac{h_0(5)\\exp[\\beta x_1]}{h_0(5)\\exp[\\beta x_1]+h_0(5)\\exp[\\beta x_2]+\\dots +h_0(5)\\exp[\\beta x_{45}]}\\] where \\(x_i\\) is the value of \\(x\\) for the \\(i^{th}\\) patient. This leads to a considerable simplication because the unspecified function \\(h_0(5)\\) is common to every term in the expression. Canceling, we get \\[L_1=\\frac{\\exp[\\beta x_1]}{\\exp[\\beta x_1]+\\exp[\\beta x_2]+\\dots +\\exp[\\beta x_{45}]}\\] It is this cancellation of the \\(\\lambda\\)s that makes it possible to estimate the \\(\\beta\\) coefficients without haveing to specify the baseline function. We can also test that the partial likelihood depends only on the order of the event times, not on their exact values. Although the first death occurred in month 5, \\(L_1\\) would be exactly the same if it had occurred at any time from 0 up to (but not including) 8, the month of the second event. Similarly, \\(L_2\\) would have been the same if the second death had occurred any time greater than 5 and less than 10 (the month of the third death). Therefore, a general expression for the partial likelihood for data with time-invariant covariates from a proportional hazards model is \\[PL=\\prod_{i=1}^{n}[\\frac{\\exp(\\beta x_i)}{\\sum_{j=1}^{n} Y_{ij}\\exp{\\beta x_j}}]^{\\delta_i}\\] where \\(Y_{ij}=1\\) if \\(t_j \\le t_i\\); and \\(Y_{ij}=0\\) if \\(t_j &lt; t_i\\) The \\(Y\\)s are just a convenient mechanism for excluding from the denominator those individuals who have already experienced the event and are, thus, not part of the risk set. Although this expression has the product taken over all individuals rather than over all events, the terms corresponding to censored observations are effectively excluded because \\(\\delta_i=0\\) for those cases. This expression is not valid for tied event times, but it does allow for ties between one event time and one or more censoring times. Once the partial likelihood is constructed, we can maximize it with respect to \\(\\beta\\) just like an ordinary likelihood, which is \\[\\log PL=\\sum_{i=1}^{n} \\delta_i [\\beta x_i - \\log \\sum_{j=1}^{n} Y_{ij}\\exp{\\beta x_j}]\\] Most partial likelihood programs use some version of the Newton-Raphson alhorithm to maximize this function with respect to \\(\\beta\\). To account for the time-varying covariates9, we need to modify the partial likelihood function to accommodate these types of variables. Essentially, at each failure time, there are a certain number of patients at risk, and one fails. However, the contributions of each subject can change from one failure time to the next. The hazard function if given by \\(h(t)=h_0 (t)e^{z_k(t_i)\\beta}\\), where the covariate \\(z_k(t_i)\\) is the value of the time-varying covariate for the \\(k^{th}\\) subject at time \\(t_i\\). Sample of six patients from the Stanford heart transplant dataset The maximum partial likelihood is \\[L(\\beta)=\\prod_{i=1}^{D} \\frac{\\psi_{ii}}{\\sum\\limits_{k \\in R_i} \\psi_{ki}}, \\;\\;\\; where\\;\\; \\psi_{ki}=e^{z_k(t_i)\\beta} \\] When the covariates were fixed at time 0, so that \\(z_k(t_i)\\)=\\(z_k\\) for all failure times \\(t_i\\), and the denominator at each time could be computed by, as time passes, successively deleting the value of \\(\\psi_i\\) for the subject (or subjects) that failed at that time. With a time dependent covariate, by contrast, the entire denominator has to be recalculated at each failure time, since the values of the covariates for each subject may change from one failure time to the next. For example, the patient #2 is the first to fail, at \\(t=5\\). At this time, all six patients are at risk, but only one, patient #95, has had a transplant at this time. So the denominator for the first factor is \\(5+e^{\\beta}\\), and the numerator is 1, since it was a non-transplant patient who died. Patient #12 is the next to die, at time \\(t=7\\), and none of the patients in the risk set have changed their covariate value. But when the third patient #95 dies at \\(t=15\\), one of the other patients (#10) has switched from being a non-transplant patient to one who has had one. There are now four patients at risk, of which two (#10 and #95) are transplant patients. The denominator is thus \\(2+2e^{\\beta}\\) and the numerator is \\(e^{\\beta}\\), since it was a transplant patient that died. Therefore, the full partial likelihood in this example is \\[L(\\beta)=\\frac{1}{5+e^{\\beta}}\\cdot\\frac{1}{4+e^{\\beta}}\\cdot\\frac{e^{\\beta}}{2+2e^{\\beta}}\\cdot\\frac{1}{2+e^{\\beta}}\\cdot\\frac{e^{\\beta}}{1+e^{\\beta}}\\cdot\\frac{e^{\\beta}}{e^{\\beta}}\\] Essentially, this approach divides the time data for patients who had a heart transplant into two time periods, one before the transplant and one after. For example, patient #10 was a non-transplant patient from entry until day 11. Since that patient received a transplant at that time, the future for that patient, had he or she not received a transplant, is unknown. Thus, we censor that portion of the patient’s life experience at \\(t=11\\). Following the transplant, we start a new record for patient #10. This second piece of the record is left-truncated (i.e., patient’s survival experience with the transplant starts at that point) at time \\(t=11\\), and a death is recorded at time \\(t=57\\). For the first part of this patient’s experience, the ‘start’ time is 0, and the ‘stop’ time is 11, which is recorded as a censored observation. For the second piece of that patient’s experience, the start time is 11 and the stop time is 57. Thus, to put the sdata in start-stop format, the record of every patient with no transplant is carried forward as is, where as the record of each patient who received a transplant is split into pre-transplant and post-transplant records. Use “tmerge” in R to simplify this conversion. Start-stop counting process 8.6.5 Dependence among the observations A common reaction to the methods described here is that there must be something wrong. In general, when multiple observations are created for a single individual, it’s reasonable to suppose that those observations are not independent, thereby violating a basic assumption used to construct the likelihood function. The consequence of dependence is usually biased standard error estimates and inflated test statistics. Even worse, there are different numbers of observations for different individuals, so some apper to get more weight than others. While concern about dependence is often legitimate, it is not applicable here. In this case, the creation of multiple observations in not an ad-hoc method; rather, it follows directly from factoring the likelihood function for the data. The basic idea is this: in its original form, the likelihood for data with no censoring can be written as a product of probabilities over all \\(n\\) observations, as follows: \\[\\prod_{j=1}{n} P(T_i = t_i)\\] where \\(T_i\\) is the random variable and \\(t_i\\) is the particular value of observed for individual \\(i\\). Each of the probabilities can be factored in the following way. If \\(t_i=5\\), we have \\[P(T_i=5)=P_{i5}(1-P_{i4})(1-P_{i3})(1-P_{i2})(1-P_{i1})\\] where, again, \\(P_{it}\\) is the conditional probability of an event at time \\(t\\), given that an event has not already occurred. This factorization follows directly from the definition of conditional probability. Each of the five terms behaves as if it came from a distinct, independent observation. This lack of dependency holds only when no individual has more than one event. When events are repeatble, there is a real problem of dependence. But the problem is neither more nor less serious than it is for other methods fo survival analysis. 8.7 Tied or Discrete Data Analysis10 8.7.1 Example: Recidivism in the U.S.11 The dataset considered here is analyzed in Wooldridge (2002) and credited to Chung, Schmidt and Witte (1991). The data pertain to a random sample of convicts released from prison between July 1, 1977 and June 30, 1978. Of interest is the time until they return to prison. The information was collected retrospectively by looking at records in April 1984, so the maximum possible length of observation is 81 months. The data are available in binary format from the Stata website and consists of 1445 observations on 18 variables. workprg: an indicator of participation in a work program priors: the number of previous convictions tserved: the time served rounded to months felon: an indicator of felony sentences alcohol: an indicator of alcohol problems drugs:an indicator of drug use history black: an indicator for African Americans married: an indicator if married when incarcerated educ: the number of years of schooling, and age: in months. durat: represents time in months until return to prison or end of follow up cens: the censoring indicator and is coded 1 if the observation was censored (i.e. the individual had not returned to prison) 8.7.2 Data management library(survival) library(dplyr) library(foreign) recid &lt;- read.dta(&quot;https://www.stata.com/data/jwooldridge/eacsap/recid.dta&quot;) ## Warning in read.dta(&quot;https://www.stata.com/data/jwooldridge/eacsap/recid.dta&quot;): ## cannot read factor labels from Stata 5 files head(recid) ## black alcohol drugs super married felon workprg property person priors educ ## 1 0 1 0 1 1 0 1 0 0 0 7 ## 2 1 0 0 1 0 1 1 1 0 0 12 ## 3 0 0 0 0 0 0 1 1 0 0 9 ## 4 0 0 1 1 0 1 1 1 0 2 9 ## 5 0 0 1 1 0 0 0 0 0 0 9 ## 6 1 0 0 1 0 0 1 0 0 1 12 ## rules age tserved follow durat cens ldurat ## 1 2 441 30 72 72 1 4.276666 ## 2 0 307 19 75 75 1 4.317488 ## 3 5 262 27 81 9 0 2.197225 ## 4 3 253 38 76 25 0 3.218876 ## 5 0 244 4 81 81 1 4.394449 ## 6 0 277 13 79 79 1 4.369448 recid$fail &lt;- 1 - recid$cens recidx &lt;- survSplit(recid, cut = seq(12, 60, 12), start = &quot;t0&quot;, end = &quot;durat&quot;, event = &quot;fail&quot;, episode = &quot;interval&quot;) labels &lt;- paste(&quot;(&quot;,seq(0,60,12),&quot;,&quot;,c(seq(12,60,12),81), &quot;]&quot;,sep=&quot;&quot;) recidx &lt;- mutate(recidx, exposure = durat - t0, interval = factor(interval + 1, labels = labels)) mf &lt;- Surv(durat, fail) ~ workprg + priors + tserved + felon + alcohol + drugs + black + married + educ + age 8.7.3 The treatment of ties Breslow’s method, the standard formular for partial likelihood estimation with tied data, is often a poor approximation when there are many ties. This problem was remedied by two exact methods, one that assumed that ties result from imprecise measurement and another that assumed that events really occur at the same (discrete) time. Efron’s method provides a good approximation to the exact methods. 8.7.3.1 The Exact method Let’s begin with the exact method bacause its underlying model is probably more plausible for most application. Since events can occur at any point in time, it’s reasonable to suppose that ties are merely the result of imprecise measurement of time and that there is a true but unknown time ordering for the tied events. If we knew that ordering, we could construct the partial likelihood in the usual way. In the absence of any knowledge of that ordering, however, we have to consider all the possibilities. For example, with five tied events, there are \\(5!=120\\) different possible ordering. Let’s denote each of those possibilities by \\(A_i\\), where \\(i=1, 2, \\dots, 120\\). What we want is the probability of the union of those possibilities, that is, \\(P\\)(\\(A_1\\) or \\(A_2\\) or \\(\\dots\\) or \\(A_{120}\\)). Now the fundamental law of probability theory is that the probability of the union of a set of mutually exclusive event is just the sum of the probabilities for each of the events. Therefore, we can write, for example, the five tied event at \\(L_8\\) as \\[L_8 = \\sum_{i}^{120} p(A_i)\\] Each of these 120 probabilities is just a standard partial likelihood. Suppose, for example, that we arbitrarily label the five events at time 8 with the numbers 8, 9, 10, 11, and 12, and suppose further that \\(A_1\\) denotes the ordering \\({8, 9, 10, 11, 12}\\). Then On the other hand, if \\(A_2\\) denotes the ordering \\({9, 10, 11, 12}\\), we have We continue in this way for the rest of the combinations. 8.7.3.2 The Breslow and Efron method Early recognition of these computational difficulties in the exact method led to the development of approximations. If the exact methods are too time-comsuming, use the Efron approximation. It is nearly always better than the Breslow method, with virtually no increase in computing time. Farewell and Prentice (1980) showed that the Breslow approximation deteriorates as the number of ties at a particular point in time becomes a large proportion of the number of cases at risk. 8.7.3.3 Comparisons Let us compare all available methods of handling ties. As is often the case, the Efron method comes closer to the exact partial likelihood estimate with substantial;y less computational effort, although in this application all methods yield very similar results. cox_efron &lt;- coxph(mf, data = recidx, ties=&quot;efron&quot;) cox_beslow &lt;- coxph(mf, data = recidx, ties=&quot;breslow&quot;) cox_exact &lt;- coxph(mf, data = recidx, ties=&quot;exact&quot;) data.frame(exactp = coef(cox_exact), efron = coef(cox_efron), breslow = coef(cox_beslow)) ## exactp efron breslow ## workprg 0.111590748 0.111560134 0.111337070 ## priors 0.096271298 0.095985297 0.095859808 ## tserved 0.015595528 0.015558389 0.015519980 ## felon -0.334451818 -0.333671514 -0.333261160 ## alcohol 0.478596659 0.477865298 0.477164506 ## drugs 0.327465984 0.327094665 0.326467040 ## black 0.504462343 0.503957605 0.503016140 ## married -0.153975705 -0.153542571 -0.153523788 ## educ -0.024847512 -0.024770080 -0.024746660 ## age -0.004199204 -0.004195258 -0.004187421 8.8 The discrete method The discrete method is also an exact method but one based on a fundamentally different model. In fact, this is NOT a proportional hazard model at all. The model does fall within the framework of Cox regression, however, because it was proposed by Cox in his original 1972 paper and because the estimation method is a form of partial likelihood. Unlike the exact model, which assumes that ties are merely the result of imprecise measurement of time, the discrete model assumes that time is really discrete. When two or more events appear to happen at the same time, there is no underlying ordering - they really happened at the same time. Cox’s model for discrete-time data can be described as follows. The time variable \\(t\\) can only take on integer values. Let \\(P_{it}\\) be the conditional probability that individual \\(i\\) has an event at time \\(t\\), given that an event has not already occurred to that individual. This probability is sometimes called the discrete-time hazard. The model says that \\(P_{it}\\) is related to the covariates by a logistic regression equation: \\[\\log[\\frac{P_{it}}{1-P_{it}}]=\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_i x_i\\] The expression on the left side of the equation is the logit or log-odds of \\(P_{it}\\). On the right side, we have a linear function of the covariates, plus a term \\(\\beta_0\\) that is a set of constants that can vary arbitrarily from one time point to another. This model can be described as proportional odds model. The odds that individual \\(i\\) has an event at time \\(t\\) (given that \\(i\\) did not already have an event) is \\(O_{it}=\\frac{P_{it}}{1-P_{it}}\\). The model implies that the ratio of the odds for any two individuals \\(\\frac{O_{it}}{O_{jt}}\\) does not depend on time (although it may vary with covariates) Estimation with partial likelihood: \\(PL=\\sum_{j=1}^{J} L_i\\), where \\(L_j\\) is the partial likelihood of the \\(j^{th}\\) event. This approach can be very cumbersome. Let’s say that at time 1, 22 people had events out of 100 people who were at risk. To get \\(L_1\\), we ask the question: given that 22 events occurred, what is the probability that they occurred to these particular 22 people rather than to some different set of 22 people from among the 100 at risk? How many different ways are there of selecting 22 people out of a set of 100? It’s \\(_{22} C_{ 100} = 7.3321 \\times 10 ^{21}\\)…. In general, for a given set \\(q\\), let \\(\\psi_q\\) be the product of the odds for all the individuals in that set. Thus, if the individuals who actually experienced events are labeled \\(i=1\\) to \\(n\\), we have \\[\\psi_1 = \\prod_{i=1}^{n} O_{i1}\\] We can then write \\[L_1 = \\frac{\\psi_1}{\\psi_1+\\psi_2+ \\cdots + \\psi_q}\\] This looks like a simple expression, but there are trillions of terms being summed in the denominator. Fortunately, there is a recursive algorithm that makes it practical, even with substantial numbers of ties. Estimation with maximum likelihood method The basic approach Each individual’s survival history is broken down into a set of discrete time units that are treated as distinct observation After pooling these observations, the next step is to estimate a binary regression model predicting whether an event did or did not occur in each time unit Covariates are allowed to vary over time from one time unit to another This approach has two versions depending on the form of the binary regression model: By specifying a logit link, we get estimates of the discrete-time proportional odds model (this model is identical to the model estimates when we specify the “ties=discrete” in SAS PROC PHREG) By specifying a complementary log-log link, we get estimates of an underlying proportional hazard modeling continuous time. This is identical to the model that is estimated when we specify the “ties=exact” option in SAS PROC PHREG) Advantages This method does not rely on approximations The computations are manageable even with large data sets This method is particularly good at handling large numbers of time-dependent covariates This method makes it easy to test hypotheses about the dependence of the hazard on time This approach is similar to those of the piecewise exponential model and the counting process. The main difference is that those methods assumed that we know the exact time of the event within a given interval. By contrast, the discrete model presume that we know only that an event occured within a given interval. 8.8.1 Continuous and Discrete Models12 Let’s have another look at the recidivism data. We will split duration into single years with an open-ended category at 5+ and fit a piecewise exponential model with the same covariates as Wooldridge. We will then treat the data as discrete, assuming that all we know is that recidivism occurred somewhere in the year. We will fit a binary data model with a logit link, which corresponds to the discrete time model, and using a complementary-log-log link, which corresponds to a grouped continuous time model. 8.8.1.1 A Piecewise Exponential Model13 This model is equivalent to the Poisson regression for a positive mean, which is a GLM assumes a Poisson distribution for \\(Y\\) and uses the log link function. GLMs for the Poisson mean can use the identity link, but it is more commone to model the log of the mean. Like the linear predictor \\(\\beta_0 + \\beta_1 x_1\\), the log of the mean can take any real-number value. \\[\\log \\mu = \\beta_0 + \\beta_1 x_1\\] The mean satisfies the exponential relationship \\[\\mu = \\exp(\\beta_0 + \\beta_1 x_1) = \\exp(\\beta_0)\\exp(\\beta_1 x_1)\\] A one-unit increase in \\(x\\) has a multiplicative impact of \\(\\exp(\\beta_1)\\) on \\(\\mu\\): the mean of \\(Y\\) at \\(x+1\\) equals the mean of \\(Y\\) at \\(x\\) multiplied by \\(\\exp(\\beta_1)\\). If \\(\\beta_1=0\\), then \\(\\exp(\\beta_1)=\\exp(0)=1\\) and the multiplicative factor is 1. Then, then mean of \\(Y\\) does not change at \\(x\\) changes. If \\(\\beta_1 &gt;0\\), then \\(\\exp(\\beta_1)&gt;1\\), and the mean of \\(Y\\) increases as \\(x\\) increases. If \\(\\beta_1 &lt;0\\), then \\(\\exp(\\beta_1)&lt;1\\), and the mean of \\(Y\\) decreases as \\(x\\) increases. Overdispersion Count data often vary more than we would expect if the response distribution truly were Poisson. A common cause of overdispersion is heterogeneity among subjects. If the variance equals the mean wehn all relevant variables are controlled, it exceeds the mean wehn only a subset of those variables is controlled. Overdispersion is not an issue in ordinary regression models assuming normally distributed \\(Y\\), because the normal has a seperate parameter from the mean (i.e., the variance, \\(\\sigma^2\\)) to describe variability. However, the variance equals the mean in the Poisson distribution, thus overdispersion is common in applying Poisson GLMs to counts. Negative binomial regression When the Poisson means follw a gamma distribution, unconditionally the distribution is the negative binomial. The negative binomial is another distribution that is concentrated on the nonnegative integers. Unlike the Poisson, it has an additional parameter such that the variance can exceed the mean. \\[E(Y)=\\mu, \\;\\;\\; Var(Y)=\\mu + D\\mu^2\\] The index, \\(D\\), which is nonnegative, is called a dispersion parameter. The negative binomial distribution arises as a type of mixture of Poisson distributions. Greater heterogeneity in the Poisson means results in a larger value of \\(D\\). As \\(D \\to 0\\), \\(Var(Y) \\to \\mu\\) and the negative binomial distribution converges to the Poisson variability. Negative binomial GLMs for counts express \\(\\mu\\) in terms of explanatory variables. Most common is the log link, as in Poisson loglinear models, but sometimes the identity link is adequate. It is common to assume that the dispersion parameter \\(D\\) takes the same value at all predictor values, much as regression models for a normal response take the variance parameter to be constant. mmf &lt;- fail ~ interval + workprg + priors + tserved + felon + alcohol + drugs + black + married + educ + age pwe &lt;- glm(mmf, offset = log(exposure), data = recidx, family = poisson) coef(summary(pwe)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.830127469 0.280267334 -13.6659789 1.621090e-42 ## interval(12,24] 0.036531989 0.109361775 0.3340471 7.383440e-01 ## interval(24,36] -0.373815644 0.129611909 -2.8841150 3.925154e-03 ## interval(36,48] -0.811543632 0.156401452 -5.1888497 2.115971e-07 ## interval(48,60] -0.938231113 0.168321156 -5.5740534 2.488794e-08 ## interval(60,81] -1.547177936 0.203348918 -7.6084886 2.773196e-14 ## workprg 0.083829106 0.090794162 0.9232874 3.558575e-01 ## priors 0.087245826 0.013473463 6.4753825 9.457203e-11 ## tserved 0.013008862 0.001685901 7.7162667 1.197865e-14 ## felon -0.283925203 0.106148770 -2.6747856 7.477705e-03 ## alcohol 0.432442493 0.105721133 4.0904073 4.306163e-05 ## drugs 0.274714115 0.097863462 2.8071162 4.998720e-03 ## black 0.433555955 0.088362277 4.9065729 9.268154e-07 ## married -0.154047742 0.109211869 -1.4105403 1.583802e-01 ## educ -0.021416177 0.019444026 -1.1014271 2.707108e-01 ## age -0.003580003 0.000522249 -6.8549738 7.132557e-12 8.8.1.2 A Logit Model For a discrete-time survival analysis we have to make sure we only include intervals with complete exposure, where we can classify the outcome as failure or survival. The convicts were released between July 1, 1977 and June 30, 1978 and the data were collected in April 1984, so the length of observation ranges between 70 and 81 months. We therefore restrict our attention to 5 years or 60 months. (We could go up to 6 years or 72 months for some convicts, but unfortunately we don’t have the date of release, so we can’t identify these cases and must censor everyone at 60.) recidx &lt;- filter(recidx, interval != &quot;(60,81]&quot;) logit &lt;- glm(mmf, data = recidx, family = binomial) # no offset coef(summary(logit)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.140802599 0.3084159337 -3.6989094 2.165279e-04 ## interval(12,24] 0.030528163 0.1193582701 0.2557692 7.981291e-01 ## interval(24,36] -0.413140262 0.1384064532 -2.9849783 2.835984e-03 ## interval(36,48] -0.864148699 0.1639957690 -5.2693353 1.369186e-07 ## interval(48,60] -0.993662524 0.1756321916 -5.6576332 1.534747e-08 ## workprg 0.110988653 0.1003087410 1.1064704 2.685230e-01 ## priors 0.099292063 0.0164653717 6.0303566 1.635983e-09 ## tserved 0.014922136 0.0021429307 6.9634244 3.320994e-12 ## felon -0.319662098 0.1178116529 -2.7133318 6.661038e-03 ## alcohol 0.472499810 0.1184176515 3.9901130 6.604183e-05 ## drugs 0.316729032 0.1086092071 2.9162264 3.542934e-03 ## black 0.458027506 0.0973977193 4.7026512 2.568049e-06 ## married -0.204807338 0.1204592720 -1.7002206 8.908944e-02 ## educ -0.026725931 0.0215052145 -1.2427651 2.139544e-01 ## age -0.004023087 0.0005840427 -6.8883431 5.644594e-12 8.8.1.3 A Complementary Log-log Model Finally we use a complementary log-log link cloglog &lt;- glm(mmf, data = recidx, family = binomial(link = cloglog)) coef(summary(cloglog)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.238795113 0.2893607427 -4.2811444 1.859347e-05 ## interval(12,24] 0.021613951 0.1095604758 0.1972787 8.436094e-01 ## interval(24,36] -0.392613793 0.1297681301 -3.0255024 2.482204e-03 ## interval(36,48] -0.824996440 0.1566132100 -5.2677321 1.381194e-07 ## interval(48,60] -0.948338328 0.1684247392 -5.6306356 1.795467e-08 ## workprg 0.104466422 0.0934228228 1.1182109 2.634769e-01 ## priors 0.088706984 0.0145113085 6.1129556 9.780261e-10 ## tserved 0.013266906 0.0018142064 7.3127875 2.616567e-13 ## felon -0.288542238 0.1096491770 -2.6315039 8.500789e-03 ## alcohol 0.439780479 0.1090998881 4.0309893 5.554258e-05 ## drugs 0.299102966 0.1003895869 2.9794222 2.887925e-03 ## black 0.427210098 0.0910947168 4.6897352 2.735589e-06 ## married -0.183040394 0.1136073451 -1.6111669 1.071433e-01 ## educ -0.023334468 0.0202349457 -1.1531767 2.488379e-01 ## age -0.003851008 0.0005486362 -7.0192372 2.230827e-12 8.8.1.4 Comparison of Estimates cbind(coef(pwe)[-6], coef(cloglog), coef(logit)) ## [,1] [,2] [,3] ## (Intercept) -3.830127469 -1.238795113 -1.140802599 ## interval(12,24] 0.036531989 0.021613951 0.030528163 ## interval(24,36] -0.373815644 -0.392613793 -0.413140262 ## interval(36,48] -0.811543632 -0.824996440 -0.864148699 ## interval(48,60] -0.938231113 -0.948338328 -0.993662524 ## workprg 0.083829106 0.104466422 0.110988653 ## priors 0.087245826 0.088706984 0.099292063 ## tserved 0.013008862 0.013266906 0.014922136 ## felon -0.283925203 -0.288542238 -0.319662098 ## alcohol 0.432442493 0.439780479 0.472499810 ## drugs 0.274714115 0.299102966 0.316729032 ## black 0.433555955 0.427210098 0.458027506 ## married -0.154047742 -0.183040394 -0.204807338 ## educ -0.021416177 -0.023334468 -0.026725931 ## age -0.003580003 -0.003851008 -0.004023087 As one would expect, the estimates of the relative risks based on the c-log-log link are closer to the continuous time estimates than those based on the logit link. This result makes sense because the piecewise exponential and c-log-log link models are estimating the same continuous time hazard, one from continuous and one from grouped data, while the logit model is estimating a discrete time hazard. Recall that in a continuous time model the relative risk multiplies the hazard or instantaneous failure rate, whereas in a discrete time logit model it multiplies the conditional odds of failure at a given time (or in a given time interval) given survival to that time (or the start of the interval). Interpretation of the results should take this fact into account. All three approaches, however, lead to similar predicted survival probabilities. 8.8.2 Interval censoring14 Discrete data are often the result of interval-censoring. Events might happen in a continuous range of time, but they can only be observed at discrete moments (e.g., longitudinal data by waves). The modeling paradigm for interval-censored survival data is essentially the same as for non-interval-censored data. Interpretation and presentation of the results of a fitted proportional hazards model is identical for the two types of data. However, model building with interval-censored data uses the binary regression likelihood if intervals are the same for all subjects. This implies that model building details, such as variable selections, identification of the scale of continuous covariates, and inclusion of interactions, use techniques based on binary regression modeling with the complimentary log-log model. 8.8.2.1 Conditional logistic regression and stratified Cox model15 Under a particular data structure, the loglikelihood for a conditional logistic regression model is the same with loglikelihood from a Cox model. A stratified Cox model with each case/control group assigned to its own stratum, time set to a constant, status of 1=case 0=control, and using the exact partial likelihood has the same likelihood formula as a conditional logistic regression. The clogit routine creates the necessary dummy variable of times (all 1) and the strata, then calls coxph. Stratified Cox model cox1 &lt;- coxph(Surv(durat, fail) ~ workprg + married + educ + age + strata(black), data = recidx, ties=&quot;efron&quot;) coef(summary(cox1)) ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## workprg 0.156200755 1.1690609 0.0897549296 1.740303 8.180586e-02 ## married -0.323341332 0.7237268 0.1133119425 -2.853550 4.323368e-03 ## educ -0.049860264 0.9513624 0.0190833630 -2.612761 8.981412e-03 ## age -0.001898137 0.9981037 0.0004532662 -4.187687 2.818123e-05 Conditional logistic model clogit &lt;- clogit(fail ~ workprg + married + educ + age + strata(black), data = recidx, method=c(&quot;efron&quot;)) coef(summary(clogit)) ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## workprg 0.151890146 1.1640324 0.0897545659 1.692283 9.059199e-02 ## married -0.310120691 0.7333584 0.1132727550 -2.737822 6.184746e-03 ## educ -0.047916781 0.9532131 0.0191298600 -2.504816 1.225151e-02 ## age -0.001812212 0.9981894 0.0004512763 -4.015749 5.925726e-05 Goran Brostrom, Event History Analysis with R; Kleinbaum and Klein, Survival Analysis↩︎ This section is a summary from Cleves et al, An Introduction to Survival Analysis Using Stata↩︎ Moore, Dirk. 2016. Applied Survival Analysis Using R↩︎ Allison, P. Survival Analysis Using SAS. \\(2^{nd}\\) eds.↩︎ Germán Rodríguez, Survival Analysis, https://data.princeton.edu/pop509/recid1↩︎ Germán Rodríguez, Survival Analysis, https://data.princeton.edu/pop509/recid3↩︎ Agresti, Categorical Data Analysis; Germán Rodríguez, Survival Analysis, https://data.princeton.edu/pop509/recid3↩︎ Hosmer, Remeshow, and May. Applied Survival Analysis↩︎ Conditional logistic regression. https://rdrr.io/cran/survival/man/clogit.html↩︎ "],["add-health-project.html", "Chapter 9 Add Health Project 9.1 Library 9.2 Access datasets 9.3 Load 5-wave sample 9.4 Long-form dataset (wave-person) 9.5 Exploring variables in Add Health 9.6 Outcomes, exposures, and confounders 9.7 Demographic variables - time-invariant 9.8 Merging datasets 9.9 Data management, recoding, and so on 9.10 Analytic approach", " Chapter 9 Add Health Project 9.1 Library library(tidyverse) library(data.table) library(lme4) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack library(Matrix) 9.2 Access datasets Let’s use the public datasets available at https://www.icpsr.umich.edu/web/ICPSR/studies/21600?archive=ICPSR&amp;q=21600 9.3 Load 5-wave sample First, each RDA dataset will be loaded and then saved as WAVE0X. After assigning a WAVE variable, we will keep the WAVE0X dataset and WX datasets only. #1st wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0001/21600-0001-Data.rda&quot;) wave01 &lt;- da21600.0001 wave01$wave &lt;- 1 rm(da21600.0001) w1 = subset(wave01, select = c(AID, wave)) #2nd wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0005/21600-0005-Data.rda&quot;) wave02 &lt;- da21600.0005 wave02$wave &lt;- 2 rm(da21600.0005) w2 = subset(wave02, select = c(AID, wave)) #3rd wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0008/21600-0008-Data.rda&quot;) wave03 &lt;- da21600.0008 wave03$wave &lt;- 3 rm(da21600.0008) w3 = subset(wave03, select = c(AID, wave)) #4th wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0022/21600-0022-Data.rda&quot;) wave04 &lt;- da21600.0022 wave04$wave &lt;- 4 rm(da21600.0022) w4 = subset(wave04, select = c(AID, wave)) # 5th wave load(&quot;~/NAS_Share/HP/AddHealthPublic/ICPSR_21600/DS0032/21600-0032-Data.rda&quot;) wave05 &lt;- da21600.0032 wave05$wave &lt;- 5 rm(da21600.0032) w5 = subset(wave05, select = c(AID, wave)) The complete list of respondents can be obtained by aggregating all WX datasets and then getting the unique AID. aaa &lt;- rbind(w1, w2, w3, w4, w5) AH01 &lt;- unique(subset(rbind(w1, w2, w3, w4, w5), select = c(AID))) It looks like the first wave contains all respondents - no addtional respondents were added. To check this observation, we will see both AID from the first wave and All matched. test01 &lt;- cbind(wave01, AH01, by=&quot;AID&quot;) Because the n didn’t change, we confirmed that WAVE01 contains all respondents of the study. 9.4 Long-form dataset (wave-person) Aggregating WX datasets will generate a long-form dataset per wave-person. lf01 &lt;- rbind(w1, w2, w3, w4, w5) lf &lt;- lf01[order(lf01$AID, lf01$wave), ] # 6504, 4834, 4882, 5114, 4196, 25530 We will save LF for the later use. 9.5 Exploring variables in Add Health Use “Variables” or other documentations at https://www.icpsr.umich.edu/web/ICPSR/studies/21600?archive=ICPSR&amp;q=21600 9.6 Outcomes, exposures, and confounders Let’s assume we are interested in the BMI trajectory, which calculation requires both weight and height in each wave. We also keep and rename exposures and confounders. This process requires getting back and force to add, rename, and remove a set of variables. To note, whenever keeping variables in your datasets, add AID and wave as default variables. It is a good practice to keep the variable names consistent - for example, variables for adolescence will have “a_”, while those for adolescence’s parents will have “p_”, and those of adolescence’s offspring will have “0_”. By keeping all variable selection in one place, you can minimize any confusions in managing variables later. So, it looks like either lbs or kg, cm or inch was used for weight and height. Also, there are multiple variables for each weight or height, requiring your further study about which one is better than others. Here, I will simply go with the following variables, using BMI formula at https://www.cdc.gov/healthyweight/assessing/bmi/childrens_BMI/childrens_BMI_formula.html#:~:text=The%20formula%20for%20BMI%20is,to%20convert%20this%20to%20meters.&amp;text=When%20using%20English%20measurements%2C%20pounds,2%20to%20kg%2Fm2. bmiwgt1 &lt;- wave01 %&gt;% select(AID, wave, &quot;a_srh&quot; = H1GH1, &quot;a_wgt_lbs&quot; = H1GH60, &quot;a_hgt_ft&quot; = H1GH59A, &quot;a_hgt_in&quot; = H1GH59B, &quot;a_weightimage&quot; = H1GH28, &quot;a_poorappetite&quot; = H1FS2) summary(bmiwgt1) ## AID wave a_srh a_wgt_lbs ## 57100270: 1 Min. :1 (1) (1) Excellent:1847 Min. : 50.0 ## 57101310: 1 1st Qu.:1 (2) (2) Very good:2608 1st Qu.:118.0 ## 57103171: 1 Median :1 (3) (3) Good :1605 Median :135.0 ## 57103869: 1 Mean :1 (4) (4) Fair : 408 Mean :141.1 ## 57104553: 1 3rd Qu.:1 (5) (5) Poor : 28 3rd Qu.:160.0 ## 57104649: 1 Max. :1 NA&#39;s : 8 Max. :360.0 ## (Other) :6498 NA&#39;s :156 ## a_hgt_ft a_hgt_in ## (4) (4) 4 feet: 214 (04) (4) 4 inches: 693 ## (5) (5) 5 feet:5448 (06) (6) 6 inches: 669 ## (6) (6) 6 feet: 758 (03) (3) 3 inches: 629 ## NA&#39;s : 84 (02) (2) 2 inches: 569 ## (05) (5) 5 inches: 562 ## (Other) :3287 ## NA&#39;s : 95 ## a_weightimage a_poorappetite ## (1) (1) Very underweight : 128 (0) (0) Never/rarely :4192 ## (2) (2) Slightly underweight : 935 (1) (1) Sometimes :1744 ## (3) (3) About the right weight:3381 (2) (2) A lot of the time : 410 ## (4) (4) Slightly overweight :1808 (3) (3) Most/all of the time: 141 ## (5) (5) Very overweight : 238 NA&#39;s : 17 ## NA&#39;s : 14 ## bmiwgt2 &lt;- wave02 %&gt;% select(AID, wave, &quot;a_srh&quot; = H2GH1, &quot;a_wgt_lbs&quot; = H2GH53, &quot;a_hgt_ft&quot; = H2WS16HF, &quot;a_hgt_in&quot; = H2WS16HI, &quot;a_weightimage&quot; = H2GH30, &quot;a_poorappetite&quot; = H2GH22) summary(bmiwgt2) ## AID wave a_srh a_wgt_lbs ## 57101310: 1 Min. :2 (1) (1) Excellent:1434 Min. : 60.0 ## 57103869: 1 1st Qu.:2 (2) (2) Very good:1923 1st Qu.:120.0 ## 57104649: 1 Median :2 (3) (3) Good :1179 Median :140.0 ## 57104676: 1 Mean :2 (4) (4) Fair : 286 Mean :145.7 ## 57109625: 1 3rd Qu.:2 (5) (5) Poor : 10 3rd Qu.:163.0 ## 57110897: 1 Max. :2 NA&#39;s : 2 Max. :350.0 ## (Other) :4828 NA&#39;s :86 ## a_hgt_ft a_hgt_in ## (4) (4) 4 feet: 88 (06) (6) 6 inches: 537 ## (5) (5) 5 feet:4077 (05) (5) 5 inches: 462 ## (6) (6) 6 feet: 638 (04) (4) 4 inches: 461 ## NA&#39;s : 31 (07) (7) 7 inches: 459 ## (02) (2) 2 inches: 415 ## (Other) :2469 ## NA&#39;s : 31 ## a_weightimage a_poorappetite ## (1) (1) Very underweight : 56 (0) (0) Never :2312 ## (2) (2) Slightly underweight : 697 (1) (1) Just a few times :1834 ## (3) (3) About the right weight:2576 (2) (2) About once a week: 508 ## (4) (4) Slightly overweight :1338 (3) (3) Almost every day : 136 ## (5) (5) Very overweight : 162 (4) (4) Every day : 42 ## NA&#39;s : 5 NA&#39;s : 2 ## bmiwgt3 &lt;- wave03 %&gt;% select(AID, wave, &quot;a_srh&quot; = H3GH1, &quot;a_wgt_lbs&quot; = H3DA44, &quot;a_hgt_ft&quot; = H3HGT_F, &quot;a_hgt_in&quot; = H3HGT_I) summary(bmiwgt3) ## AID wave a_srh a_wgt_lbs ## 57100270: 1 Min. :3 (1) (1) Excellent:1601 Min. : 80.0 ## 57101310: 1 1st Qu.:3 (2) (2) Very good:2000 1st Qu.:136.0 ## 57103869: 1 Median :3 (3) (3) Good :1055 Median :160.0 ## 57104676: 1 Mean :3 (4) (4) Fair : 206 Mean :168.5 ## 57109625: 1 3rd Qu.:3 (5) (5) Poor : 20 3rd Qu.:190.0 ## 57111071: 1 Max. :3 Max. :430.0 ## (Other) :4876 NA&#39;s :111 ## a_hgt_ft a_hgt_in ## (4) (4) 4 feet: 84 (04) (4) 4 inches: 470 ## (5) (5) 5 feet:3913 (06) (6) 6 inches: 458 ## (6) (6) 6 feet: 732 (02) (2) 2 inches: 448 ## (7) (7) 7 feet: 1 (03) (3) 3 inches: 424 ## NA&#39;s : 152 (07) (7) 7 inches: 409 ## (Other) :2518 ## NA&#39;s : 155 bmiwgt4 &lt;- wave04 %&gt;% select(AID, wave, &quot;a_srh&quot; = H4GH1, &quot;a_wgt_lbs&quot; = H4GH6, &quot;a_hgt_ft&quot; = H4GH5F, &quot;a_hgt_in&quot; = H4GH5I) summary(bmiwgt4) ## AID wave a_srh a_wgt_lbs ## 57101310: 1 Min. :4 (1) (1) Excellent: 979 Min. : 18.0 ## 57103869: 1 1st Qu.:4 (2) (2) Very good:1963 1st Qu.:150.0 ## 57109625: 1 Median :4 (3) (3) Good :1683 Median :178.0 ## 57111071: 1 Mean :4 (4) (4) Fair : 434 Mean :184.1 ## 57113943: 1 3rd Qu.:4 (5) (5) Poor : 55 3rd Qu.:211.0 ## 57117542: 1 Max. :4 Max. :525.0 ## (Other) :5108 NA&#39;s :79 ## a_hgt_ft a_hgt_in ## Min. :4.000 Min. : 0.000 ## 1st Qu.:5.000 1st Qu.: 2.000 ## Median :5.000 Median : 5.000 ## Mean :5.172 Mean : 5.386 ## 3rd Qu.:5.000 3rd Qu.: 8.000 ## Max. :6.000 Max. :11.000 ## NA&#39;s :6 NA&#39;s :10 bmiwgt5 &lt;- wave05 %&gt;% select(AID, wave, &quot;a_srh&quot; = H5ID1, &quot;a_wgt_lbs&quot; = H5ID3, &quot;a_hgt_ft&quot; = H5ID2F, &quot;a_hgt_in&quot; = H5ID2I) summary(bmiwgt5) ## AID wave a_srh a_wgt_lbs a_hgt_ft ## 57101310: 1 Min. :5 Min. :1.000 Min. :100.0 Min. : 4.000 ## 57111071: 1 1st Qu.:5 1st Qu.:2.000 1st Qu.:155.0 1st Qu.: 5.000 ## 57111786: 1 Median :5 Median :2.000 Median :185.0 Median : 5.000 ## 57113943: 1 Mean :5 Mean :2.473 Mean :192.2 Mean : 5.379 ## 57117997: 1 3rd Qu.:5 3rd Qu.:3.000 3rd Qu.:220.0 3rd Qu.: 5.000 ## 57118381: 1 Max. :5 Max. :5.000 Max. :400.0 Max. :98.000 ## (Other) :4190 NA&#39;s :4 NA&#39;s :25 NA&#39;s :2 ## a_hgt_in ## Min. : 0.0 ## 1st Qu.: 3.0 ## Median : 5.0 ## Mean : 10.6 ## 3rd Qu.: 8.0 ## Max. :998.0 ## NA&#39;s :6 typeof(bmiwgt5$a_hgt_ft) ## [1] &quot;double&quot; The “Rbind” function requires all datasets have a same numbers of columns. “setDT” and “fill=TRUE” are the functions from a “data.table” package that override this requirement. Now, we have created a long-form dataset (i.e., vars) from five sets of cross-sectional datasets. vars01 &lt;- rbind(setDT(bmiwgt1), setDT(bmiwgt2), setDT(bmiwgt3), setDT(bmiwgt4), setDT(bmiwgt5), fill=TRUE) vars &lt;- vars01[order(vars01$AID, vars01$wave), ] # 6504, 4834, 4882, 5114, 4196, 25530 9.7 Demographic variables - time-invariant demo_TI &lt;- wave01 %&gt;% select(AID, &quot;a_sex&quot; = BIO_SEX) 9.8 Merging datasets The following code merges a long-form dataset (i.e., vars) and a time-invariant dataset (i.e., demo_TI). Final01 &lt;- merge(vars, demo_TI, by = c(&quot;AID&quot;)) summary(Final01) ## AID wave a_srh a_wgt_lbs ## 57101310: 5 Min. :1.00 (2) (2) Very good:8494 Min. : 18.0 ## 57111071: 5 1st Qu.:1.00 (1) (1) Excellent:5861 1st Qu.:130.0 ## 57113943: 5 Median :3.00 (3) (3) Good :5522 Median :155.0 ## 57118381: 5 Mean :2.83 2 :1503 Mean :164.3 ## 57118943: 5 3rd Qu.:4.00 3 :1393 3rd Qu.:190.0 ## 57121404: 5 Max. :5.00 (Other) :2743 Max. :525.0 ## (Other) :25500 NA&#39;s : 14 NA&#39;s :457 ## a_hgt_ft a_hgt_in ## (5) (5) 5 feet:13438 (06) (6) 6 inches: 1664 ## 5 : 7504 (04) (4) 4 inches: 1624 ## (6) (6) 6 feet: 2128 (03) (3) 3 inches: 1451 ## 6 : 1664 (02) (2) 2 inches: 1432 ## (4) (4) 4 feet: 386 (07) (7) 7 inches: 1430 ## (Other) : 135 (Other) :17632 ## NA&#39;s : 275 NA&#39;s : 297 ## a_weightimage a_poorappetite ## (1) (1) Very underweight : 184 (0) (0) Never/rarely : 4192 ## (2) (2) Slightly underweight : 1632 (0) (0) Never : 2312 ## (3) (3) About the right weight: 5957 (1) (1) Just a few times : 1834 ## (4) (4) Slightly overweight : 3146 (1) (1) Sometimes : 1744 ## (5) (5) Very overweight : 400 (2) (2) About once a week: 508 ## NA&#39;s :14211 (Other) : 729 ## NA&#39;s :14211 ## a_sex ## (1) (1) Male :11874 ## (2) (2) Female:13654 ## NA&#39;s : 2 ## ## ## ## 9.9 Data management, recoding, and so on 9.9.1 BMI Alright, the following code does not work…. Final01$a_wgt_flag &lt;- ifelse(Final01$a_wgt_lbs &lt; 50, 1, 0) Final01$a_wgt_flag &lt;- ifelse(430 &lt; Final01$a_wgt_lbs, 1, 0) Final01$a_hgt_flag &lt;- ifelse(as.integer(Final01$a_hgt_ft) &lt; 4, 1, 0) Final01$a_hgt_flag &lt;- ifelse(95 &lt; as.integer(Final01$a_hgt_ft), 1, 0) Final01$a_hgt_flag &lt;- ifelse(95 &lt; as.integer(Final01$a_hgt_in), 1, 0) summary(Final01) ## AID wave a_srh a_wgt_lbs ## 57101310: 5 Min. :1.00 (2) (2) Very good:8494 Min. : 18.0 ## 57111071: 5 1st Qu.:1.00 (1) (1) Excellent:5861 1st Qu.:130.0 ## 57113943: 5 Median :3.00 (3) (3) Good :5522 Median :155.0 ## 57118381: 5 Mean :2.83 2 :1503 Mean :164.3 ## 57118943: 5 3rd Qu.:4.00 3 :1393 3rd Qu.:190.0 ## 57121404: 5 Max. :5.00 (Other) :2743 Max. :525.0 ## (Other) :25500 NA&#39;s : 14 NA&#39;s :457 ## a_hgt_ft a_hgt_in ## (5) (5) 5 feet:13438 (06) (6) 6 inches: 1664 ## 5 : 7504 (04) (4) 4 inches: 1624 ## (6) (6) 6 feet: 2128 (03) (3) 3 inches: 1451 ## 6 : 1664 (02) (2) 2 inches: 1432 ## (4) (4) 4 feet: 386 (07) (7) 7 inches: 1430 ## (Other) : 135 (Other) :17632 ## NA&#39;s : 275 NA&#39;s : 297 ## a_weightimage a_poorappetite ## (1) (1) Very underweight : 184 (0) (0) Never/rarely : 4192 ## (2) (2) Slightly underweight : 1632 (0) (0) Never : 2312 ## (3) (3) About the right weight: 5957 (1) (1) Just a few times : 1834 ## (4) (4) Slightly overweight : 3146 (1) (1) Sometimes : 1744 ## (5) (5) Very overweight : 400 (2) (2) About once a week: 508 ## NA&#39;s :14211 (Other) : 729 ## NA&#39;s :14211 ## a_sex a_wgt_flag a_hgt_flag ## (1) (1) Male :11874 Min. :0e+00 Min. :0 ## (2) (2) Female:13654 1st Qu.:0e+00 1st Qu.:0 ## NA&#39;s : 2 Median :0e+00 Median :0 ## Mean :2e-04 Mean :0 ## 3rd Qu.:0e+00 3rd Qu.:0 ## Max. :1e+00 Max. :0 ## NA&#39;s :457 NA&#39;s :297 9.9.2 Sampling weights 9.9.3 Multiple imputation 9.10 Analytic approach The following models are demonstration only - mostly, the models themselves do not make sense. 9.10.1 A linear regression with the current dataset lmer(a_wgt_lbs ~ as.numeric(a_srh) + a_sex + (1 | AID), data=Final01) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: a_wgt_lbs ~ as.numeric(a_srh) + a_sex + (1 | AID) ## Data: Final01 ## REML criterion at convergence: 248599 ## Random effects: ## Groups Name Std.Dev. ## AID (Intercept) 33.69 ## Residual 26.88 ## Number of obs: 25069, groups: AID, 6492 ## Fixed Effects: ## (Intercept) as.numeric(a_srh) a_sex(2) (2) Female ## 160.802 5.422 -25.999 "]]
